{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "nf=pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "nf.columns = nf.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 62)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id'],axis=1,inplace=True)\n",
    "df.drop(['url'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf.drop(['id'],axis=1,inplace=True)\n",
    "nf.drop(['url'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=df.iloc[0:,:]\n",
    "df_Test=nf.iloc[0:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['shares'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 59)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=df_Train.drop(['shares'],axis=1)\n",
    "y_train=df_Train['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "classifier=xgboost.XGBRegressor()\n",
    "#classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error', n_jobs = 4,\n",
    "            verbose = 5,\n",
    "            return_train_score = True,\n",
    "            random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 14.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:02:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=...\n",
       "                   iid='deprecated', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gblinear', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimators=1100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, booster='gblinear', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=4, missing=None, n_estimators=1100,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:08:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gblinear', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=4, missing=None, n_estimators=1100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(classifier,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2008.1556 ,  3805.1055 ,  1593.3964 ,  3119.2427 ,  2903.9421 ,\n",
       "        2281.2107 ,  3961.6245 ,  3131.9353 ,  2760.266  ,  1443.113  ,\n",
       "        2765.7485 ,  6143.937  ,  2301.7837 ,  2922.5728 ,  5293.079  ,\n",
       "        2862.6003 ,  2266.796  ,  3764.5337 ,  2416.1182 ,  2941.2861 ,\n",
       "        1835.5144 ,  3313.9636 ,  3939.4856 ,  3006.2668 ,  2168.3198 ,\n",
       "        2444.818  ,  2020.1921 ,  2397.2134 ,  3107.4634 ,  3180.3582 ,\n",
       "        2881.2852 ,  4017.6208 ,  3234.4412 ,  2133.9045 ,  4343.807  ,\n",
       "        2857.626  ,  3343.316  ,  5144.4077 ,  5260.9736 ,  3328.548  ,\n",
       "        3222.242  ,  2238.3276 ,  2295.8176 ,  2587.6243 ,  5300.913  ,\n",
       "        3152.892  ,  2687.4634 ,  4703.2007 ,  1794.3013 ,  4120.181  ,\n",
       "        5080.7583 ,  5554.434  ,  3515.063  ,  3248.7832 ,  2924.6433 ,\n",
       "        7514.6255 ,  2694.819  ,  2665.3042 ,  3603.2576 ,  2230.12   ,\n",
       "        6986.5425 ,  3733.2317 ,  1178.0173 ,  4788.1426 ,  6826.5605 ,\n",
       "        2384.8857 ,  4253.1157 ,  3305.4224 ,  2275.6082 ,  1465.6736 ,\n",
       "        6370.649  ,  4521.5034 ,  2891.9792 ,  4162.027  ,  3542.4033 ,\n",
       "        2037.0609 ,  7160.059  ,  4037.8442 ,  4011.7    ,  2559.7388 ,\n",
       "        1104.5791 ,  3676.8918 ,  3309.452  ,  4217.616  ,  2356.3323 ,\n",
       "        7182.851  ,  2974.362  ,  3712.7231 ,  3323.9265 ,  2602.1777 ,\n",
       "        2598.7883 ,  6373.6006 ,  2784.1455 ,  3219.0342 ,  3248.6436 ,\n",
       "        2662.4114 ,  4244.4536 ,  4006.7605 ,  6652.6675 ,  1895.3175 ,\n",
       "        3755.5996 ,  3706.155  ,  1599.5043 ,  1115.1345 ,  2059.306  ,\n",
       "        6529.367  ,  2457.9998 ,  6465.0435 ,  3374.8616 ,  4457.791  ,\n",
       "        1751.2009 ,  3071.162  ,  5419.867  ,  2911.366  ,  2112.4827 ,\n",
       "        6716.6963 ,  1603.7628 ,  2560.524  ,  2773.446  ,  3475.2537 ,\n",
       "        5357.421  ,  2404.8442 ,  1751.0168 ,  2674.0845 ,  2838.4136 ,\n",
       "        1348.8184 ,  2361.8997 ,  3562.5586 ,  1123.494  ,  6066.134  ,\n",
       "        3448.1453 ,  3491.5486 ,  7108.6274 ,  4240.1475 ,  2512.0713 ,\n",
       "        2765.5564 ,  1883.2073 ,  2657.4294 ,  3766.265  ,  2280.5474 ,\n",
       "        3586.6565 ,  2982.831  ,  3060.8533 ,  7334.024  ,  4618.3765 ,\n",
       "        3713.0188 ,  2688.6995 ,  2913.0488 ,  2049.6235 ,  3834.5889 ,\n",
       "        2340.8767 ,  3804.8928 ,  2334.035  ,  2834.0076 ,  3859.434  ,\n",
       "        2487.703  ,  3393.8435 ,  2489.8618 ,  2220.9673 ,  5327.8423 ,\n",
       "        6481.639  ,  3002.8005 ,  4456.276  ,  6961.209  ,  3050.0618 ,\n",
       "        5368.616  ,  3231.7566 ,  4190.0273 ,  3097.406  ,  4613.9336 ,\n",
       "        5255.046  ,  6994.8535 ,  2741.9612 ,  4434.0747 ,  2412.706  ,\n",
       "        3695.8125 ,  2123.6555 ,  4161.7983 ,  2413.683  ,  2836.9011 ,\n",
       "        3531.6948 ,  1945.8306 ,  4794.88   ,  3891.5776 ,  3532.3545 ,\n",
       "        3378.9565 ,  8308.035  ,  4588.4624 ,  8135.947  ,  3372.4646 ,\n",
       "        4062.8118 ,  1711.2527 ,  4130.062  ,  3289.9517 ,  2677.6614 ,\n",
       "        4042.9744 ,  1839.1793 ,  3185.906  ,  2376.2415 ,  2709.5508 ,\n",
       "        3516.322  ,  3479.6213 ,  3838.754  ,  4765.3667 ,  4513.9116 ,\n",
       "        2843.5505 ,  3157.8728 ,  2979.2793 ,  3864.3782 ,  6079.356  ,\n",
       "        6359.5874 ,  1461.6199 ,  3199.756  , 13108.776  ,  2130.1653 ,\n",
       "        6756.756  ,  6220.116  ,  2877.2434 ,  4497.891  ,  1668.4213 ,\n",
       "        2622.7258 ,  1941.9454 ,  2573.664  ,  1803.0826 ,  2239.8035 ,\n",
       "        2807.8928 ,  7710.7183 ,  5406.9243 ,  2671.8582 ,  4199.3325 ,\n",
       "        5083.9194 ,  2872.445  ,  2241.2878 ,  2547.295  ,  4167.878  ,\n",
       "        4012.5154 ,  2121.4836 ,  2531.6113 ,  5335.3022 ,  2984.883  ,\n",
       "        1773.182  ,  1604.2396 ,  4394.6543 ,  2599.0557 ,  2900.0918 ,\n",
       "        4292.23   ,  3016.7524 ,  2222.994  ,  2644.1682 ,  1858.8411 ,\n",
       "        2404.0347 ,  3361.4944 ,  7908.148  ,  2679.98   ,  1939.6237 ,\n",
       "        3296.9119 ,  3385.6873 ,  2757.9502 ,  7851.9883 ,  3804.7683 ,\n",
       "        3317.5415 ,  4089.6074 ,  3162.634  ,  2925.88   ,  3025.639  ,\n",
       "        3080.934  ,  1638.7039 ,  5492.421  ,  3496.2566 ,  3468.446  ,\n",
       "        4126.67   ,  3643.437  ,  2262.6304 ,  4863.5894 ,  3222.6975 ,\n",
       "        3496.6646 ,  3394.6328 ,  4947.842  ,  4950.7734 ,  6533.3853 ,\n",
       "        3758.8452 ,  2323.6157 ,  2826.9583 ,  4398.341  ,  2485.2063 ,\n",
       "        2325.838  ,  6868.112  ,  3496.9377 ,  2479.1736 ,  2775.0354 ,\n",
       "        2845.0686 ,  1766.2756 ,  4604.8276 ,  2784.7192 ,  3019.9924 ,\n",
       "        2540.8171 ,  6009.0044 ,  2644.7131 ,  1960.3682 ,  3099.2446 ,\n",
       "        4615.1885 ,  4450.667  ,  3275.3135 ,  6054.5015 ,  5611.0215 ,\n",
       "        3325.3818 ,  2185.7168 ,  6198.1865 ,  2911.0073 ,  4280.4    ,\n",
       "        3223.4744 ,  2926.9253 ,  4930.182  ,  2900.3542 ,  3066.7292 ,\n",
       "        2785.8936 ,  3632.1318 ,  3038.9338 ,  2954.1055 ,  2135.4849 ,\n",
       "        4217.228  ,  5446.0293 ,  5135.3643 ,  3703.794  ,  3193.411  ,\n",
       "        9412.028  ,  3260.1702 ,  1294.9274 ,  2559.103  ,  7579.9727 ,\n",
       "        6808.6416 ,  4439.49   ,  3333.3376 ,  4934.194  ,  2993.3276 ,\n",
       "        3370.9592 ,  2535.3745 ,  3638.6394 ,  3512.5383 ,  3383.9546 ,\n",
       "        4382.021  ,  2745.6472 ,  3842.7424 ,  3088.7996 ,  2355.2932 ,\n",
       "        1499.6656 ,  3615.078  ,  2085.3682 ,  2016.8947 ,  1849.0664 ,\n",
       "        3626.4983 ,  1922.2665 ,  2339.9807 ,  9293.603  ,  5934.3184 ,\n",
       "        3072.486  ,  8148.897  ,  1822.5784 ,  1111.5596 ,  2197.8948 ,\n",
       "        2819.5732 ,  2620.4187 ,  5071.189  ,  2529.7292 ,  7951.6016 ,\n",
       "        2579.841  ,  2762.2227 ,  3064.6882 ,  3593.6477 ,  3451.6833 ,\n",
       "        1804.0216 ,  1846.19   ,  2699.719  ,  5396.6396 ,  3479.5352 ,\n",
       "        1889.4883 ,  5046.023  ,  2659.166  ,  3912.5947 ,  4185.1636 ,\n",
       "        3938.9834 ,  4652.6274 ,  2570.3323 ,  5608.491  ,  2690.9578 ,\n",
       "        3104.5723 ,  5912.128  ,  4827.8716 ,  2904.06   ,  3592.5242 ,\n",
       "        1648.2319 ,  4350.371  ,  5240.0137 ,  2568.108  ,  7437.8745 ,\n",
       "        3462.3438 ,  3409.8723 ,  4743.3296 ,  3070.8962 ,  4552.3555 ,\n",
       "        1840.3853 ,  7865.045  ,  3068.204  ,  3095.4592 ,  1753.9622 ,\n",
       "        4160.3184 ,  3950.1624 ,  1964.2169 ,  5333.637  ,  2402.0278 ,\n",
       "        4548.6885 ,  1982.0077 ,  1866.1732 ,  1934.7637 ,  1885.1637 ,\n",
       "        3191.5894 ,  2527.813  ,  1983.4615 ,  3013.2993 ,  2606.3809 ,\n",
       "        3371.9875 ,  2655.36   ,  2704.521  ,  1865.3907 ,  4204.727  ,\n",
       "        1793.5162 ,  3777.319  ,  3064.4543 ,  2296.2178 ,  2533.1436 ,\n",
       "        4937.596  ,  2159.9512 ,  4190.209  ,  1842.8712 ,  2461.8281 ,\n",
       "        1629.4828 ,  4906.207  ,  3853.1584 ,  2800.4326 ,  2967.4238 ,\n",
       "        2616.941  ,  2977.7512 ,  2811.5215 ,  5180.413  ,  2745.8547 ,\n",
       "        2808.9346 ,  3046.9814 ,  2029.9727 ,  4436.3247 ,  3593.038  ,\n",
       "        2716.4807 ,  2197.0884 ,  2775.891  ,  1738.9542 ,  2200.0115 ,\n",
       "        2444.9512 ,  2141.5732 ,  2655.2212 ,  1674.9052 ,  3573.1965 ,\n",
       "        6664.841  ,  3062.9321 ,  2642.9192 ,  2644.0994 ,  2575.8892 ,\n",
       "        5463.091  ,  4308.612  ,  2084.905  ,  2921.6987 ,  4134.5903 ,\n",
       "        3239.4626 ,  3013.5486 ,  3459.5674 ,  3132.402  ,  3558.1965 ,\n",
       "        5726.481  ,  1705.0172 ,  2368.9243 ,  5413.0767 ,  3018.1226 ,\n",
       "        4041.7883 ,  3449.2817 ,  3249.993  ,  2971.3694 ,  5024.375  ,\n",
       "        2050.4673 ,  1544.8256 ,  8538.373  ,  2463.2058 ,  2850.8115 ,\n",
       "        4706.0645 ,  3262.8845 ,  3399.8647 ,  2623.84   ,  3849.4958 ,\n",
       "        6672.0747 ,  2231.6245 ,  6335.5483 ,  1703.0369 ,  3008.583  ,\n",
       "        5258.0513 ,  2861.6653 ,  3338.4307 ,  2912.1768 ,  2805.2686 ,\n",
       "        4138.722  ,  1491.2352 , 10775.186  ,  5140.3936 ,  3309.3474 ,\n",
       "        2201.0925 ,  4212.642  ,  2720.123  ,  2773.9062 ,  2594.2903 ,\n",
       "        1552.5791 ,  3372.7888 ,  2918.6536 ,  2745.954  ,  4039.3264 ,\n",
       "        2538.0225 ,  3611.846  ,  3361.716  ,  2866.8271 ,  2450.8262 ,\n",
       "        4699.5073 ,  4224.6846 ,  5938.6587 ,  3616.6272 ,  2927.8953 ,\n",
       "        9097.606  ,  2772.1614 ,  2961.2517 ,  2333.359  ,  2573.0833 ,\n",
       "        6464.623  ,  3478.5393 ,  5333.508  ,  3561.9631 ,  3682.8066 ,\n",
       "        2301.338  ,  6318.0195 ,  3022.737  ,  3931.2703 ,  4471.1816 ,\n",
       "        2501.324  ,  3488.4932 ,  2543.095  ,  2704.6694 ,  6005.132  ,\n",
       "        3057.969  ,  3586.9204 ,  4118.2656 ,  2376.8677 ,  6892.767  ,\n",
       "        1871.2455 ,  2544.3862 ,  2065.6018 ,  2866.6472 ,   834.3174 ,\n",
       "        2207.688  ,  2474.346  ,  2403.2466 ,  3690.8608 ,  4026.082  ,\n",
       "        2823.483  ,  2386.7073 ,  2547.254  ,  1644.3617 ,  2641.6614 ,\n",
       "        4895.435  ,  2968.1882 ,  1371.7216 ,  2449.1487 ,  1701.3872 ,\n",
       "        2393.3494 ,  6787.625  ,  3939.4224 ,  2662.978  ,  1851.9402 ,\n",
       "        2106.636  ,  5332.4688 ,  2597.4133 ,  3227.669  ,  2168.5305 ,\n",
       "        1822.4272 ,  2118.7292 ,  3641.3699 ,  3832.8318 ,  3675.34   ,\n",
       "        5981.6836 ,  2116.1414 ,  4467.4985 ,  3543.5835 ,  2540.7986 ,\n",
       "        1590.3779 ,  4618.856  ,  2493.4636 ,  4604.459  ,  3353.1082 ,\n",
       "        3181.7551 ,  3757.441  ,  2779.648  ,  6383.576  ,  3010.8557 ,\n",
       "        5960.401  ,  4325.5205 ,  3126.7493 ,  3744.981  ,  3336.2825 ,\n",
       "        4273.653  ,  3502.4834 ,  2676.8352 ,  3507.2422 ,  2570.96   ,\n",
       "        2038.7202 ,  5595.914  ,  3912.0698 ,  3155.0232 ,  2838.828  ,\n",
       "        4731.7275 ,  3953.9785 ,  9428.856  ,  6082.499  ,  2585.008  ,\n",
       "        3356.2285 ,   980.35614,  2638.621  ,  4638.9717 ,  5169.7725 ,\n",
       "        3638.5754 ,  1717.3176 ,  2107.2283 ,  2891.2554 ,  2678.291  ,\n",
       "        3747.4111 ,  5620.9844 ,  2230.7578 ,  3233.2776 ,  3460.579  ,\n",
       "        5204.8784 ,  1708.06   ,  2522.0886 ,  1558.4814 ,  3848.6921 ,\n",
       "        2838.576  ,  5731.681  ,  2187.5886 ,  8366.152  , 12705.955  ,\n",
       "        2765.5203 ,  5108.644  ,  3698.2502 ,  4052.0361 ,  7732.42   ,\n",
       "        3685.5469 ,  2094.714  ,  4620.711  ,  2199.9836 ,  2773.1123 ,\n",
       "        4043.5723 ,  2577.2913 ,  8461.376  ,  3717.5166 ,  3245.344  ,\n",
       "        4383.682  ,  3112.465  ,  3941.3145 ,  4527.657  ,  2333.38   ,\n",
       "        4999.5854 ,  2443.1514 ,  2972.229  ,  3863.471  ,  3131.8389 ,\n",
       "        2060.3352 ,  1482.9615 ,  2968.2036 ,  3200.717  ,  1882.9248 ,\n",
       "        2792.2146 ,  1988.9462 ,  3525.3706 ,  4661.01   ,  5355.3413 ,\n",
       "        3714.419  ,  2748.8135 ,  2532.113  ,  2742.1797 ,  2489.3704 ,\n",
       "        4333.8877 ,  3561.069  ,  3634.9426 ,  3677.836  ,  2360.5903 ,\n",
       "        4229.174  ,  1970.358  ,  2505.504  ,  2764.6628 ,  2506.8918 ,\n",
       "        4778.8193 ,  2570.0044 ,  4869.7773 ,  1841.0687 ,  3781.6953 ,\n",
       "        4842.967  ,  2820.2534 ,  3399.7068 ,  1726.6125 ,  4435.241  ,\n",
       "        2375.2197 ,  2296.0005 ,  2125.813  ,  3624.2236 ,  3371.5667 ,\n",
       "        2087.5388 ,  2258.8337 ,  1457.3296 ,  3702.8445 ,  3669.3718 ,\n",
       "        1510.5984 ,  3587.6118 ,  6820.9424 ,  3025.1172 ,  2005.3109 ,\n",
       "        3147.1035 ,  2421.8276 ,  2131.8425 ,  4198.1006 ,  1929.3425 ,\n",
       "        3235.9004 ,  1732.3827 ,  4009.209  ,  7016.2285 ,  1322.323  ,\n",
       "        3421.0916 ,  6310.0723 ,  5125.737  ,  3505.6013 ,  5445.003  ,\n",
       "        1732.8177 ,  3394.1846 ,  2218.4158 ,  3240.4172 ,  5913.4927 ,\n",
       "        1776.4578 ,  1619.5796 ,  2933.6458 ,  3364.999  ,  3493.639  ,\n",
       "        2751.3005 ,  6972.6523 ,  2530.142  ,  3105.673  ,  4885.937  ,\n",
       "        3339.2656 ,  1846.3514 ,  3043.046  ,  2277.9526 ,  2358.982  ,\n",
       "        3433.5544 ,  3316.409  ,  2281.9116 ,  4092.7854 ,  2880.3972 ,\n",
       "        8184.199  ,  1516.8022 ,  3011.6316 ,  2310.062  ,  2105.4333 ,\n",
       "        2089.9846 ,  2186.3755 ,  2769.5981 ,  1995.4327 ,  2224.0525 ,\n",
       "        3777.0522 ,  3186.595  ,  4064.3044 ,  2549.9631 ,  2793.428  ,\n",
       "        3140.7007 ,  2572.9304 ,  2108.5366 ,  2412.9663 ,  4968.0938 ,\n",
       "        2254.6345 ,  3750.128  ,  3135.5107 ,  5145.902  ,  3445.2703 ,\n",
       "       15425.043  ,  2617.7505 ,  3486.3389 ,  2460.2007 ,  3727.0386 ,\n",
       "        3442.0024 ,  5661.2734 ,  2199.3457 ,  4714.269  ,  6167.2505 ,\n",
       "        2094.0605 ,  2983.5198 ,  3006.2085 ,  2704.6636 ,  1915.6691 ,\n",
       "        3373.7866 ,  3152.174  ,  1520.046  ,  8823.166  ,  9510.9795 ,\n",
       "        3072.708  ,  3627.4622 ,  2731.2551 ,  3197.8704 ,  2210.806  ,\n",
       "        3817.4756 ,  1582.777  ,  3407.0469 ,  5629.0757 ,  3383.72   ,\n",
       "        1714.148  ,  2965.0046 , 12252.578  ,  2426.2651 ,  2062.193  ,\n",
       "        5217.0933 ,  2705.4683 ,  2717.2878 ,  5831.2812 ,  2493.758  ,\n",
       "        3784.4895 ,  1156.17   ,  1567.0798 ,  3565.2495 ,  2247.899  ,\n",
       "        4179.351  ,  2519.7356 ,  1504.8062 ,  1728.7985 ,  2312.0835 ,\n",
       "        2016.5195 ,  3990.088  ,  3380.224  ,  3535.4949 ,  2973.2056 ,\n",
       "        2686.0815 ,  2788.3057 ,  1894.842  ,  2321.8557 ,  4252.574  ,\n",
       "        3254.1428 ,  4086.9402 ,  6378.34   ,  2176.7986 ,  1571.7277 ,\n",
       "        1667.1282 ,  2942.5479 ,  5220.11   ,  2795.0603 ,  1913.5117 ,\n",
       "        1567.919  ,  5285.2993 ,  1891.6935 ,  2691.871  ,  2198.845  ,\n",
       "        3911.0916 ,  4074.2776 ,  1319.6782 ,  2456.7686 ,  4141.2305 ,\n",
       "        7942.347  ,  2250.6094 ,  1468.3022 ,  3078.3152 ,  2400.4587 ,\n",
       "        2698.7085 ,  4428.6655 ,  5268.3003 ,  2685.3757 ,  5024.294  ,\n",
       "        3655.8127 ,  2472.8137 ,  3608.4382 ,  2142.3586 ,  2661.1575 ,\n",
       "        1524.116  ,  3223.769  ,  6798.4272 ,  4970.5244 ,  3253.653  ,\n",
       "        3801.5237 ,  4985.454  ,  4054.055  ,  3176.0874 ,  2659.988  ,\n",
       "        2003.7412 ,  2859.2986 ,  5637.4175 ,  3681.377  ,  2394.1597 ,\n",
       "        5369.3467 ,  3178.4878 ,  6626.855  ,  4469.0156 ,  1659.8676 ,\n",
       "        1148.2015 ,  5468.3877 ,  4934.316  ,  3524.8403 ,  4179.5747 ,\n",
       "        2664.437  ,  2504.732  ,  3206.2085 ,  2879.8303 ,  3143.9749 ,\n",
       "        2921.0605 ,  3176.4817 ,  3802.4082 ,  2332.1755 ,  3300.439  ,\n",
       "        4205.329  ,  2889.4678 ,  2265.6213 ,  5597.825  ,  4914.562  ,\n",
       "        3246.2903 ,  1984.6299 ,  4121.9194 ,  3344.396  ,  4832.189  ,\n",
       "        2784.9973 ,  2767.56   ,  3430.1343 ,  3339.3574 ,  4249.1436 ,\n",
       "        3617.2744 ,  2638.935  ,  6762.3423 ,  2356.6511 ,  3362.799  ,\n",
       "        5017.4756 ,  3495.5693 ,  3910.5125 ,  2224.7437 ,  2979.8208 ,\n",
       "        4426.4355 ,  6147.02   ,  3493.4785 ,  6264.1646 ,  2325.2524 ,\n",
       "        4333.238  ,  2753.411  ,  2327.2512 ,  5174.6    ,  4045.3179 ,\n",
       "        3185.75   ,  3024.8306 ,  2981.7825 ,  4539.3315 ,  4074.1    ,\n",
       "        3202.4995 ,  1852.0872 ,  2861.6892 ,  2505.6155 ,  3156.612  ,\n",
       "        3011.4832 ,  2884.17   ,  1746.5021 ,  1950.6439 ,  4861.5938 ,\n",
       "        5477.1167 ,  3728.852  ,  2671.4985 ,  4129.3467 ,  4795.5005 ,\n",
       "        1965.9745 ,  3629.9507 ,  4287.356  ,  3126.6848 ,  2128.3826 ,\n",
       "        3592.693  ,  4974.45   ,  2807.4036 ,  3450.8787 ,  2987.2544 ,\n",
       "        2072.6797 ,  3830.4055 ,  3627.6045 ,  2323.2974 ,  2392.9675 ,\n",
       "        3111.6514 ,  1344.317  ,  2769.2153 ,  2196.2039 ,  3283.615  ,\n",
       "        6973.323  ,  3231.2612 ,  2419.7764 ,  2595.6914 ,  4867.498  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df_Train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timedelta</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.243861</td>\n",
       "      <td>-0.062606</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.012354</td>\n",
       "      <td>-0.011096</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.084135</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.002302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078065</td>\n",
       "      <td>0.107565</td>\n",
       "      <td>-0.027320</td>\n",
       "      <td>0.055668</td>\n",
       "      <td>-0.085431</td>\n",
       "      <td>-0.011182</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.015111</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>-0.011844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_title</th>\n",
       "      <td>-0.243861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.010082</td>\n",
       "      <td>-0.009861</td>\n",
       "      <td>-0.010093</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>-0.020840</td>\n",
       "      <td>0.070137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.020748</td>\n",
       "      <td>-0.002784</td>\n",
       "      <td>-0.026924</td>\n",
       "      <td>0.024747</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>-0.150208</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.010694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_content</th>\n",
       "      <td>-0.062606</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025534</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.027342</td>\n",
       "      <td>0.453081</td>\n",
       "      <td>0.319458</td>\n",
       "      <td>0.365091</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259488</td>\n",
       "      <td>0.410739</td>\n",
       "      <td>-0.127551</td>\n",
       "      <td>-0.432253</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.025070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010082</td>\n",
       "      <td>0.025534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>0.076078</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>-0.040915</td>\n",
       "      <td>0.025734</td>\n",
       "      <td>0.025433</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>-0.012074</td>\n",
       "      <td>-0.003994</td>\n",
       "      <td>-0.026061</td>\n",
       "      <td>-0.009672</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <td>-0.012354</td>\n",
       "      <td>-0.009861</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.027346</td>\n",
       "      <td>0.079556</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015892</td>\n",
       "      <td>-0.036938</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>-0.012317</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>-0.009818</td>\n",
       "      <td>0.004003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <td>-0.011096</td>\n",
       "      <td>-0.010093</td>\n",
       "      <td>0.027342</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>0.025153</td>\n",
       "      <td>0.074568</td>\n",
       "      <td>-0.004042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012904</td>\n",
       "      <td>-0.038530</td>\n",
       "      <td>0.024402</td>\n",
       "      <td>0.023428</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>-0.012543</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.025816</td>\n",
       "      <td>-0.010055</td>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_hrefs</th>\n",
       "      <td>0.001155</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>0.453081</td>\n",
       "      <td>-0.001368</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>-0.001454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>0.373871</td>\n",
       "      <td>0.096269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094226</td>\n",
       "      <td>0.301379</td>\n",
       "      <td>-0.154175</td>\n",
       "      <td>-0.264165</td>\n",
       "      <td>0.050412</td>\n",
       "      <td>0.047865</td>\n",
       "      <td>0.037257</td>\n",
       "      <td>-0.001619</td>\n",
       "      <td>0.058085</td>\n",
       "      <td>0.032497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <td>0.084135</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>0.319458</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>0.027346</td>\n",
       "      <td>0.025153</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.281426</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062645</td>\n",
       "      <td>0.204620</td>\n",
       "      <td>-0.052529</td>\n",
       "      <td>-0.100594</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>-0.009001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_imgs</th>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.020840</td>\n",
       "      <td>0.365091</td>\n",
       "      <td>0.076078</td>\n",
       "      <td>0.079556</td>\n",
       "      <td>0.074568</td>\n",
       "      <td>0.373871</td>\n",
       "      <td>0.281426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.071728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037248</td>\n",
       "      <td>0.175201</td>\n",
       "      <td>-0.074474</td>\n",
       "      <td>-0.135062</td>\n",
       "      <td>0.037679</td>\n",
       "      <td>0.062114</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>-0.017980</td>\n",
       "      <td>0.069438</td>\n",
       "      <td>0.063019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_videos</th>\n",
       "      <td>-0.002302</td>\n",
       "      <td>0.070137</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.004042</td>\n",
       "      <td>0.096269</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>-0.071728</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012927</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>-0.117901</td>\n",
       "      <td>-0.140754</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>0.081046</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>-0.050267</td>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.015876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_token_length</th>\n",
       "      <td>0.139347</td>\n",
       "      <td>-0.077376</td>\n",
       "      <td>0.177912</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.014498</td>\n",
       "      <td>0.220437</td>\n",
       "      <td>0.131153</td>\n",
       "      <td>0.056121</td>\n",
       "      <td>-0.002076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209098</td>\n",
       "      <td>0.482247</td>\n",
       "      <td>-0.342789</td>\n",
       "      <td>-0.299359</td>\n",
       "      <td>-0.203175</td>\n",
       "      <td>-0.022579</td>\n",
       "      <td>-0.020348</td>\n",
       "      <td>0.019910</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.035962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_keywords</th>\n",
       "      <td>0.049123</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.087727</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>-0.002120</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.125918</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>-0.013286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020139</td>\n",
       "      <td>0.067138</td>\n",
       "      <td>-0.002142</td>\n",
       "      <td>-0.021641</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.030090</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>-0.013859</td>\n",
       "      <td>0.050314</td>\n",
       "      <td>0.033619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <td>0.073851</td>\n",
       "      <td>-0.063675</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>-0.003802</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.003469</td>\n",
       "      <td>0.053029</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>0.015586</td>\n",
       "      <td>-0.048291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014450</td>\n",
       "      <td>0.056676</td>\n",
       "      <td>-0.002905</td>\n",
       "      <td>-0.026245</td>\n",
       "      <td>0.040465</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>0.036361</td>\n",
       "      <td>0.024134</td>\n",
       "      <td>0.007844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <td>-0.056924</td>\n",
       "      <td>0.143992</td>\n",
       "      <td>0.056494</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>0.031753</td>\n",
       "      <td>0.031544</td>\n",
       "      <td>-0.004510</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>0.095384</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017896</td>\n",
       "      <td>0.097288</td>\n",
       "      <td>-0.144261</td>\n",
       "      <td>-0.126649</td>\n",
       "      <td>-0.019636</td>\n",
       "      <td>0.066871</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>-0.076972</td>\n",
       "      <td>0.048874</td>\n",
       "      <td>-0.022928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <td>0.059449</td>\n",
       "      <td>-0.037195</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>-0.005716</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.069959</td>\n",
       "      <td>-0.046711</td>\n",
       "      <td>-0.149137</td>\n",
       "      <td>-0.054609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052687</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.081754</td>\n",
       "      <td>0.090965</td>\n",
       "      <td>-0.007269</td>\n",
       "      <td>-0.043188</td>\n",
       "      <td>0.025550</td>\n",
       "      <td>-0.003141</td>\n",
       "      <td>-0.025362</td>\n",
       "      <td>-0.031095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <td>0.067716</td>\n",
       "      <td>-0.094391</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>-0.003626</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.053698</td>\n",
       "      <td>0.059030</td>\n",
       "      <td>-0.004338</td>\n",
       "      <td>-0.029995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070476</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>-0.029768</td>\n",
       "      <td>-0.036112</td>\n",
       "      <td>0.019416</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>-0.027533</td>\n",
       "      <td>0.007476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <td>0.084291</td>\n",
       "      <td>-0.037074</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>-0.005782</td>\n",
       "      <td>-0.006146</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.195332</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>-0.084695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030435</td>\n",
       "      <td>0.035468</td>\n",
       "      <td>0.104738</td>\n",
       "      <td>0.119615</td>\n",
       "      <td>0.023769</td>\n",
       "      <td>-0.046619</td>\n",
       "      <td>0.008651</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>-0.048378</td>\n",
       "      <td>-0.016670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <td>-0.169721</td>\n",
       "      <td>0.044960</td>\n",
       "      <td>0.065606</td>\n",
       "      <td>-0.008245</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.007495</td>\n",
       "      <td>-0.021516</td>\n",
       "      <td>-0.112074</td>\n",
       "      <td>-0.121207</td>\n",
       "      <td>-0.071202</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033642</td>\n",
       "      <td>-0.110580</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>-0.097589</td>\n",
       "      <td>0.078292</td>\n",
       "      <td>-0.050736</td>\n",
       "      <td>-0.075792</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>-0.053924</td>\n",
       "      <td>-0.068843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_min</th>\n",
       "      <td>0.592796</td>\n",
       "      <td>-0.111593</td>\n",
       "      <td>-0.052682</td>\n",
       "      <td>-0.004466</td>\n",
       "      <td>-0.005019</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>-0.053208</td>\n",
       "      <td>-0.002237</td>\n",
       "      <td>-0.024425</td>\n",
       "      <td>-0.003576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>0.007985</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>0.045612</td>\n",
       "      <td>-0.031771</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.053949</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>-0.005863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_min</th>\n",
       "      <td>0.053658</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>-0.002148</td>\n",
       "      <td>-0.002033</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>0.018856</td>\n",
       "      <td>-0.009818</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>-0.003146</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>0.020378</td>\n",
       "      <td>0.044156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_min</th>\n",
       "      <td>0.187519</td>\n",
       "      <td>-0.039873</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>-0.004807</td>\n",
       "      <td>-0.004753</td>\n",
       "      <td>-0.004821</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.005844</td>\n",
       "      <td>-0.020396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>-0.000877</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.032688</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.037032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_max</th>\n",
       "      <td>-0.093037</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>-0.028123</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>-0.026384</td>\n",
       "      <td>-0.026775</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019228</td>\n",
       "      <td>-0.032778</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.027707</td>\n",
       "      <td>-0.001734</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>-0.016580</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.017206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_max</th>\n",
       "      <td>-0.641035</td>\n",
       "      <td>0.124062</td>\n",
       "      <td>0.060417</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>-0.029203</td>\n",
       "      <td>0.040453</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.012579</td>\n",
       "      <td>-0.008825</td>\n",
       "      <td>-0.047707</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>-0.009081</td>\n",
       "      <td>-0.037994</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>-0.025784</td>\n",
       "      <td>0.012985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_max</th>\n",
       "      <td>-0.505892</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>-0.115268</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>-0.028051</td>\n",
       "      <td>-0.111212</td>\n",
       "      <td>-0.003972</td>\n",
       "      <td>0.111875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011946</td>\n",
       "      <td>-0.120579</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.046277</td>\n",
       "      <td>-0.027468</td>\n",
       "      <td>0.043056</td>\n",
       "      <td>-0.005047</td>\n",
       "      <td>-0.018589</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.058693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_min_avg</th>\n",
       "      <td>-0.171088</td>\n",
       "      <td>-0.010765</td>\n",
       "      <td>-0.029881</td>\n",
       "      <td>0.016062</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.015336</td>\n",
       "      <td>0.044773</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>0.087074</td>\n",
       "      <td>0.019808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>-0.028414</td>\n",
       "      <td>0.037507</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>-0.023609</td>\n",
       "      <td>0.040703</td>\n",
       "      <td>0.050671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_max_avg</th>\n",
       "      <td>-0.056568</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>-0.048522</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>0.091403</td>\n",
       "      <td>-0.049193</td>\n",
       "      <td>0.068658</td>\n",
       "      <td>0.025647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019942</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.028306</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>-0.010364</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.043212</td>\n",
       "      <td>-0.003757</td>\n",
       "      <td>0.055310</td>\n",
       "      <td>0.119248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <td>-0.175863</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>-0.101057</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.004358</td>\n",
       "      <td>0.113970</td>\n",
       "      <td>-0.066422</td>\n",
       "      <td>0.142477</td>\n",
       "      <td>0.074436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038974</td>\n",
       "      <td>-0.025256</td>\n",
       "      <td>-0.046174</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>-0.044430</td>\n",
       "      <td>0.088449</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>0.084475</td>\n",
       "      <td>0.176541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <td>-0.032486</td>\n",
       "      <td>-0.016239</td>\n",
       "      <td>-0.027554</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>-0.001701</td>\n",
       "      <td>-0.004777</td>\n",
       "      <td>-0.028707</td>\n",
       "      <td>0.011460</td>\n",
       "      <td>-0.005222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028709</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>-0.038870</td>\n",
       "      <td>-0.013826</td>\n",
       "      <td>-0.038404</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>-0.007016</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.044372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <td>-0.029940</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>-0.003191</td>\n",
       "      <td>0.065204</td>\n",
       "      <td>0.082080</td>\n",
       "      <td>0.050686</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.044143</td>\n",
       "      <td>-0.062038</td>\n",
       "      <td>-0.041191</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>0.041802</td>\n",
       "      <td>-0.008877</td>\n",
       "      <td>-0.010291</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.049706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <td>-0.035886</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>-0.020454</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.024371</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>0.028704</td>\n",
       "      <td>0.018201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>-0.056211</td>\n",
       "      <td>-0.027592</td>\n",
       "      <td>-0.041024</td>\n",
       "      <td>0.041550</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>-0.008375</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.057214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <td>0.007925</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>-0.006344</td>\n",
       "      <td>-0.006366</td>\n",
       "      <td>-0.006374</td>\n",
       "      <td>-0.028754</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016701</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>-0.006369</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>-0.012166</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.002465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <td>-0.022480</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>0.030420</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>-0.008003</td>\n",
       "      <td>-0.013999</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.007180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.019740</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>-0.005057</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>-0.014037</td>\n",
       "      <td>-0.016349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.011365</td>\n",
       "      <td>-0.024090</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>-0.006744</td>\n",
       "      <td>-0.025812</td>\n",
       "      <td>-0.022206</td>\n",
       "      <td>-0.021225</td>\n",
       "      <td>-0.008147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014974</td>\n",
       "      <td>-0.024390</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>-0.004549</td>\n",
       "      <td>-0.014489</td>\n",
       "      <td>0.020998</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.001515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <td>0.007744</td>\n",
       "      <td>-0.019434</td>\n",
       "      <td>-0.007859</td>\n",
       "      <td>-0.006807</td>\n",
       "      <td>-0.006813</td>\n",
       "      <td>-0.006855</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.011046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002578</td>\n",
       "      <td>-0.023033</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.008980</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>-0.021351</td>\n",
       "      <td>-0.009966</td>\n",
       "      <td>-0.016816</td>\n",
       "      <td>-0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <td>0.002065</td>\n",
       "      <td>-0.007648</td>\n",
       "      <td>-0.011274</td>\n",
       "      <td>-0.005696</td>\n",
       "      <td>-0.005913</td>\n",
       "      <td>-0.005624</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.036644</td>\n",
       "      <td>-0.032297</td>\n",
       "      <td>0.013644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>-0.015997</td>\n",
       "      <td>-0.014531</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>-0.017925</td>\n",
       "      <td>-0.015648</td>\n",
       "      <td>-0.011279</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>-0.007456</td>\n",
       "      <td>-0.009143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <td>-0.020008</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>0.052729</td>\n",
       "      <td>-0.004192</td>\n",
       "      <td>-0.003740</td>\n",
       "      <td>-0.004252</td>\n",
       "      <td>0.051606</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.049946</td>\n",
       "      <td>-0.009968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030335</td>\n",
       "      <td>0.034436</td>\n",
       "      <td>-0.015193</td>\n",
       "      <td>-0.039330</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>-0.019795</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.032922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <td>0.024860</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>-0.003620</td>\n",
       "      <td>-0.003468</td>\n",
       "      <td>-0.003573</td>\n",
       "      <td>0.048404</td>\n",
       "      <td>0.033236</td>\n",
       "      <td>0.036759</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033614</td>\n",
       "      <td>0.043947</td>\n",
       "      <td>-0.024792</td>\n",
       "      <td>-0.043346</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.019801</td>\n",
       "      <td>0.023452</td>\n",
       "      <td>-0.015683</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.022785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_weekend</th>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.055945</td>\n",
       "      <td>-0.005727</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>-0.005736</td>\n",
       "      <td>0.073352</td>\n",
       "      <td>0.044309</td>\n",
       "      <td>0.063529</td>\n",
       "      <td>-0.015040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.057563</td>\n",
       "      <td>-0.029394</td>\n",
       "      <td>-0.060680</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.024966</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>-0.026003</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>0.040807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_00</th>\n",
       "      <td>0.083775</td>\n",
       "      <td>-0.082722</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>-0.009796</td>\n",
       "      <td>-0.009426</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>-0.020900</td>\n",
       "      <td>-0.033404</td>\n",
       "      <td>-0.154152</td>\n",
       "      <td>-0.069660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105383</td>\n",
       "      <td>0.036618</td>\n",
       "      <td>0.057796</td>\n",
       "      <td>0.065765</td>\n",
       "      <td>-0.010934</td>\n",
       "      <td>-0.047871</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>-0.028934</td>\n",
       "      <td>-0.013572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_01</th>\n",
       "      <td>0.014188</td>\n",
       "      <td>0.066882</td>\n",
       "      <td>-0.009765</td>\n",
       "      <td>-0.008228</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.008341</td>\n",
       "      <td>-0.045997</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.051335</td>\n",
       "      <td>0.016424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023996</td>\n",
       "      <td>0.054408</td>\n",
       "      <td>-0.112610</td>\n",
       "      <td>-0.065220</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>0.057648</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>-0.082134</td>\n",
       "      <td>0.041141</td>\n",
       "      <td>-0.021573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_02</th>\n",
       "      <td>-0.140516</td>\n",
       "      <td>0.042068</td>\n",
       "      <td>0.086241</td>\n",
       "      <td>-0.011688</td>\n",
       "      <td>-0.010425</td>\n",
       "      <td>-0.010902</td>\n",
       "      <td>-0.011716</td>\n",
       "      <td>-0.083911</td>\n",
       "      <td>-0.107091</td>\n",
       "      <td>-0.085427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038408</td>\n",
       "      <td>-0.092270</td>\n",
       "      <td>0.014754</td>\n",
       "      <td>-0.095213</td>\n",
       "      <td>0.079385</td>\n",
       "      <td>-0.067029</td>\n",
       "      <td>-0.065236</td>\n",
       "      <td>0.068938</td>\n",
       "      <td>-0.061856</td>\n",
       "      <td>-0.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_03</th>\n",
       "      <td>-0.046638</td>\n",
       "      <td>0.048605</td>\n",
       "      <td>-0.156419</td>\n",
       "      <td>-0.010488</td>\n",
       "      <td>-0.012899</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>0.094172</td>\n",
       "      <td>-0.076106</td>\n",
       "      <td>0.207916</td>\n",
       "      <td>0.262369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097537</td>\n",
       "      <td>-0.031902</td>\n",
       "      <td>-0.103050</td>\n",
       "      <td>-0.022575</td>\n",
       "      <td>-0.078399</td>\n",
       "      <td>0.119949</td>\n",
       "      <td>0.034636</td>\n",
       "      <td>-0.031529</td>\n",
       "      <td>0.107354</td>\n",
       "      <td>0.122088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA_04</th>\n",
       "      <td>0.097435</td>\n",
       "      <td>-0.062409</td>\n",
       "      <td>0.065851</td>\n",
       "      <td>-0.011662</td>\n",
       "      <td>-0.010392</td>\n",
       "      <td>-0.010982</td>\n",
       "      <td>-0.030078</td>\n",
       "      <td>0.179745</td>\n",
       "      <td>-0.004006</td>\n",
       "      <td>-0.127112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019526</td>\n",
       "      <td>0.050796</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>0.101666</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>-0.050114</td>\n",
       "      <td>-0.028306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_subjectivity</th>\n",
       "      <td>0.151879</td>\n",
       "      <td>-0.055111</td>\n",
       "      <td>0.133345</td>\n",
       "      <td>-0.047001</td>\n",
       "      <td>-0.045838</td>\n",
       "      <td>-0.044899</td>\n",
       "      <td>0.205969</td>\n",
       "      <td>0.127944</td>\n",
       "      <td>0.097084</td>\n",
       "      <td>0.064576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>0.535157</td>\n",
       "      <td>-0.445317</td>\n",
       "      <td>-0.343187</td>\n",
       "      <td>-0.238871</td>\n",
       "      <td>0.119017</td>\n",
       "      <td>0.026397</td>\n",
       "      <td>-0.004305</td>\n",
       "      <td>0.095193</td>\n",
       "      <td>0.043230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <td>0.152291</td>\n",
       "      <td>-0.063163</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>-0.015440</td>\n",
       "      <td>-0.015289</td>\n",
       "      <td>-0.015022</td>\n",
       "      <td>0.092287</td>\n",
       "      <td>0.105212</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>-0.031491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.421433</td>\n",
       "      <td>0.223395</td>\n",
       "      <td>0.294983</td>\n",
       "      <td>-0.065863</td>\n",
       "      <td>0.023185</td>\n",
       "      <td>0.215099</td>\n",
       "      <td>-0.012337</td>\n",
       "      <td>0.076423</td>\n",
       "      <td>0.019364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <td>0.206725</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>0.129404</td>\n",
       "      <td>-0.028309</td>\n",
       "      <td>-0.027668</td>\n",
       "      <td>-0.026746</td>\n",
       "      <td>0.074390</td>\n",
       "      <td>0.138732</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.075777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113016</td>\n",
       "      <td>0.477197</td>\n",
       "      <td>-0.134261</td>\n",
       "      <td>-0.103099</td>\n",
       "      <td>-0.069972</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>0.127186</td>\n",
       "      <td>-0.130807</td>\n",
       "      <td>0.115170</td>\n",
       "      <td>0.010702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.014770</td>\n",
       "      <td>0.121222</td>\n",
       "      <td>-0.019376</td>\n",
       "      <td>-0.018706</td>\n",
       "      <td>-0.018127</td>\n",
       "      <td>0.037218</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.186342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062312</td>\n",
       "      <td>0.220780</td>\n",
       "      <td>-0.356421</td>\n",
       "      <td>-0.485794</td>\n",
       "      <td>0.077871</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>-0.111294</td>\n",
       "      <td>-0.074649</td>\n",
       "      <td>0.068923</td>\n",
       "      <td>0.009240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_positive_words</th>\n",
       "      <td>0.204920</td>\n",
       "      <td>-0.073556</td>\n",
       "      <td>0.102779</td>\n",
       "      <td>-0.044875</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>-0.042799</td>\n",
       "      <td>0.114514</td>\n",
       "      <td>0.151534</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>-0.046545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042277</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>-0.041965</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>-0.212130</td>\n",
       "      <td>-0.018060</td>\n",
       "      <td>0.124610</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>-0.020869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate_negative_words</th>\n",
       "      <td>-0.057985</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>0.104934</td>\n",
       "      <td>-0.023657</td>\n",
       "      <td>-0.022625</td>\n",
       "      <td>-0.022279</td>\n",
       "      <td>0.052057</td>\n",
       "      <td>-0.013974</td>\n",
       "      <td>0.017767</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212715</td>\n",
       "      <td>0.070767</td>\n",
       "      <td>-0.358567</td>\n",
       "      <td>-0.476291</td>\n",
       "      <td>0.029209</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>-0.166958</td>\n",
       "      <td>0.028734</td>\n",
       "      <td>-0.019408</td>\n",
       "      <td>-0.014646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <td>0.135051</td>\n",
       "      <td>-0.046673</td>\n",
       "      <td>0.130090</td>\n",
       "      <td>-0.041458</td>\n",
       "      <td>-0.040515</td>\n",
       "      <td>-0.039629</td>\n",
       "      <td>0.189887</td>\n",
       "      <td>0.109214</td>\n",
       "      <td>0.097903</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464627</td>\n",
       "      <td>0.700916</td>\n",
       "      <td>-0.286520</td>\n",
       "      <td>-0.235143</td>\n",
       "      <td>-0.159296</td>\n",
       "      <td>0.062933</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>0.007518</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.020892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <td>0.078065</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.259488</td>\n",
       "      <td>-0.012719</td>\n",
       "      <td>-0.015892</td>\n",
       "      <td>-0.012904</td>\n",
       "      <td>-0.094226</td>\n",
       "      <td>-0.062645</td>\n",
       "      <td>-0.037248</td>\n",
       "      <td>-0.012927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>-0.060462</td>\n",
       "      <td>0.079207</td>\n",
       "      <td>-0.177367</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.003991</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.014251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <td>0.107565</td>\n",
       "      <td>-0.020748</td>\n",
       "      <td>0.410739</td>\n",
       "      <td>-0.040915</td>\n",
       "      <td>-0.036938</td>\n",
       "      <td>-0.038530</td>\n",
       "      <td>0.301379</td>\n",
       "      <td>0.204620</td>\n",
       "      <td>0.175201</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.285687</td>\n",
       "      <td>-0.357855</td>\n",
       "      <td>-0.024620</td>\n",
       "      <td>0.047715</td>\n",
       "      <td>0.057378</td>\n",
       "      <td>-0.009577</td>\n",
       "      <td>0.076947</td>\n",
       "      <td>-0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <td>-0.027320</td>\n",
       "      <td>-0.002784</td>\n",
       "      <td>-0.127551</td>\n",
       "      <td>0.025734</td>\n",
       "      <td>0.024621</td>\n",
       "      <td>0.024402</td>\n",
       "      <td>-0.154175</td>\n",
       "      <td>-0.052529</td>\n",
       "      <td>-0.074474</td>\n",
       "      <td>-0.117901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060462</td>\n",
       "      <td>-0.285687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752729</td>\n",
       "      <td>0.569063</td>\n",
       "      <td>-0.085575</td>\n",
       "      <td>0.077411</td>\n",
       "      <td>0.016855</td>\n",
       "      <td>-0.077484</td>\n",
       "      <td>-0.036185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <td>0.055668</td>\n",
       "      <td>-0.026924</td>\n",
       "      <td>-0.432253</td>\n",
       "      <td>0.025433</td>\n",
       "      <td>0.021593</td>\n",
       "      <td>0.023428</td>\n",
       "      <td>-0.264165</td>\n",
       "      <td>-0.100594</td>\n",
       "      <td>-0.135062</td>\n",
       "      <td>-0.140754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079207</td>\n",
       "      <td>-0.357855</td>\n",
       "      <td>0.752729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083448</td>\n",
       "      <td>-0.065263</td>\n",
       "      <td>0.066106</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>-0.056011</td>\n",
       "      <td>-0.022202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <td>-0.085431</td>\n",
       "      <td>0.024747</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.050412</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>0.037679</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177367</td>\n",
       "      <td>-0.024620</td>\n",
       "      <td>0.569063</td>\n",
       "      <td>0.083448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027888</td>\n",
       "      <td>-0.003542</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>-0.033328</td>\n",
       "      <td>-0.033918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_subjectivity</th>\n",
       "      <td>-0.011182</td>\n",
       "      <td>0.083849</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>-0.012074</td>\n",
       "      <td>-0.012317</td>\n",
       "      <td>-0.012543</td>\n",
       "      <td>0.047865</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>0.062114</td>\n",
       "      <td>0.081046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.047715</td>\n",
       "      <td>-0.085575</td>\n",
       "      <td>-0.065263</td>\n",
       "      <td>-0.027888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.231207</td>\n",
       "      <td>-0.483577</td>\n",
       "      <td>0.725020</td>\n",
       "      <td>0.046853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>-0.003994</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.037257</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003991</td>\n",
       "      <td>0.057378</td>\n",
       "      <td>0.077411</td>\n",
       "      <td>0.066106</td>\n",
       "      <td>-0.003542</td>\n",
       "      <td>0.231207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.249742</td>\n",
       "      <td>0.399680</td>\n",
       "      <td>0.039401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <td>0.015111</td>\n",
       "      <td>-0.150208</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>-0.026061</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>-0.025816</td>\n",
       "      <td>-0.001619</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>-0.017980</td>\n",
       "      <td>-0.050267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>-0.009577</td>\n",
       "      <td>0.016855</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>-0.483577</td>\n",
       "      <td>-0.249742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.413649</td>\n",
       "      <td>-0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <td>0.010796</td>\n",
       "      <td>0.054882</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.009672</td>\n",
       "      <td>-0.009818</td>\n",
       "      <td>-0.010055</td>\n",
       "      <td>0.058085</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.069438</td>\n",
       "      <td>0.058370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.076947</td>\n",
       "      <td>-0.077484</td>\n",
       "      <td>-0.056011</td>\n",
       "      <td>-0.033328</td>\n",
       "      <td>0.725020</td>\n",
       "      <td>0.399680</td>\n",
       "      <td>-0.413649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shares</th>\n",
       "      <td>-0.011844</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>-0.025070</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.032497</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>0.063019</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014251</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.036185</td>\n",
       "      <td>-0.022202</td>\n",
       "      <td>-0.033918</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>0.054514</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               timedelta  n_tokens_title  n_tokens_content  \\\n",
       "timedelta                       1.000000       -0.243861         -0.062606   \n",
       "n_tokens_title                 -0.243861        1.000000          0.016624   \n",
       "n_tokens_content               -0.062606        0.016624          1.000000   \n",
       "n_unique_tokens                -0.011415       -0.010082          0.025534   \n",
       "n_non_stop_words               -0.012354       -0.009861          0.033500   \n",
       "n_non_stop_unique_tokens       -0.011096       -0.010093          0.027342   \n",
       "num_hrefs                       0.001155       -0.044708          0.453081   \n",
       "num_self_hrefs                  0.084135       -0.005632          0.319458   \n",
       "num_imgs                       -0.001696       -0.020840          0.365091   \n",
       "num_videos                     -0.002302        0.070137          0.085006   \n",
       "average_token_length            0.139347       -0.077376          0.177912   \n",
       "num_keywords                    0.049123        0.013567          0.087727   \n",
       "data_channel_is_lifestyle       0.073851       -0.063675          0.042877   \n",
       "data_channel_is_entertainment  -0.056924        0.143992          0.056494   \n",
       "data_channel_is_bus             0.059449       -0.037195         -0.020974   \n",
       "data_channel_is_socmed          0.067716       -0.094391          0.035741   \n",
       "data_channel_is_tech            0.084291       -0.037074          0.038588   \n",
       "data_channel_is_world          -0.169721        0.044960          0.065606   \n",
       "kw_min_min                      0.592796       -0.111593         -0.052682   \n",
       "kw_max_min                      0.053658       -0.006361          0.005217   \n",
       "kw_avg_min                      0.187519       -0.039873         -0.000839   \n",
       "kw_min_max                     -0.093037        0.004555         -0.028123   \n",
       "kw_max_max                     -0.641035        0.124062          0.060417   \n",
       "kw_avg_max                     -0.505892        0.114790         -0.115268   \n",
       "kw_min_avg                     -0.171088       -0.010765         -0.029881   \n",
       "kw_max_avg                     -0.056568        0.011405         -0.048522   \n",
       "kw_avg_avg                     -0.175863        0.000996         -0.101057   \n",
       "self_reference_min_shares      -0.032486       -0.016239         -0.027554   \n",
       "self_reference_max_shares      -0.029940        0.019554          0.008892   \n",
       "self_reference_avg_sharess     -0.035886        0.006658         -0.020454   \n",
       "weekday_is_monday               0.007925        0.000763         -0.006534   \n",
       "weekday_is_tuesday             -0.022480        0.006224         -0.000232   \n",
       "weekday_is_wednesday            0.001402        0.011365         -0.024090   \n",
       "weekday_is_thursday             0.007744       -0.019434         -0.007859   \n",
       "weekday_is_friday               0.002065       -0.007648         -0.011274   \n",
       "weekday_is_saturday            -0.020008       -0.017650          0.052729   \n",
       "weekday_is_sunday               0.024860        0.030070          0.023770   \n",
       "is_weekend                      0.003836        0.009406          0.055945   \n",
       "LDA_00                          0.083775       -0.082722          0.013160   \n",
       "LDA_01                          0.014188        0.066882         -0.009765   \n",
       "LDA_02                         -0.140516        0.042068          0.086241   \n",
       "LDA_03                         -0.046638        0.048605         -0.156419   \n",
       "LDA_04                          0.097435       -0.062409          0.065851   \n",
       "global_subjectivity             0.151879       -0.055111          0.133345   \n",
       "global_sentiment_polarity       0.152291       -0.063163          0.022357   \n",
       "global_rate_positive_words      0.206725       -0.074218          0.129404   \n",
       "global_rate_negative_words      0.019191        0.014770          0.121222   \n",
       "rate_positive_words             0.204920       -0.073556          0.102779   \n",
       "rate_negative_words            -0.057985        0.028194          0.104934   \n",
       "avg_positive_polarity           0.135051       -0.046673          0.130090   \n",
       "min_positive_polarity           0.078065       -0.042524         -0.259488   \n",
       "max_positive_polarity           0.107565       -0.020748          0.410739   \n",
       "avg_negative_polarity          -0.027320       -0.002784         -0.127551   \n",
       "min_negative_polarity           0.055668       -0.026924         -0.432253   \n",
       "max_negative_polarity          -0.085431        0.024747          0.230331   \n",
       "title_subjectivity             -0.011182        0.083849          0.011605   \n",
       "title_sentiment_polarity        0.053576        0.003470          0.008783   \n",
       "abs_title_subjectivity          0.015111       -0.150208          0.006558   \n",
       "abs_title_sentiment_polarity    0.010796        0.054882          0.003933   \n",
       "shares                         -0.011844        0.010694         -0.025070   \n",
       "\n",
       "                               n_unique_tokens  n_non_stop_words  \\\n",
       "timedelta                            -0.011415         -0.012354   \n",
       "n_tokens_title                       -0.010082         -0.009861   \n",
       "n_tokens_content                      0.025534          0.033500   \n",
       "n_unique_tokens                       1.000000          0.999946   \n",
       "n_non_stop_words                      0.999946          1.000000   \n",
       "n_non_stop_unique_tokens              0.999981          0.999942   \n",
       "num_hrefs                            -0.001368          0.002370   \n",
       "num_self_hrefs                        0.024778          0.027346   \n",
       "num_imgs                              0.076078          0.079556   \n",
       "num_videos                           -0.003933         -0.003970   \n",
       "average_token_length                  0.011638          0.013560   \n",
       "num_keywords                         -0.003007         -0.002120   \n",
       "data_channel_is_lifestyle            -0.003802         -0.003226   \n",
       "data_channel_is_entertainment         0.031645          0.031753   \n",
       "data_channel_is_bus                  -0.005685         -0.005716   \n",
       "data_channel_is_socmed               -0.003626         -0.003223   \n",
       "data_channel_is_tech                 -0.006599         -0.005782   \n",
       "data_channel_is_world                -0.008245         -0.007173   \n",
       "kw_min_min                           -0.004466         -0.005019   \n",
       "kw_max_min                           -0.002148         -0.002033   \n",
       "kw_avg_min                           -0.004807         -0.004753   \n",
       "kw_min_max                            0.001523          0.001183   \n",
       "kw_max_max                            0.004720          0.005467   \n",
       "kw_avg_max                            0.006708          0.005210   \n",
       "kw_min_avg                            0.016062          0.015525   \n",
       "kw_max_avg                           -0.005776         -0.006365   \n",
       "kw_avg_avg                           -0.002835         -0.004214   \n",
       "self_reference_min_shares            -0.001632         -0.001817   \n",
       "self_reference_max_shares            -0.003121         -0.003103   \n",
       "self_reference_avg_sharess            0.001035          0.000875   \n",
       "weekday_is_monday                    -0.006344         -0.006366   \n",
       "weekday_is_tuesday                    0.030420          0.030440   \n",
       "weekday_is_wednesday                 -0.006698         -0.006876   \n",
       "weekday_is_thursday                  -0.006807         -0.006813   \n",
       "weekday_is_friday                    -0.005696         -0.005913   \n",
       "weekday_is_saturday                  -0.004192         -0.003740   \n",
       "weekday_is_sunday                    -0.003620         -0.003468   \n",
       "is_weekend                           -0.005727         -0.005287   \n",
       "LDA_00                               -0.009796         -0.009426   \n",
       "LDA_01                               -0.008228         -0.008820   \n",
       "LDA_02                               -0.011688         -0.010425   \n",
       "LDA_03                               -0.010488         -0.012899   \n",
       "LDA_04                               -0.011662         -0.010392   \n",
       "global_subjectivity                  -0.047001         -0.045838   \n",
       "global_sentiment_polarity            -0.015440         -0.015289   \n",
       "global_rate_positive_words           -0.028309         -0.027668   \n",
       "global_rate_negative_words           -0.019376         -0.018706   \n",
       "rate_positive_words                  -0.044875         -0.043682   \n",
       "rate_negative_words                  -0.023657         -0.022625   \n",
       "avg_positive_polarity                -0.041458         -0.040515   \n",
       "min_positive_polarity                -0.012719         -0.015892   \n",
       "max_positive_polarity                -0.040915         -0.036938   \n",
       "avg_negative_polarity                 0.025734          0.024621   \n",
       "min_negative_polarity                 0.025433          0.021593   \n",
       "max_negative_polarity                 0.011709          0.013944   \n",
       "title_subjectivity                   -0.012074         -0.012317   \n",
       "title_sentiment_polarity             -0.003994         -0.003886   \n",
       "abs_title_subjectivity               -0.026061         -0.025810   \n",
       "abs_title_sentiment_polarity         -0.009672         -0.009818   \n",
       "shares                                0.004472          0.004003   \n",
       "\n",
       "                               n_non_stop_unique_tokens  num_hrefs  \\\n",
       "timedelta                                     -0.011096   0.001155   \n",
       "n_tokens_title                                -0.010093  -0.044708   \n",
       "n_tokens_content                               0.027342   0.453081   \n",
       "n_unique_tokens                                0.999981  -0.001368   \n",
       "n_non_stop_words                               0.999942   0.002370   \n",
       "n_non_stop_unique_tokens                       1.000000  -0.001454   \n",
       "num_hrefs                                     -0.001454   1.000000   \n",
       "num_self_hrefs                                 0.025153   0.374062   \n",
       "num_imgs                                       0.074568   0.373871   \n",
       "num_videos                                    -0.004042   0.096269   \n",
       "average_token_length                           0.014498   0.220437   \n",
       "num_keywords                                  -0.003229   0.140526   \n",
       "data_channel_is_lifestyle                     -0.003469   0.053029   \n",
       "data_channel_is_entertainment                  0.031544  -0.004510   \n",
       "data_channel_is_bus                           -0.005009  -0.069959   \n",
       "data_channel_is_socmed                        -0.003560   0.053698   \n",
       "data_channel_is_tech                          -0.006146  -0.044176   \n",
       "data_channel_is_world                         -0.007495  -0.021516   \n",
       "kw_min_min                                    -0.004356  -0.053208   \n",
       "kw_max_min                                    -0.002277   0.018856   \n",
       "kw_avg_min                                    -0.004821   0.014536   \n",
       "kw_min_max                                     0.001116  -0.026384   \n",
       "kw_max_max                                     0.004634   0.058883   \n",
       "kw_avg_max                                     0.005561  -0.028051   \n",
       "kw_min_avg                                     0.015336   0.044773   \n",
       "kw_max_avg                                    -0.006512   0.091403   \n",
       "kw_avg_avg                                    -0.004358   0.113970   \n",
       "self_reference_min_shares                     -0.001701  -0.004777   \n",
       "self_reference_max_shares                     -0.003191   0.065204   \n",
       "self_reference_avg_sharess                     0.000940   0.024371   \n",
       "weekday_is_monday                             -0.006374  -0.028754   \n",
       "weekday_is_tuesday                             0.030488  -0.008003   \n",
       "weekday_is_wednesday                          -0.006744  -0.025812   \n",
       "weekday_is_thursday                           -0.006855   0.002031   \n",
       "weekday_is_friday                             -0.005624  -0.005009   \n",
       "weekday_is_saturday                           -0.004252   0.051606   \n",
       "weekday_is_sunday                             -0.003573   0.048404   \n",
       "is_weekend                                    -0.005736   0.073352   \n",
       "LDA_00                                        -0.008928  -0.020900   \n",
       "LDA_01                                        -0.008341  -0.045997   \n",
       "LDA_02                                        -0.010902  -0.011716   \n",
       "LDA_03                                        -0.012643   0.094172   \n",
       "LDA_04                                        -0.010982  -0.030078   \n",
       "global_subjectivity                           -0.044899   0.205969   \n",
       "global_sentiment_polarity                     -0.015022   0.092287   \n",
       "global_rate_positive_words                    -0.026746   0.074390   \n",
       "global_rate_negative_words                    -0.018127   0.037218   \n",
       "rate_positive_words                           -0.042799   0.114514   \n",
       "rate_negative_words                           -0.022279   0.052057   \n",
       "avg_positive_polarity                         -0.039629   0.189887   \n",
       "min_positive_polarity                         -0.012904  -0.094226   \n",
       "max_positive_polarity                         -0.038530   0.301379   \n",
       "avg_negative_polarity                          0.024402  -0.154175   \n",
       "min_negative_polarity                          0.023428  -0.264165   \n",
       "max_negative_polarity                          0.011911   0.050412   \n",
       "title_subjectivity                            -0.012543   0.047865   \n",
       "title_sentiment_polarity                      -0.004321   0.037257   \n",
       "abs_title_subjectivity                        -0.025816  -0.001619   \n",
       "abs_title_sentiment_polarity                  -0.010055   0.058085   \n",
       "shares                                         0.004024   0.032497   \n",
       "\n",
       "                               num_self_hrefs  num_imgs  num_videos  ...  \\\n",
       "timedelta                            0.084135 -0.001696   -0.002302  ...   \n",
       "n_tokens_title                      -0.005632 -0.020840    0.070137  ...   \n",
       "n_tokens_content                     0.319458  0.365091    0.085006  ...   \n",
       "n_unique_tokens                      0.024778  0.076078   -0.003933  ...   \n",
       "n_non_stop_words                     0.027346  0.079556   -0.003970  ...   \n",
       "n_non_stop_unique_tokens             0.025153  0.074568   -0.004042  ...   \n",
       "num_hrefs                            0.374062  0.373871    0.096269  ...   \n",
       "num_self_hrefs                       1.000000  0.281426    0.027783  ...   \n",
       "num_imgs                             0.281426  1.000000   -0.071728  ...   \n",
       "num_videos                           0.027783 -0.071728    1.000000  ...   \n",
       "average_token_length                 0.131153  0.056121   -0.002076  ...   \n",
       "num_keywords                         0.125918  0.092720   -0.013286  ...   \n",
       "data_channel_is_lifestyle           -0.032071  0.015586   -0.048291  ...   \n",
       "data_channel_is_entertainment        0.019640  0.095384    0.173410  ...   \n",
       "data_channel_is_bus                 -0.046711 -0.149137   -0.054609  ...   \n",
       "data_channel_is_socmed               0.059030 -0.004338   -0.029995  ...   \n",
       "data_channel_is_tech                 0.195332  0.012467   -0.084695  ...   \n",
       "data_channel_is_world               -0.112074 -0.121207   -0.071202  ...   \n",
       "kw_min_min                          -0.002237 -0.024425   -0.003576  ...   \n",
       "kw_max_min                          -0.009818  0.011960   -0.013592  ...   \n",
       "kw_avg_min                          -0.004974  0.005844   -0.020396  ...   \n",
       "kw_min_max                          -0.026775  0.015407    0.010699  ...   \n",
       "kw_max_max                          -0.029203  0.040453   -0.002153  ...   \n",
       "kw_avg_max                          -0.111212 -0.003972    0.111875  ...   \n",
       "kw_min_avg                           0.013978  0.087074    0.019808  ...   \n",
       "kw_max_avg                          -0.049193  0.068658    0.025647  ...   \n",
       "kw_avg_avg                          -0.066422  0.142477    0.074436  ...   \n",
       "self_reference_min_shares           -0.028707  0.011460   -0.005222  ...   \n",
       "self_reference_max_shares            0.082080  0.050686    0.035499  ...   \n",
       "self_reference_avg_sharess           0.008118  0.028704    0.018201  ...   \n",
       "weekday_is_monday                    0.034262 -0.001236    0.005043  ...   \n",
       "weekday_is_tuesday                  -0.013999 -0.003689   -0.007180  ...   \n",
       "weekday_is_wednesday                -0.022206 -0.021225   -0.008147  ...   \n",
       "weekday_is_thursday                 -0.002474 -0.000027    0.011046  ...   \n",
       "weekday_is_friday                   -0.036644 -0.032297    0.013644  ...   \n",
       "weekday_is_saturday                  0.027108  0.049946   -0.009968  ...   \n",
       "weekday_is_sunday                    0.033236  0.036759   -0.010527  ...   \n",
       "is_weekend                           0.044309  0.063529   -0.015040  ...   \n",
       "LDA_00                              -0.033404 -0.154152   -0.069660  ...   \n",
       "LDA_01                               0.010004  0.051335    0.016424  ...   \n",
       "LDA_02                              -0.083911 -0.107091   -0.085427  ...   \n",
       "LDA_03                              -0.076106  0.207916    0.262369  ...   \n",
       "LDA_04                               0.179745 -0.004006   -0.127112  ...   \n",
       "global_subjectivity                  0.127944  0.097084    0.064576  ...   \n",
       "global_sentiment_polarity            0.105212  0.050336   -0.031491  ...   \n",
       "global_rate_positive_words           0.138732  0.008327    0.075777  ...   \n",
       "global_rate_negative_words           0.012044  0.021736    0.186342  ...   \n",
       "rate_positive_words                  0.151534  0.015543   -0.046545  ...   \n",
       "rate_negative_words                 -0.013974  0.017767    0.076547  ...   \n",
       "avg_positive_polarity                0.109214  0.097903    0.094400  ...   \n",
       "min_positive_polarity               -0.062645 -0.037248   -0.012927  ...   \n",
       "max_positive_polarity                0.204620  0.175201    0.116689  ...   \n",
       "avg_negative_polarity               -0.052529 -0.074474   -0.117901  ...   \n",
       "min_negative_polarity               -0.100594 -0.135062   -0.140754  ...   \n",
       "max_negative_polarity                0.032277  0.037679    0.026423  ...   \n",
       "title_subjectivity                  -0.002725  0.062114    0.081046  ...   \n",
       "title_sentiment_polarity             0.026615  0.047269    0.029413  ...   \n",
       "abs_title_subjectivity              -0.005920 -0.017980   -0.050267  ...   \n",
       "abs_title_sentiment_polarity         0.004348  0.069438    0.058370  ...   \n",
       "shares                              -0.009001  0.063019    0.015876  ...   \n",
       "\n",
       "                               min_positive_polarity  max_positive_polarity  \\\n",
       "timedelta                                   0.078065               0.107565   \n",
       "n_tokens_title                             -0.042524              -0.020748   \n",
       "n_tokens_content                           -0.259488               0.410739   \n",
       "n_unique_tokens                            -0.012719              -0.040915   \n",
       "n_non_stop_words                           -0.015892              -0.036938   \n",
       "n_non_stop_unique_tokens                   -0.012904              -0.038530   \n",
       "num_hrefs                                  -0.094226               0.301379   \n",
       "num_self_hrefs                             -0.062645               0.204620   \n",
       "num_imgs                                   -0.037248               0.175201   \n",
       "num_videos                                 -0.012927               0.116689   \n",
       "average_token_length                        0.209098               0.482247   \n",
       "num_keywords                               -0.020139               0.067138   \n",
       "data_channel_is_lifestyle                  -0.014450               0.056676   \n",
       "data_channel_is_entertainment              -0.017896               0.097288   \n",
       "data_channel_is_bus                        -0.052687               0.010076   \n",
       "data_channel_is_socmed                     -0.070476               0.011733   \n",
       "data_channel_is_tech                        0.030435               0.035468   \n",
       "data_channel_is_world                      -0.033642              -0.110580   \n",
       "kw_min_min                                  0.035626               0.007985   \n",
       "kw_max_min                                  0.008265              -0.000661   \n",
       "kw_avg_min                                  0.017171               0.013902   \n",
       "kw_min_max                                 -0.019228              -0.032778   \n",
       "kw_max_max                                 -0.050984              -0.012579   \n",
       "kw_avg_max                                 -0.011946              -0.120579   \n",
       "kw_min_avg                                 -0.001514              -0.023957   \n",
       "kw_max_avg                                  0.019942              -0.001457   \n",
       "kw_avg_avg                                  0.038974              -0.025256   \n",
       "self_reference_min_shares                   0.028709              -0.000892   \n",
       "self_reference_max_shares                   0.017616               0.044143   \n",
       "self_reference_avg_sharess                  0.028198               0.021960   \n",
       "weekday_is_monday                          -0.016701               0.008757   \n",
       "weekday_is_tuesday                         -0.002432               0.003483   \n",
       "weekday_is_wednesday                        0.014974              -0.024390   \n",
       "weekday_is_thursday                        -0.002578              -0.023033   \n",
       "weekday_is_friday                           0.004005              -0.015997   \n",
       "weekday_is_saturday                        -0.030335               0.034436   \n",
       "weekday_is_sunday                           0.033614               0.043947   \n",
       "is_weekend                                  0.002799               0.057563   \n",
       "LDA_00                                     -0.105383               0.036618   \n",
       "LDA_01                                      0.023996               0.054408   \n",
       "LDA_02                                     -0.038408              -0.092270   \n",
       "LDA_03                                      0.097537              -0.031902   \n",
       "LDA_04                                      0.019526               0.050796   \n",
       "global_subjectivity                         0.214203               0.535157   \n",
       "global_sentiment_polarity                   0.081169               0.421433   \n",
       "global_rate_positive_words                 -0.113016               0.477197   \n",
       "global_rate_negative_words                  0.062312               0.220780   \n",
       "rate_positive_words                         0.042277               0.441815   \n",
       "rate_negative_words                         0.212715               0.070767   \n",
       "avg_positive_polarity                       0.464627               0.700916   \n",
       "min_positive_polarity                       1.000000               0.014505   \n",
       "max_positive_polarity                       0.014505               1.000000   \n",
       "avg_negative_polarity                      -0.060462              -0.285687   \n",
       "min_negative_polarity                       0.079207              -0.357855   \n",
       "max_negative_polarity                      -0.177367              -0.024620   \n",
       "title_subjectivity                          0.000150               0.047715   \n",
       "title_sentiment_polarity                   -0.003991               0.057378   \n",
       "abs_title_subjectivity                      0.019403              -0.009577   \n",
       "abs_title_sentiment_polarity                0.009601               0.076947   \n",
       "shares                                      0.014251              -0.000914   \n",
       "\n",
       "                               avg_negative_polarity  min_negative_polarity  \\\n",
       "timedelta                                  -0.027320               0.055668   \n",
       "n_tokens_title                             -0.002784              -0.026924   \n",
       "n_tokens_content                           -0.127551              -0.432253   \n",
       "n_unique_tokens                             0.025734               0.025433   \n",
       "n_non_stop_words                            0.024621               0.021593   \n",
       "n_non_stop_unique_tokens                    0.024402               0.023428   \n",
       "num_hrefs                                  -0.154175              -0.264165   \n",
       "num_self_hrefs                             -0.052529              -0.100594   \n",
       "num_imgs                                   -0.074474              -0.135062   \n",
       "num_videos                                 -0.117901              -0.140754   \n",
       "average_token_length                       -0.342789              -0.299359   \n",
       "num_keywords                               -0.002142              -0.021641   \n",
       "data_channel_is_lifestyle                  -0.002905              -0.026245   \n",
       "data_channel_is_entertainment              -0.144261              -0.126649   \n",
       "data_channel_is_bus                         0.081754               0.090965   \n",
       "data_channel_is_socmed                      0.003356               0.020255   \n",
       "data_channel_is_tech                        0.104738               0.119615   \n",
       "data_channel_is_world                       0.012025              -0.097589   \n",
       "kw_min_min                                  0.005516               0.045612   \n",
       "kw_max_min                                  0.006616              -0.003146   \n",
       "kw_avg_min                                  0.004788               0.004016   \n",
       "kw_min_max                                  0.020695               0.027707   \n",
       "kw_max_max                                 -0.008825              -0.047707   \n",
       "kw_avg_max                                  0.014496               0.046277   \n",
       "kw_min_avg                                 -0.013320               0.005647   \n",
       "kw_max_avg                                 -0.028306              -0.002949   \n",
       "kw_avg_avg                                 -0.046174               0.009435   \n",
       "self_reference_min_shares                  -0.038870              -0.013826   \n",
       "self_reference_max_shares                  -0.062038              -0.041191   \n",
       "self_reference_avg_sharess                 -0.056211              -0.027592   \n",
       "weekday_is_monday                           0.012601               0.014489   \n",
       "weekday_is_tuesday                          0.019740               0.025179   \n",
       "weekday_is_wednesday                        0.002977               0.013882   \n",
       "weekday_is_thursday                         0.004305              -0.001305   \n",
       "weekday_is_friday                          -0.014531               0.001816   \n",
       "weekday_is_saturday                        -0.015193              -0.039330   \n",
       "weekday_is_sunday                          -0.024792              -0.043346   \n",
       "is_weekend                                 -0.029394              -0.060680   \n",
       "LDA_00                                      0.057796               0.065765   \n",
       "LDA_01                                     -0.112610              -0.065220   \n",
       "LDA_02                                      0.014754              -0.095213   \n",
       "LDA_03                                     -0.103050              -0.022575   \n",
       "LDA_04                                      0.116573               0.101666   \n",
       "global_subjectivity                        -0.445317              -0.343187   \n",
       "global_sentiment_polarity                   0.223395               0.294983   \n",
       "global_rate_positive_words                 -0.134261              -0.103099   \n",
       "global_rate_negative_words                 -0.356421              -0.485794   \n",
       "rate_positive_words                        -0.041965               0.094422   \n",
       "rate_negative_words                        -0.358567              -0.476291   \n",
       "avg_positive_polarity                      -0.286520              -0.235143   \n",
       "min_positive_polarity                      -0.060462               0.079207   \n",
       "max_positive_polarity                      -0.285687              -0.357855   \n",
       "avg_negative_polarity                       1.000000               0.752729   \n",
       "min_negative_polarity                       0.752729               1.000000   \n",
       "max_negative_polarity                       0.569063               0.083448   \n",
       "title_subjectivity                         -0.085575              -0.065263   \n",
       "title_sentiment_polarity                    0.077411               0.066106   \n",
       "abs_title_subjectivity                      0.016855               0.010451   \n",
       "abs_title_sentiment_polarity               -0.077484              -0.056011   \n",
       "shares                                     -0.036185              -0.022202   \n",
       "\n",
       "                               max_negative_polarity  title_subjectivity  \\\n",
       "timedelta                                  -0.085431           -0.011182   \n",
       "n_tokens_title                              0.024747            0.083849   \n",
       "n_tokens_content                            0.230331            0.011605   \n",
       "n_unique_tokens                             0.011709           -0.012074   \n",
       "n_non_stop_words                            0.013944           -0.012317   \n",
       "n_non_stop_unique_tokens                    0.011911           -0.012543   \n",
       "num_hrefs                                   0.050412            0.047865   \n",
       "num_self_hrefs                              0.032277           -0.002725   \n",
       "num_imgs                                    0.037679            0.062114   \n",
       "num_videos                                  0.026423            0.081046   \n",
       "average_token_length                       -0.203175           -0.022579   \n",
       "num_keywords                                0.030961            0.030090   \n",
       "data_channel_is_lifestyle                   0.040465            0.012823   \n",
       "data_channel_is_entertainment              -0.019636            0.066871   \n",
       "data_channel_is_bus                        -0.007269           -0.043188   \n",
       "data_channel_is_socmed                     -0.029768           -0.036112   \n",
       "data_channel_is_tech                        0.023769           -0.046619   \n",
       "data_channel_is_world                       0.078292           -0.050736   \n",
       "kw_min_min                                 -0.031771            0.003261   \n",
       "kw_max_min                                  0.010965            0.024031   \n",
       "kw_avg_min                                 -0.000877            0.016053   \n",
       "kw_min_max                                 -0.001734            0.031140   \n",
       "kw_max_max                                  0.025414           -0.009081   \n",
       "kw_avg_max                                 -0.027468            0.043056   \n",
       "kw_min_avg                                 -0.028414            0.037507   \n",
       "kw_max_avg                                 -0.010364            0.051206   \n",
       "kw_avg_avg                                 -0.044430            0.088449   \n",
       "self_reference_min_shares                  -0.038404            0.031391   \n",
       "self_reference_max_shares                  -0.028109            0.041802   \n",
       "self_reference_avg_sharess                 -0.041024            0.041550   \n",
       "weekday_is_monday                           0.011087           -0.006369   \n",
       "weekday_is_tuesday                         -0.005057           -0.000071   \n",
       "weekday_is_wednesday                        0.003423           -0.004549   \n",
       "weekday_is_thursday                        -0.008980            0.003183   \n",
       "weekday_is_friday                          -0.017925           -0.015648   \n",
       "weekday_is_saturday                         0.025168            0.014182   \n",
       "weekday_is_sunday                           0.000197            0.019801   \n",
       "is_weekend                                  0.018455            0.024966   \n",
       "LDA_00                                     -0.010934           -0.047871   \n",
       "LDA_01                                     -0.061865            0.057648   \n",
       "LDA_02                                      0.079385           -0.067029   \n",
       "LDA_03                                     -0.078399            0.119949   \n",
       "LDA_04                                      0.055272           -0.052304   \n",
       "global_subjectivity                        -0.238871            0.119017   \n",
       "global_sentiment_polarity                  -0.065863            0.023185   \n",
       "global_rate_positive_words                 -0.069972            0.121791   \n",
       "global_rate_negative_words                  0.077871            0.103575   \n",
       "rate_positive_words                        -0.212130           -0.018060   \n",
       "rate_negative_words                         0.029209            0.004942   \n",
       "avg_positive_polarity                      -0.159296            0.062933   \n",
       "min_positive_polarity                      -0.177367            0.000150   \n",
       "max_positive_polarity                      -0.024620            0.047715   \n",
       "avg_negative_polarity                       0.569063           -0.085575   \n",
       "min_negative_polarity                       0.083448           -0.065263   \n",
       "max_negative_polarity                       1.000000           -0.027888   \n",
       "title_subjectivity                         -0.027888            1.000000   \n",
       "title_sentiment_polarity                   -0.003542            0.231207   \n",
       "abs_title_subjectivity                      0.011490           -0.483577   \n",
       "abs_title_sentiment_polarity               -0.033328            0.725020   \n",
       "shares                                     -0.033918            0.046853   \n",
       "\n",
       "                               title_sentiment_polarity  \\\n",
       "timedelta                                      0.053576   \n",
       "n_tokens_title                                 0.003470   \n",
       "n_tokens_content                               0.008783   \n",
       "n_unique_tokens                               -0.003994   \n",
       "n_non_stop_words                              -0.003886   \n",
       "n_non_stop_unique_tokens                      -0.004321   \n",
       "num_hrefs                                      0.037257   \n",
       "num_self_hrefs                                 0.026615   \n",
       "num_imgs                                       0.047269   \n",
       "num_videos                                     0.029413   \n",
       "average_token_length                          -0.020348   \n",
       "num_keywords                                   0.026295   \n",
       "data_channel_is_lifestyle                      0.014182   \n",
       "data_channel_is_entertainment                 -0.005497   \n",
       "data_channel_is_bus                            0.025550   \n",
       "data_channel_is_socmed                         0.019416   \n",
       "data_channel_is_tech                           0.008651   \n",
       "data_channel_is_world                         -0.075792   \n",
       "kw_min_min                                     0.053949   \n",
       "kw_max_min                                     0.024641   \n",
       "kw_avg_min                                     0.032688   \n",
       "kw_min_max                                     0.008048   \n",
       "kw_max_max                                    -0.037994   \n",
       "kw_avg_max                                    -0.005047   \n",
       "kw_min_avg                                     0.031000   \n",
       "kw_max_avg                                     0.043212   \n",
       "kw_avg_avg                                     0.058967   \n",
       "self_reference_min_shares                      0.007064   \n",
       "self_reference_max_shares                     -0.008877   \n",
       "self_reference_avg_sharess                    -0.004382   \n",
       "weekday_is_monday                              0.007767   \n",
       "weekday_is_tuesday                             0.007243   \n",
       "weekday_is_wednesday                          -0.014489   \n",
       "weekday_is_thursday                           -0.021351   \n",
       "weekday_is_friday                             -0.011279   \n",
       "weekday_is_saturday                            0.025925   \n",
       "weekday_is_sunday                              0.023452   \n",
       "is_weekend                                     0.036210   \n",
       "LDA_00                                         0.032247   \n",
       "LDA_01                                         0.000319   \n",
       "LDA_02                                        -0.065236   \n",
       "LDA_03                                         0.034636   \n",
       "LDA_04                                        -0.000510   \n",
       "global_subjectivity                            0.026397   \n",
       "global_sentiment_polarity                      0.215099   \n",
       "global_rate_positive_words                     0.127186   \n",
       "global_rate_negative_words                    -0.111294   \n",
       "rate_positive_words                            0.124610   \n",
       "rate_negative_words                           -0.166958   \n",
       "avg_positive_polarity                          0.073486   \n",
       "min_positive_polarity                         -0.003991   \n",
       "max_positive_polarity                          0.057378   \n",
       "avg_negative_polarity                          0.077411   \n",
       "min_negative_polarity                          0.066106   \n",
       "max_negative_polarity                         -0.003542   \n",
       "title_subjectivity                             0.231207   \n",
       "title_sentiment_polarity                       1.000000   \n",
       "abs_title_subjectivity                        -0.249742   \n",
       "abs_title_sentiment_polarity                   0.399680   \n",
       "shares                                         0.039401   \n",
       "\n",
       "                               abs_title_subjectivity  \\\n",
       "timedelta                                    0.015111   \n",
       "n_tokens_title                              -0.150208   \n",
       "n_tokens_content                             0.006558   \n",
       "n_unique_tokens                             -0.026061   \n",
       "n_non_stop_words                            -0.025810   \n",
       "n_non_stop_unique_tokens                    -0.025816   \n",
       "num_hrefs                                   -0.001619   \n",
       "num_self_hrefs                              -0.005920   \n",
       "num_imgs                                    -0.017980   \n",
       "num_videos                                  -0.050267   \n",
       "average_token_length                         0.019910   \n",
       "num_keywords                                -0.013859   \n",
       "data_channel_is_lifestyle                    0.036361   \n",
       "data_channel_is_entertainment               -0.076972   \n",
       "data_channel_is_bus                         -0.003141   \n",
       "data_channel_is_socmed                       0.016428   \n",
       "data_channel_is_tech                         0.015172   \n",
       "data_channel_is_world                        0.051549   \n",
       "kw_min_min                                  -0.005931   \n",
       "kw_max_min                                   0.005154   \n",
       "kw_avg_min                                   0.011704   \n",
       "kw_min_max                                  -0.016580   \n",
       "kw_max_max                                   0.002457   \n",
       "kw_avg_max                                  -0.018589   \n",
       "kw_min_avg                                  -0.023609   \n",
       "kw_max_avg                                  -0.003757   \n",
       "kw_avg_avg                                  -0.023536   \n",
       "self_reference_min_shares                   -0.007016   \n",
       "self_reference_max_shares                   -0.010291   \n",
       "self_reference_avg_sharess                  -0.008375   \n",
       "weekday_is_monday                           -0.012166   \n",
       "weekday_is_tuesday                           0.002636   \n",
       "weekday_is_wednesday                         0.020998   \n",
       "weekday_is_thursday                         -0.009966   \n",
       "weekday_is_friday                            0.022915   \n",
       "weekday_is_saturday                         -0.019795   \n",
       "weekday_is_sunday                           -0.015683   \n",
       "is_weekend                                  -0.026003   \n",
       "LDA_00                                       0.001962   \n",
       "LDA_01                                      -0.082134   \n",
       "LDA_02                                       0.068938   \n",
       "LDA_03                                      -0.031529   \n",
       "LDA_04                                       0.023690   \n",
       "global_subjectivity                         -0.004305   \n",
       "global_sentiment_polarity                   -0.012337   \n",
       "global_rate_positive_words                  -0.130807   \n",
       "global_rate_negative_words                  -0.074649   \n",
       "rate_positive_words                         -0.010747   \n",
       "rate_negative_words                          0.028734   \n",
       "avg_positive_polarity                        0.007518   \n",
       "min_positive_polarity                        0.019403   \n",
       "max_positive_polarity                       -0.009577   \n",
       "avg_negative_polarity                        0.016855   \n",
       "min_negative_polarity                        0.010451   \n",
       "max_negative_polarity                        0.011490   \n",
       "title_subjectivity                          -0.483577   \n",
       "title_sentiment_polarity                    -0.249742   \n",
       "abs_title_subjectivity                       1.000000   \n",
       "abs_title_sentiment_polarity                -0.413649   \n",
       "shares                                      -0.000278   \n",
       "\n",
       "                               abs_title_sentiment_polarity    shares  \n",
       "timedelta                                          0.010796 -0.011844  \n",
       "n_tokens_title                                     0.054882  0.010694  \n",
       "n_tokens_content                                   0.003933 -0.025070  \n",
       "n_unique_tokens                                   -0.009672  0.004472  \n",
       "n_non_stop_words                                  -0.009818  0.004003  \n",
       "n_non_stop_unique_tokens                          -0.010055  0.004024  \n",
       "num_hrefs                                          0.058085  0.032497  \n",
       "num_self_hrefs                                     0.004348 -0.009001  \n",
       "num_imgs                                           0.069438  0.063019  \n",
       "num_videos                                         0.058370  0.015876  \n",
       "average_token_length                              -0.014416 -0.035962  \n",
       "num_keywords                                       0.050314  0.033619  \n",
       "data_channel_is_lifestyle                          0.024134  0.007844  \n",
       "data_channel_is_entertainment                      0.048874 -0.022928  \n",
       "data_channel_is_bus                               -0.025362 -0.031095  \n",
       "data_channel_is_socmed                            -0.027533  0.007476  \n",
       "data_channel_is_tech                              -0.048378 -0.016670  \n",
       "data_channel_is_world                             -0.053924 -0.068843  \n",
       "kw_min_min                                         0.022615 -0.005863  \n",
       "kw_max_min                                         0.020378  0.044156  \n",
       "kw_avg_min                                         0.015544  0.037032  \n",
       "kw_min_max                                         0.009981  0.017206  \n",
       "kw_max_max                                        -0.025784  0.012985  \n",
       "kw_avg_max                                         0.014789  0.058693  \n",
       "kw_min_avg                                         0.040703  0.050671  \n",
       "kw_max_avg                                         0.055310  0.119248  \n",
       "kw_avg_avg                                         0.084475  0.176541  \n",
       "self_reference_min_shares                          0.011850  0.044372  \n",
       "self_reference_max_shares                          0.011004  0.049706  \n",
       "self_reference_avg_sharess                         0.010678  0.057214  \n",
       "weekday_is_monday                                  0.007744  0.002465  \n",
       "weekday_is_tuesday                                -0.014037 -0.016349  \n",
       "weekday_is_wednesday                               0.002847  0.001515  \n",
       "weekday_is_thursday                               -0.016816 -0.015115  \n",
       "weekday_is_friday                                 -0.007456 -0.009143  \n",
       "weekday_is_saturday                                0.017284  0.032922  \n",
       "weekday_is_sunday                                  0.024958  0.022785  \n",
       "is_weekend                                         0.031038  0.040807  \n",
       "LDA_00                                            -0.028934 -0.013572  \n",
       "LDA_01                                             0.041141 -0.021573  \n",
       "LDA_02                                            -0.061856 -0.067500  \n",
       "LDA_03                                             0.107354  0.122088  \n",
       "LDA_04                                            -0.050114 -0.028306  \n",
       "global_subjectivity                                0.095193  0.043230  \n",
       "global_sentiment_polarity                          0.076423  0.019364  \n",
       "global_rate_positive_words                         0.115170  0.010702  \n",
       "global_rate_negative_words                         0.068923  0.009240  \n",
       "rate_positive_words                                0.011817 -0.020869  \n",
       "rate_negative_words                               -0.019408 -0.014646  \n",
       "avg_positive_polarity                              0.108800  0.020892  \n",
       "min_positive_polarity                              0.009601  0.014251  \n",
       "max_positive_polarity                              0.076947 -0.000914  \n",
       "avg_negative_polarity                             -0.077484 -0.036185  \n",
       "min_negative_polarity                             -0.056011 -0.022202  \n",
       "max_negative_polarity                             -0.033328 -0.033918  \n",
       "title_subjectivity                                 0.725020  0.046853  \n",
       "title_sentiment_polarity                           0.399680  0.039401  \n",
       "abs_title_subjectivity                            -0.413649 -0.000278  \n",
       "abs_title_sentiment_polarity                       1.000000  0.054514  \n",
       "shares                                             0.054514  1.000000  \n",
       "\n",
       "[60 rows x 60 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range (i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = df_Train.columns[columns]\n",
    "data = df_Train[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "share_count = 0\n",
    "\n",
    "with open('sample_submission2.csv', 'w', newline='') as csvfile:\n",
    "    \n",
    "    fieldnames = ['id','shares']\n",
    "    thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    thewriter.writeheader()\n",
    "    \n",
    "    for shares in ann_pred:\n",
    "        share_count += 1\n",
    "        thewriter.writerow({'id':share_count, 'shares':shares})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=59, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dfgbe\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "4000/4000 [==============================] - 0s 118us/step - loss: 8833.5787 - val_loss: 6335.1604\n",
      "Epoch 2/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 6037.8473 - val_loss: 6701.5905\n",
      "Epoch 3/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 5714.9799 - val_loss: 5705.2977\n",
      "Epoch 4/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 5260.5407 - val_loss: 5730.1547\n",
      "Epoch 5/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 5296.7786 - val_loss: 5687.8912\n",
      "Epoch 6/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 5272.3281 - val_loss: 5483.3916\n",
      "Epoch 7/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 5234.2627 - val_loss: 5466.1653\n",
      "Epoch 8/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 5088.8831 - val_loss: 5688.1686\n",
      "Epoch 9/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4992.8605 - val_loss: 5499.1710\n",
      "Epoch 10/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4822.4885 - val_loss: 5647.3584\n",
      "Epoch 11/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4795.9596 - val_loss: 5441.2772\n",
      "Epoch 12/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4795.0107 - val_loss: 5533.3231\n",
      "Epoch 13/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4751.0424 - val_loss: 5575.5038\n",
      "Epoch 14/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4767.5528 - val_loss: 5577.3753\n",
      "Epoch 15/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4835.9813 - val_loss: 5466.7186\n",
      "Epoch 16/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4763.6436 - val_loss: 5387.5009\n",
      "Epoch 17/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4742.0141 - val_loss: 5453.0874\n",
      "Epoch 18/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4660.2095 - val_loss: 5427.9114\n",
      "Epoch 19/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4795.4489 - val_loss: 5581.2469\n",
      "Epoch 20/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4655.9266 - val_loss: 5539.9516\n",
      "Epoch 21/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4616.6718 - val_loss: 5391.3981\n",
      "Epoch 22/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4790.0442 - val_loss: 5422.9620\n",
      "Epoch 23/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4758.9313 - val_loss: 5415.6444\n",
      "Epoch 24/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4741.2186 - val_loss: 5415.2559\n",
      "Epoch 25/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4673.8520 - val_loss: 5377.8739\n",
      "Epoch 26/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4784.2333 - val_loss: 5491.4997\n",
      "Epoch 27/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4764.2147 - val_loss: 5402.3555\n",
      "Epoch 28/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4714.5758 - val_loss: 5380.9469\n",
      "Epoch 29/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4658.3411 - val_loss: 5391.1546\n",
      "Epoch 30/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4744.8159 - val_loss: 5387.3306\n",
      "Epoch 31/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4758.8590 - val_loss: 5453.1048\n",
      "Epoch 32/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4742.7298 - val_loss: 5509.7807\n",
      "Epoch 33/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4708.6058 - val_loss: 5524.4394\n",
      "Epoch 34/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4694.2120 - val_loss: 5471.5136\n",
      "Epoch 35/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4716.2225 - val_loss: 5402.2644\n",
      "Epoch 36/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4640.0205 - val_loss: 5362.4280\n",
      "Epoch 37/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4642.2535 - val_loss: 5385.4076\n",
      "Epoch 38/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4710.7051 - val_loss: 5451.9472\n",
      "Epoch 39/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4732.9672 - val_loss: 5490.9825\n",
      "Epoch 40/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4701.6247 - val_loss: 5373.1912\n",
      "Epoch 41/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4625.8654 - val_loss: 5432.0730\n",
      "Epoch 42/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4719.3807 - val_loss: 5374.1790\n",
      "Epoch 43/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4710.0868 - val_loss: 5510.0329\n",
      "Epoch 44/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4670.5702 - val_loss: 5409.8616\n",
      "Epoch 45/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4597.0904 - val_loss: 5366.8979\n",
      "Epoch 46/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4672.5311 - val_loss: 5360.0648\n",
      "Epoch 47/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4647.4963 - val_loss: 5379.0324\n",
      "Epoch 48/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4728.7033 - val_loss: 5335.5766\n",
      "Epoch 49/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4670.2568 - val_loss: 5395.7124\n",
      "Epoch 50/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4644.7074 - val_loss: 5403.2832\n",
      "Epoch 51/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4608.2151 - val_loss: 5348.9942\n",
      "Epoch 52/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4578.8678 - val_loss: 5360.9130\n",
      "Epoch 53/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4656.4884 - val_loss: 5365.9132\n",
      "Epoch 54/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4629.9373 - val_loss: 5345.1680\n",
      "Epoch 55/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4670.0908 - val_loss: 5339.7578\n",
      "Epoch 56/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4592.3896 - val_loss: 5356.8019\n",
      "Epoch 57/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4669.4567 - val_loss: 5330.1263\n",
      "Epoch 58/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4636.3045 - val_loss: 5351.2446\n",
      "Epoch 59/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4601.0479 - val_loss: 5356.3318\n",
      "Epoch 60/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4689.4718 - val_loss: 5372.8669\n",
      "Epoch 61/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4686.8802 - val_loss: 5333.5226\n",
      "Epoch 62/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4606.9996 - val_loss: 5332.5662\n",
      "Epoch 63/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4669.6573 - val_loss: 5389.9315\n",
      "Epoch 64/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4599.3770 - val_loss: 5337.3245\n",
      "Epoch 65/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4590.8427 - val_loss: 5345.5012\n",
      "Epoch 66/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4589.8002 - val_loss: 5361.2556\n",
      "Epoch 67/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4608.6691 - val_loss: 5331.9703\n",
      "Epoch 68/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4601.8538 - val_loss: 5370.7912\n",
      "Epoch 69/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4670.9326 - val_loss: 5320.9651\n",
      "Epoch 70/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4681.4559 - val_loss: 5387.2724\n",
      "Epoch 71/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4576.9160 - val_loss: 5396.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4657.3325 - val_loss: 5379.1053\n",
      "Epoch 73/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4562.3388 - val_loss: 5326.5016\n",
      "Epoch 74/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4603.7167 - val_loss: 5335.8071\n",
      "Epoch 75/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4590.7499 - val_loss: 5356.9001\n",
      "Epoch 76/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4675.3365 - val_loss: 5356.9060\n",
      "Epoch 77/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4658.2879 - val_loss: 5362.0756\n",
      "Epoch 78/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4552.3671 - val_loss: 5331.8274\n",
      "Epoch 79/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4619.8739 - val_loss: 5340.7592\n",
      "Epoch 80/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4593.3917 - val_loss: 5365.1916\n",
      "Epoch 81/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4533.6077 - val_loss: 5338.4045\n",
      "Epoch 82/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4583.9340 - val_loss: 5326.1651\n",
      "Epoch 83/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4605.9136 - val_loss: 5333.2183\n",
      "Epoch 84/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4643.9983 - val_loss: 5350.0581\n",
      "Epoch 85/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4654.9301 - val_loss: 5326.2574\n",
      "Epoch 86/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4587.9774 - val_loss: 5355.8878\n",
      "Epoch 87/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4612.9236 - val_loss: 5331.6542\n",
      "Epoch 88/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4560.9139 - val_loss: 5334.3759\n",
      "Epoch 89/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4617.6635 - val_loss: 5349.2991\n",
      "Epoch 90/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4583.0631 - val_loss: 5355.8264\n",
      "Epoch 91/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4615.5301 - val_loss: 5351.1317\n",
      "Epoch 92/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4574.4372 - val_loss: 5404.5976\n",
      "Epoch 93/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4591.3269 - val_loss: 5444.3413\n",
      "Epoch 94/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4578.0776 - val_loss: 5315.6831\n",
      "Epoch 95/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4532.8078 - val_loss: 5310.2286\n",
      "Epoch 96/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4568.7412 - val_loss: 5327.2135\n",
      "Epoch 97/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4616.1088 - val_loss: 5340.1137\n",
      "Epoch 98/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4550.2537 - val_loss: 5309.9518\n",
      "Epoch 99/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4571.8422 - val_loss: 5313.9759\n",
      "Epoch 100/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4600.4136 - val_loss: 5325.6029\n",
      "Epoch 101/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4559.4651 - val_loss: 5375.0971\n",
      "Epoch 102/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4501.4762 - val_loss: 5324.9200\n",
      "Epoch 103/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4527.7375 - val_loss: 5337.5657\n",
      "Epoch 104/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4557.4508 - val_loss: 5368.5601\n",
      "Epoch 105/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4580.8394 - val_loss: 5355.6759\n",
      "Epoch 106/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4599.5611 - val_loss: 5341.9303\n",
      "Epoch 107/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4599.9968 - val_loss: 5335.0508\n",
      "Epoch 108/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4618.2966 - val_loss: 5369.7292\n",
      "Epoch 109/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4561.7362 - val_loss: 5341.3190\n",
      "Epoch 110/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4537.1797 - val_loss: 5336.1105\n",
      "Epoch 111/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4591.2230 - val_loss: 5340.7105\n",
      "Epoch 112/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4657.3559 - val_loss: 5417.0380\n",
      "Epoch 113/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4592.4195 - val_loss: 5353.1379\n",
      "Epoch 114/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4522.3819 - val_loss: 5406.8145\n",
      "Epoch 115/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4560.5273 - val_loss: 5321.8871\n",
      "Epoch 116/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4562.4114 - val_loss: 5323.7865\n",
      "Epoch 117/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4468.5800 - val_loss: 5444.2314\n",
      "Epoch 118/1000\n",
      "4000/4000 [==============================] - 1s 125us/step - loss: 4575.1395 - val_loss: 5368.4060\n",
      "Epoch 119/1000\n",
      "4000/4000 [==============================] - 1s 147us/step - loss: 4571.9390 - val_loss: 5339.4813\n",
      "Epoch 120/1000\n",
      "4000/4000 [==============================] - 0s 117us/step - loss: 4540.9500 - val_loss: 5322.6684\n",
      "Epoch 121/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4478.1061 - val_loss: 5334.1973\n",
      "Epoch 122/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4580.4164 - val_loss: 5346.3886\n",
      "Epoch 123/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4554.5513 - val_loss: 5350.6346\n",
      "Epoch 124/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4494.5458 - val_loss: 5365.4459\n",
      "Epoch 125/1000\n",
      "4000/4000 [==============================] - 0s 125us/step - loss: 4540.1707 - val_loss: 5376.7891\n",
      "Epoch 126/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4515.4641 - val_loss: 5339.9265\n",
      "Epoch 127/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4592.9078 - val_loss: 5503.9975\n",
      "Epoch 128/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4539.9679 - val_loss: 5353.1850\n",
      "Epoch 129/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4552.4527 - val_loss: 5345.1442\n",
      "Epoch 130/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4496.3739 - val_loss: 5440.6635\n",
      "Epoch 131/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4578.2427 - val_loss: 5331.9726\n",
      "Epoch 132/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4580.0752 - val_loss: 5341.8263\n",
      "Epoch 133/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4503.6754 - val_loss: 5341.3499\n",
      "Epoch 134/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4585.6104 - val_loss: 5332.9050\n",
      "Epoch 135/1000\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 4486.1938 - val_loss: 5317.6250\n",
      "Epoch 136/1000\n",
      "4000/4000 [==============================] - 0s 125us/step - loss: 4560.8217 - val_loss: 5362.2091\n",
      "Epoch 137/1000\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 4574.0660 - val_loss: 5322.5558\n",
      "Epoch 138/1000\n",
      "4000/4000 [==============================] - 0s 123us/step - loss: 4550.4490 - val_loss: 5431.3563\n",
      "Epoch 139/1000\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 4544.6816 - val_loss: 5339.6918\n",
      "Epoch 140/1000\n",
      "4000/4000 [==============================] - 0s 120us/step - loss: 4496.5891 - val_loss: 5319.0420\n",
      "Epoch 141/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4485.3573 - val_loss: 5336.4473\n",
      "Epoch 142/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4563.9836 - val_loss: 5347.0480\n",
      "Epoch 143/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4512.2267 - val_loss: 5370.0476\n",
      "Epoch 144/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 79us/step - loss: 4435.8000 - val_loss: 5325.6898\n",
      "Epoch 145/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4529.4684 - val_loss: 5392.8454\n",
      "Epoch 146/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4624.5642 - val_loss: 5317.4378\n",
      "Epoch 147/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4546.1156 - val_loss: 5324.4434\n",
      "Epoch 148/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4589.7148 - val_loss: 5334.8021\n",
      "Epoch 149/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4541.2822 - val_loss: 5316.0222\n",
      "Epoch 150/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4511.5452 - val_loss: 5337.6754\n",
      "Epoch 151/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4569.0329 - val_loss: 5352.0261\n",
      "Epoch 152/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4503.4352 - val_loss: 5320.5699\n",
      "Epoch 153/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4600.3887 - val_loss: 5386.8514\n",
      "Epoch 154/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4530.3669 - val_loss: 5356.0983\n",
      "Epoch 155/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4518.9615 - val_loss: 5342.2281\n",
      "Epoch 156/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4517.5229 - val_loss: 5356.5128\n",
      "Epoch 157/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4516.1444 - val_loss: 5331.3541\n",
      "Epoch 158/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4572.2457 - val_loss: 5332.7206\n",
      "Epoch 159/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4500.4239 - val_loss: 5335.9785\n",
      "Epoch 160/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4518.2372 - val_loss: 5345.4014\n",
      "Epoch 161/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4574.0030 - val_loss: 5426.6333\n",
      "Epoch 162/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4524.2821 - val_loss: 5423.2958\n",
      "Epoch 163/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4504.1587 - val_loss: 5350.9121\n",
      "Epoch 164/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4575.7372 - val_loss: 5496.0597\n",
      "Epoch 165/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4572.3247 - val_loss: 5325.1448\n",
      "Epoch 166/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4505.4630 - val_loss: 5317.9436\n",
      "Epoch 167/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4471.9207 - val_loss: 5417.5941\n",
      "Epoch 168/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4565.6043 - val_loss: 5367.3722\n",
      "Epoch 169/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4550.1557 - val_loss: 5377.1437\n",
      "Epoch 170/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4536.7155 - val_loss: 5381.8450\n",
      "Epoch 171/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4515.4619 - val_loss: 5336.8738\n",
      "Epoch 172/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4514.3663 - val_loss: 5375.1929\n",
      "Epoch 173/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4539.2880 - val_loss: 5346.4386\n",
      "Epoch 174/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4530.8644 - val_loss: 5345.6329\n",
      "Epoch 175/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4533.4553 - val_loss: 5328.6415\n",
      "Epoch 176/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4433.9373 - val_loss: 5362.9326\n",
      "Epoch 177/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4533.9193 - val_loss: 5383.1176\n",
      "Epoch 178/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4560.2558 - val_loss: 5406.9071\n",
      "Epoch 179/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4545.9973 - val_loss: 5322.9889\n",
      "Epoch 180/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4582.5185 - val_loss: 5345.7467\n",
      "Epoch 181/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4565.1934 - val_loss: 5333.3790\n",
      "Epoch 182/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4535.5295 - val_loss: 5401.4912\n",
      "Epoch 183/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4534.4734 - val_loss: 5355.9209\n",
      "Epoch 184/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4534.9989 - val_loss: 5339.2618\n",
      "Epoch 185/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4445.9181 - val_loss: 5336.4471\n",
      "Epoch 186/1000\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 4564.15 - 0s 85us/step - loss: 4523.7188 - val_loss: 5331.6437\n",
      "Epoch 187/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4575.6525 - val_loss: 5317.6871\n",
      "Epoch 188/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4559.7323 - val_loss: 5400.6584\n",
      "Epoch 189/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4480.0473 - val_loss: 5338.4250\n",
      "Epoch 190/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4530.6642 - val_loss: 5362.7161\n",
      "Epoch 191/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4583.7318 - val_loss: 5355.1810\n",
      "Epoch 192/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4540.8630 - val_loss: 5334.9336\n",
      "Epoch 193/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4535.5611 - val_loss: 5362.1628\n",
      "Epoch 194/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4563.9631 - val_loss: 5421.5079\n",
      "Epoch 195/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4574.0258 - val_loss: 5335.9006\n",
      "Epoch 196/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4542.3802 - val_loss: 5425.3404\n",
      "Epoch 197/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4482.4646 - val_loss: 5359.9631\n",
      "Epoch 198/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4603.8902 - val_loss: 5364.1238\n",
      "Epoch 199/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4534.3072 - val_loss: 5336.7903\n",
      "Epoch 200/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4527.4809 - val_loss: 5326.5891\n",
      "Epoch 201/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4514.3714 - val_loss: 5346.0300\n",
      "Epoch 202/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4480.9927 - val_loss: 5342.8825\n",
      "Epoch 203/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4522.5687 - val_loss: 5374.1771\n",
      "Epoch 204/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4553.1387 - val_loss: 5369.6964\n",
      "Epoch 205/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4566.5319 - val_loss: 5330.3508\n",
      "Epoch 206/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4429.5706 - val_loss: 5326.9639\n",
      "Epoch 207/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4548.3993 - val_loss: 5333.7069\n",
      "Epoch 208/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4578.3081 - val_loss: 5411.1174\n",
      "Epoch 209/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4539.7841 - val_loss: 5346.2598\n",
      "Epoch 210/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4565.4002 - val_loss: 5324.3512\n",
      "Epoch 211/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4479.3652 - val_loss: 5333.1846\n",
      "Epoch 212/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4536.7300 - val_loss: 5319.8008\n",
      "Epoch 213/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4490.0830 - val_loss: 5353.4894\n",
      "Epoch 214/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4551.5952 - val_loss: 5347.7953\n",
      "Epoch 215/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4529.0348 - val_loss: 5345.1630\n",
      "Epoch 216/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 82us/step - loss: 4497.1974 - val_loss: 5348.9377\n",
      "Epoch 217/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4528.7861 - val_loss: 5333.4774\n",
      "Epoch 218/1000\n",
      "4000/4000 [==============================] - 0s 75us/step - loss: 4515.9862 - val_loss: 5339.1873\n",
      "Epoch 219/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4504.1409 - val_loss: 5329.7997\n",
      "Epoch 220/1000\n",
      "4000/4000 [==============================] - 0s 76us/step - loss: 4576.2505 - val_loss: 5355.4278\n",
      "Epoch 221/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4571.9234 - val_loss: 5342.9218\n",
      "Epoch 222/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4524.0374 - val_loss: 5335.0729\n",
      "Epoch 223/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4442.9771 - val_loss: 5344.8422\n",
      "Epoch 224/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4499.3237 - val_loss: 5359.6542\n",
      "Epoch 225/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4488.1939 - val_loss: 5335.3525\n",
      "Epoch 226/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4556.7813 - val_loss: 5331.6677\n",
      "Epoch 227/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4508.0582 - val_loss: 5362.0099\n",
      "Epoch 228/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4533.4850 - val_loss: 5348.4833\n",
      "Epoch 229/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4455.9827 - val_loss: 5325.7519\n",
      "Epoch 230/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4565.4141 - val_loss: 5345.6304\n",
      "Epoch 231/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4525.4781 - val_loss: 5340.3843\n",
      "Epoch 232/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4534.8858 - val_loss: 5407.5403\n",
      "Epoch 233/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4523.2632 - val_loss: 5320.5826\n",
      "Epoch 234/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4534.4982 - val_loss: 5363.4792\n",
      "Epoch 235/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4483.4831 - val_loss: 5337.3794\n",
      "Epoch 236/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4527.0817 - val_loss: 5369.4155\n",
      "Epoch 237/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4459.1658 - val_loss: 5334.6352\n",
      "Epoch 238/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4482.3344 - val_loss: 5390.2515\n",
      "Epoch 239/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4516.4447 - val_loss: 5341.3535\n",
      "Epoch 240/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4469.5988 - val_loss: 5351.5654\n",
      "Epoch 241/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4558.0293 - val_loss: 5339.3670\n",
      "Epoch 242/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4495.2774 - val_loss: 5339.0939\n",
      "Epoch 243/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4529.9580 - val_loss: 5360.7728\n",
      "Epoch 244/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4537.0398 - val_loss: 5361.9464\n",
      "Epoch 245/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4527.1988 - val_loss: 5366.5168\n",
      "Epoch 246/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4518.7090 - val_loss: 5370.6838\n",
      "Epoch 247/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4504.4563 - val_loss: 5386.3957\n",
      "Epoch 248/1000\n",
      "4000/4000 [==============================] - 0s 78us/step - loss: 4482.8406 - val_loss: 5346.2319\n",
      "Epoch 249/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4431.3909 - val_loss: 5415.2285\n",
      "Epoch 250/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4572.4929 - val_loss: 5314.4263\n",
      "Epoch 251/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4503.3003 - val_loss: 5485.0716\n",
      "Epoch 252/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4520.5126 - val_loss: 5325.2095\n",
      "Epoch 253/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4486.0188 - val_loss: 5427.5605\n",
      "Epoch 254/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4515.7479 - val_loss: 5368.9477\n",
      "Epoch 255/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4552.3122 - val_loss: 5355.0795\n",
      "Epoch 256/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4506.1347 - val_loss: 5341.0226\n",
      "Epoch 257/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4507.2035 - val_loss: 5348.9877\n",
      "Epoch 258/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4508.8068 - val_loss: 5380.3845\n",
      "Epoch 259/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4356.2441 - val_loss: 5474.1416\n",
      "Epoch 260/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4467.4435 - val_loss: 5347.6375\n",
      "Epoch 261/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4488.0129 - val_loss: 5361.6059\n",
      "Epoch 262/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4401.6642 - val_loss: 5373.5206\n",
      "Epoch 263/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4509.8701 - val_loss: 5356.6271\n",
      "Epoch 264/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4529.0866 - val_loss: 5413.4290\n",
      "Epoch 265/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4566.2962 - val_loss: 5377.6049\n",
      "Epoch 266/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4485.4999 - val_loss: 5390.4958\n",
      "Epoch 267/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4519.9041 - val_loss: 5402.6865\n",
      "Epoch 268/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4476.5941 - val_loss: 5353.8577\n",
      "Epoch 269/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4537.4761 - val_loss: 5404.1850\n",
      "Epoch 270/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4448.7122 - val_loss: 5339.5115\n",
      "Epoch 271/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4497.4021 - val_loss: 5331.5418\n",
      "Epoch 272/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4476.7796 - val_loss: 5353.0617\n",
      "Epoch 273/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4540.5558 - val_loss: 5365.4786\n",
      "Epoch 274/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4565.3320 - val_loss: 5337.4577\n",
      "Epoch 275/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4515.0197 - val_loss: 5356.4373\n",
      "Epoch 276/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4531.3563 - val_loss: 5337.0822\n",
      "Epoch 277/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4471.9706 - val_loss: 5341.9483\n",
      "Epoch 278/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4476.7242 - val_loss: 5533.1903\n",
      "Epoch 279/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4535.5449 - val_loss: 5370.9433\n",
      "Epoch 280/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4549.2116 - val_loss: 5342.8320\n",
      "Epoch 281/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4516.8298 - val_loss: 5338.3287\n",
      "Epoch 282/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4512.6073 - val_loss: 5330.4595\n",
      "Epoch 283/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4522.3669 - val_loss: 5336.1275\n",
      "Epoch 284/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4494.3170 - val_loss: 5398.1890\n",
      "Epoch 285/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4520.8320 - val_loss: 5505.0068\n",
      "Epoch 286/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4587.1677 - val_loss: 5376.4099\n",
      "Epoch 287/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4463.0468 - val_loss: 5338.6596\n",
      "Epoch 288/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 83us/step - loss: 4524.9139 - val_loss: 5396.1299\n",
      "Epoch 289/1000\n",
      "4000/4000 [==============================] - 0s 77us/step - loss: 4579.0385 - val_loss: 5439.5756\n",
      "Epoch 290/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4481.6478 - val_loss: 5363.3934\n",
      "Epoch 291/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4520.3055 - val_loss: 5372.8450\n",
      "Epoch 292/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4466.5330 - val_loss: 5355.5478\n",
      "Epoch 293/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4445.6280 - val_loss: 5332.3122\n",
      "Epoch 294/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4468.9002 - val_loss: 5365.0598\n",
      "Epoch 295/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4457.4318 - val_loss: 5354.6331\n",
      "Epoch 296/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4369.1736 - val_loss: 5389.8482\n",
      "Epoch 297/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4490.4959 - val_loss: 5342.7911\n",
      "Epoch 298/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4503.0464 - val_loss: 5362.2940\n",
      "Epoch 299/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4568.0014 - val_loss: 5336.9087\n",
      "Epoch 300/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4499.7964 - val_loss: 5359.2573\n",
      "Epoch 301/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4488.2997 - val_loss: 5345.7233\n",
      "Epoch 302/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4537.2912 - val_loss: 5366.6671\n",
      "Epoch 303/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4463.1655 - val_loss: 5461.3644\n",
      "Epoch 304/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4577.2226 - val_loss: 5488.1498\n",
      "Epoch 305/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4497.5480 - val_loss: 5689.0586\n",
      "Epoch 306/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4505.4891 - val_loss: 5349.7046\n",
      "Epoch 307/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4515.1586 - val_loss: 5365.0022\n",
      "Epoch 308/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4513.7847 - val_loss: 5353.3442\n",
      "Epoch 309/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4503.2305 - val_loss: 5357.7594\n",
      "Epoch 310/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4503.2025 - val_loss: 5368.8100\n",
      "Epoch 311/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4516.4524 - val_loss: 5338.9192\n",
      "Epoch 312/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4437.5354 - val_loss: 5359.0605\n",
      "Epoch 313/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4480.4143 - val_loss: 5386.4702\n",
      "Epoch 314/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4501.0208 - val_loss: 5344.8630\n",
      "Epoch 315/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4504.0810 - val_loss: 5363.3738\n",
      "Epoch 316/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4415.2481 - val_loss: 5368.3619\n",
      "Epoch 317/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4520.4613 - val_loss: 5337.8105\n",
      "Epoch 318/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4476.5788 - val_loss: 5375.6767\n",
      "Epoch 319/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4488.8503 - val_loss: 5360.5945\n",
      "Epoch 320/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4468.6467 - val_loss: 5341.9149\n",
      "Epoch 321/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4508.1902 - val_loss: 5391.3992\n",
      "Epoch 322/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4515.9722 - val_loss: 5390.3253\n",
      "Epoch 323/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4467.9475 - val_loss: 5354.5567\n",
      "Epoch 324/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4518.4064 - val_loss: 5351.0927\n",
      "Epoch 325/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4405.4394 - val_loss: 5372.3669\n",
      "Epoch 326/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4491.7927 - val_loss: 5383.2602\n",
      "Epoch 327/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4519.3552 - val_loss: 5365.5588\n",
      "Epoch 328/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4484.7247 - val_loss: 5390.3653\n",
      "Epoch 329/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4542.6014 - val_loss: 5360.0310\n",
      "Epoch 330/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4523.8249 - val_loss: 5341.4803\n",
      "Epoch 331/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4527.8815 - val_loss: 5405.4223\n",
      "Epoch 332/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4514.3028 - val_loss: 5376.1351\n",
      "Epoch 333/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4523.8788 - val_loss: 5381.3797\n",
      "Epoch 334/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4563.5171 - val_loss: 5359.4657\n",
      "Epoch 335/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4503.0986 - val_loss: 5351.4744\n",
      "Epoch 336/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4522.0382 - val_loss: 5345.5172\n",
      "Epoch 337/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4508.0777 - val_loss: 5395.5841\n",
      "Epoch 338/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4444.8579 - val_loss: 5443.1659\n",
      "Epoch 339/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4503.0255 - val_loss: 5378.6121\n",
      "Epoch 340/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4512.2411 - val_loss: 5427.2004\n",
      "Epoch 341/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4560.8579 - val_loss: 5403.4038\n",
      "Epoch 342/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4539.9388 - val_loss: 5371.4346\n",
      "Epoch 343/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4492.7150 - val_loss: 5366.7635\n",
      "Epoch 344/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4529.1958 - val_loss: 5512.4874\n",
      "Epoch 345/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4514.8414 - val_loss: 5413.0597\n",
      "Epoch 346/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4416.0204 - val_loss: 5452.6392\n",
      "Epoch 347/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4458.3312 - val_loss: 5484.4743\n",
      "Epoch 348/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4507.6997 - val_loss: 5425.1994\n",
      "Epoch 349/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4456.3611 - val_loss: 5370.0746\n",
      "Epoch 350/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4499.2157 - val_loss: 5354.4267\n",
      "Epoch 351/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4502.2280 - val_loss: 5362.7878\n",
      "Epoch 352/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4517.5253 - val_loss: 5437.9807\n",
      "Epoch 353/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4463.7274 - val_loss: 5368.3599\n",
      "Epoch 354/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4496.8659 - val_loss: 5366.8885\n",
      "Epoch 355/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4494.2479 - val_loss: 5372.4591\n",
      "Epoch 356/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4529.3969 - val_loss: 5363.2990\n",
      "Epoch 357/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4504.1477 - val_loss: 5334.6981\n",
      "Epoch 358/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4419.9838 - val_loss: 5347.6442\n",
      "Epoch 359/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4519.9107 - val_loss: 5350.3337\n",
      "Epoch 360/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 82us/step - loss: 4451.4036 - val_loss: 5390.2581\n",
      "Epoch 361/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4492.2929 - val_loss: 5331.8391\n",
      "Epoch 362/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4499.3788 - val_loss: 5388.9768\n",
      "Epoch 363/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4448.9250 - val_loss: 5383.5393\n",
      "Epoch 364/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4482.9454 - val_loss: 5435.1714\n",
      "Epoch 365/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4494.0964 - val_loss: 5359.2798\n",
      "Epoch 366/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4460.8914 - val_loss: 5356.7585\n",
      "Epoch 367/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4497.3964 - val_loss: 5384.0620\n",
      "Epoch 368/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4480.6199 - val_loss: 5364.1143\n",
      "Epoch 369/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4441.0412 - val_loss: 5394.3016\n",
      "Epoch 370/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4462.7118 - val_loss: 5364.6047\n",
      "Epoch 371/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4473.3004 - val_loss: 5358.7154\n",
      "Epoch 372/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4458.0993 - val_loss: 5437.8344\n",
      "Epoch 373/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4510.5284 - val_loss: 5361.8819\n",
      "Epoch 374/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4531.5728 - val_loss: 5356.1515\n",
      "Epoch 375/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4508.7069 - val_loss: 5379.1043\n",
      "Epoch 376/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4493.7308 - val_loss: 5363.5631\n",
      "Epoch 377/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4515.0470 - val_loss: 5476.4992\n",
      "Epoch 378/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4482.9783 - val_loss: 5390.5581\n",
      "Epoch 379/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4488.2955 - val_loss: 5369.7011\n",
      "Epoch 380/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4457.5296 - val_loss: 5364.3088\n",
      "Epoch 381/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4495.8093 - val_loss: 5362.8720\n",
      "Epoch 382/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4551.5557 - val_loss: 5348.7856\n",
      "Epoch 383/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4498.8173 - val_loss: 5427.2380\n",
      "Epoch 384/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4470.4778 - val_loss: 5406.3529\n",
      "Epoch 385/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4470.0241 - val_loss: 5363.6547\n",
      "Epoch 386/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4492.7573 - val_loss: 5413.6765\n",
      "Epoch 387/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4481.6782 - val_loss: 5448.0243\n",
      "Epoch 388/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4458.4878 - val_loss: 5392.3420\n",
      "Epoch 389/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4439.7728 - val_loss: 5389.0196\n",
      "Epoch 390/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4460.2778 - val_loss: 5389.0547\n",
      "Epoch 391/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4496.8463 - val_loss: 5404.0427\n",
      "Epoch 392/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4428.9106 - val_loss: 5421.3239\n",
      "Epoch 393/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4502.1908 - val_loss: 5347.9135\n",
      "Epoch 394/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4463.5607 - val_loss: 5385.6149\n",
      "Epoch 395/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4491.8833 - val_loss: 5424.0854\n",
      "Epoch 396/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4466.1096 - val_loss: 5383.2872\n",
      "Epoch 397/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4514.7805 - val_loss: 5396.1666\n",
      "Epoch 398/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4445.8408 - val_loss: 5426.1997\n",
      "Epoch 399/1000\n",
      "4000/4000 [==============================] - 0s 81us/step - loss: 4458.5883 - val_loss: 5392.1480\n",
      "Epoch 400/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4481.8225 - val_loss: 5389.7498\n",
      "Epoch 401/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4486.4767 - val_loss: 5433.9323\n",
      "Epoch 402/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4544.0570 - val_loss: 5406.4297\n",
      "Epoch 403/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4491.5808 - val_loss: 5375.8169\n",
      "Epoch 404/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4517.1766 - val_loss: 5376.1149\n",
      "Epoch 405/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4510.1051 - val_loss: 5373.5255\n",
      "Epoch 406/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4433.0514 - val_loss: 5441.5741\n",
      "Epoch 407/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4522.9330 - val_loss: 5368.3845\n",
      "Epoch 408/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4473.3754 - val_loss: 5371.9479\n",
      "Epoch 409/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4502.9660 - val_loss: 5396.6337\n",
      "Epoch 410/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4372.4569 - val_loss: 5362.9539\n",
      "Epoch 411/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4528.4667 - val_loss: 5458.9214\n",
      "Epoch 412/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4476.4380 - val_loss: 5413.3843\n",
      "Epoch 413/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4446.7606 - val_loss: 5405.8045\n",
      "Epoch 414/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4464.5959 - val_loss: 5410.6026\n",
      "Epoch 415/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4528.3221 - val_loss: 5421.5668\n",
      "Epoch 416/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4463.3496 - val_loss: 5360.3949\n",
      "Epoch 417/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4419.9387 - val_loss: 5436.6326\n",
      "Epoch 418/1000\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 4415.44 - 0s 90us/step - loss: 4458.8712 - val_loss: 5427.6683\n",
      "Epoch 419/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4378.1949 - val_loss: 5375.9725\n",
      "Epoch 420/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4477.7938 - val_loss: 5392.4330\n",
      "Epoch 421/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4492.5338 - val_loss: 5411.4348\n",
      "Epoch 422/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4509.5361 - val_loss: 5364.9104\n",
      "Epoch 423/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4462.7944 - val_loss: 5399.4558\n",
      "Epoch 424/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4442.9520 - val_loss: 5390.9077\n",
      "Epoch 425/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4461.6988 - val_loss: 5402.0009\n",
      "Epoch 426/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4468.1423 - val_loss: 5396.6845\n",
      "Epoch 427/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4442.9998 - val_loss: 5429.2665\n",
      "Epoch 428/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4459.4157 - val_loss: 5394.7169\n",
      "Epoch 429/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4485.8011 - val_loss: 5401.5479\n",
      "Epoch 430/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4460.5660 - val_loss: 5364.8150\n",
      "Epoch 431/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4543.2242 - val_loss: 5409.6203\n",
      "Epoch 432/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 85us/step - loss: 4449.7426 - val_loss: 5419.5771\n",
      "Epoch 433/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4501.1823 - val_loss: 5440.5243\n",
      "Epoch 434/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4435.5187 - val_loss: 5342.1314\n",
      "Epoch 435/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4520.7225 - val_loss: 5403.0576\n",
      "Epoch 436/1000\n",
      "4000/4000 [==============================] - 0s 79us/step - loss: 4442.3820 - val_loss: 5352.5971\n",
      "Epoch 437/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4494.3056 - val_loss: 5393.8635\n",
      "Epoch 438/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4467.4326 - val_loss: 5526.9327\n",
      "Epoch 439/1000\n",
      "4000/4000 [==============================] - 0s 80us/step - loss: 4516.9799 - val_loss: 5464.7355\n",
      "Epoch 440/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4486.4596 - val_loss: 5416.4445\n",
      "Epoch 441/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4477.5321 - val_loss: 5382.5429\n",
      "Epoch 442/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4455.9305 - val_loss: 5372.8326\n",
      "Epoch 443/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4464.5932 - val_loss: 5372.5247\n",
      "Epoch 444/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4479.3211 - val_loss: 5386.3686\n",
      "Epoch 445/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4476.6339 - val_loss: 5400.7143\n",
      "Epoch 446/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4517.3638 - val_loss: 5439.7990\n",
      "Epoch 447/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4464.7126 - val_loss: 5431.8457\n",
      "Epoch 448/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4550.2729 - val_loss: 5520.4314\n",
      "Epoch 449/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4485.4368 - val_loss: 5447.3244\n",
      "Epoch 450/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4535.6455 - val_loss: 5388.9702\n",
      "Epoch 451/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4449.6981 - val_loss: 5363.6480\n",
      "Epoch 452/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4434.5078 - val_loss: 5518.4401\n",
      "Epoch 453/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4433.3454 - val_loss: 5394.7837\n",
      "Epoch 454/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4453.1380 - val_loss: 5481.0877\n",
      "Epoch 455/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4374.1548 - val_loss: 5374.2988\n",
      "Epoch 456/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4454.1502 - val_loss: 5396.3405\n",
      "Epoch 457/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4443.0703 - val_loss: 5387.4460\n",
      "Epoch 458/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4503.1397 - val_loss: 5404.1713\n",
      "Epoch 459/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4407.2565 - val_loss: 5368.3198\n",
      "Epoch 460/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4485.8347 - val_loss: 5415.5137\n",
      "Epoch 461/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4487.5150 - val_loss: 5350.1415\n",
      "Epoch 462/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4455.0581 - val_loss: 5423.0862\n",
      "Epoch 463/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4491.7992 - val_loss: 5412.9150\n",
      "Epoch 464/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4449.9113 - val_loss: 5404.1683\n",
      "Epoch 465/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4491.7094 - val_loss: 5371.1579\n",
      "Epoch 466/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4494.2019 - val_loss: 5418.2964\n",
      "Epoch 467/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4499.5405 - val_loss: 5459.1868\n",
      "Epoch 468/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4428.3523 - val_loss: 5412.0759\n",
      "Epoch 469/1000\n",
      "4000/4000 [==============================] - 1s 158us/step - loss: 4495.3704 - val_loss: 5407.4304\n",
      "Epoch 470/1000\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 4480.4851 - val_loss: 5394.8706\n",
      "Epoch 471/1000\n",
      "4000/4000 [==============================] - 0s 119us/step - loss: 4459.2865 - val_loss: 5397.9028\n",
      "Epoch 472/1000\n",
      "4000/4000 [==============================] - 1s 161us/step - loss: 4526.5093 - val_loss: 5450.6623\n",
      "Epoch 473/1000\n",
      "4000/4000 [==============================] - 1s 134us/step - loss: 4452.5191 - val_loss: 5410.3957\n",
      "Epoch 474/1000\n",
      "4000/4000 [==============================] - 1s 125us/step - loss: 4471.1752 - val_loss: 5385.4224\n",
      "Epoch 475/1000\n",
      "4000/4000 [==============================] - 1s 142us/step - loss: 4437.7318 - val_loss: 5569.2945\n",
      "Epoch 476/1000\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 4465.6987 - val_loss: 5393.9528\n",
      "Epoch 477/1000\n",
      "4000/4000 [==============================] - 0s 115us/step - loss: 4478.4980 - val_loss: 5426.0670\n",
      "Epoch 478/1000\n",
      "4000/4000 [==============================] - 0s 114us/step - loss: 4490.9594 - val_loss: 5390.4545\n",
      "Epoch 479/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 4498.8656 - val_loss: 5416.9040\n",
      "Epoch 480/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4462.4207 - val_loss: 5385.1271\n",
      "Epoch 481/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4529.8399 - val_loss: 5379.6562\n",
      "Epoch 482/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4478.7917 - val_loss: 5376.1394\n",
      "Epoch 483/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4498.0883 - val_loss: 5361.3674\n",
      "Epoch 484/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4470.1445 - val_loss: 5383.9662\n",
      "Epoch 485/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4435.4088 - val_loss: 5363.9266\n",
      "Epoch 486/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4504.5939 - val_loss: 5477.4685\n",
      "Epoch 487/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4446.7602 - val_loss: 5377.6154\n",
      "Epoch 488/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4472.1462 - val_loss: 5420.8456\n",
      "Epoch 489/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4434.6369 - val_loss: 5566.7293\n",
      "Epoch 490/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4451.4830 - val_loss: 5375.9221\n",
      "Epoch 491/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4483.5671 - val_loss: 5403.9771\n",
      "Epoch 492/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4485.8126 - val_loss: 5380.0246\n",
      "Epoch 493/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4481.0023 - val_loss: 5400.7754\n",
      "Epoch 494/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4459.1301 - val_loss: 5421.9829\n",
      "Epoch 495/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4523.1757 - val_loss: 5381.6044\n",
      "Epoch 496/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4427.7242 - val_loss: 5390.4465\n",
      "Epoch 497/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4410.5487 - val_loss: 5429.1779\n",
      "Epoch 498/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4472.2195 - val_loss: 5395.9671\n",
      "Epoch 499/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4437.0541 - val_loss: 5380.5021\n",
      "Epoch 500/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4467.5563 - val_loss: 5490.8607\n",
      "Epoch 501/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4465.1373 - val_loss: 5395.9971\n",
      "Epoch 502/1000\n",
      "4000/4000 [==============================] - 0s 117us/step - loss: 4399.2114 - val_loss: 5350.8726\n",
      "Epoch 503/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4485.9567 - val_loss: 5352.1621\n",
      "Epoch 504/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 93us/step - loss: 4499.4221 - val_loss: 5421.7660\n",
      "Epoch 505/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4409.9589 - val_loss: 5389.5050\n",
      "Epoch 506/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4473.7721 - val_loss: 5434.6715\n",
      "Epoch 507/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4457.9010 - val_loss: 5377.6715\n",
      "Epoch 508/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4470.9994 - val_loss: 5436.7266\n",
      "Epoch 509/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4464.9382 - val_loss: 5461.6656\n",
      "Epoch 510/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4519.4768 - val_loss: 5362.4769\n",
      "Epoch 511/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4499.4084 - val_loss: 5386.1554\n",
      "Epoch 512/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4481.9423 - val_loss: 5377.8842\n",
      "Epoch 513/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4497.0734 - val_loss: 5389.7159\n",
      "Epoch 514/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4490.9074 - val_loss: 5345.8482\n",
      "Epoch 515/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4417.7037 - val_loss: 5399.0129\n",
      "Epoch 516/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4397.9977 - val_loss: 5417.2551\n",
      "Epoch 517/1000\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 4476.1481 - val_loss: 5406.8495\n",
      "Epoch 518/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4478.1353 - val_loss: 5367.9142\n",
      "Epoch 519/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4515.1250 - val_loss: 5428.5417\n",
      "Epoch 520/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4484.1134 - val_loss: 5383.3052\n",
      "Epoch 521/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4499.6345 - val_loss: 5384.0910\n",
      "Epoch 522/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4460.9369 - val_loss: 5377.5365\n",
      "Epoch 523/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4448.5832 - val_loss: 5584.8066\n",
      "Epoch 524/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4402.0909 - val_loss: 5402.9411\n",
      "Epoch 525/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4460.0351 - val_loss: 5367.8502\n",
      "Epoch 526/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4454.1825 - val_loss: 5378.5318\n",
      "Epoch 527/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4445.3366 - val_loss: 5356.6313\n",
      "Epoch 528/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4469.8580 - val_loss: 5416.0040\n",
      "Epoch 529/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4401.5664 - val_loss: 5380.2203\n",
      "Epoch 530/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4429.9279 - val_loss: 5417.8195\n",
      "Epoch 531/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4437.4714 - val_loss: 5412.9628\n",
      "Epoch 532/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4483.8652 - val_loss: 5376.9871\n",
      "Epoch 533/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4443.7732 - val_loss: 5396.4193\n",
      "Epoch 534/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4440.6879 - val_loss: 5411.0815\n",
      "Epoch 535/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4392.8036 - val_loss: 5360.0283\n",
      "Epoch 536/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4444.1349 - val_loss: 5352.1611\n",
      "Epoch 537/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4409.9105 - val_loss: 5417.1766\n",
      "Epoch 538/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4450.2298 - val_loss: 5424.1040\n",
      "Epoch 539/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4476.5496 - val_loss: 5363.7161\n",
      "Epoch 540/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4471.6395 - val_loss: 5415.1363\n",
      "Epoch 541/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4505.6029 - val_loss: 5431.9317\n",
      "Epoch 542/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4432.2004 - val_loss: 5340.7238\n",
      "Epoch 543/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4479.5855 - val_loss: 5355.3057\n",
      "Epoch 544/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4479.6439 - val_loss: 5381.0888\n",
      "Epoch 545/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4449.9890 - val_loss: 5431.4046\n",
      "Epoch 546/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4430.6788 - val_loss: 5379.7329\n",
      "Epoch 547/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4480.8744 - val_loss: 5434.1667\n",
      "Epoch 548/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4475.6211 - val_loss: 5343.8204\n",
      "Epoch 549/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4421.2349 - val_loss: 5425.6726\n",
      "Epoch 550/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4397.6327 - val_loss: 5362.5689\n",
      "Epoch 551/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4486.2553 - val_loss: 5414.9396\n",
      "Epoch 552/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4409.4805 - val_loss: 5387.8653\n",
      "Epoch 553/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4431.8421 - val_loss: 5429.1787\n",
      "Epoch 554/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4421.4911 - val_loss: 5372.7963\n",
      "Epoch 555/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4451.5368 - val_loss: 5387.6259\n",
      "Epoch 556/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4465.5334 - val_loss: 5364.8277\n",
      "Epoch 557/1000\n",
      "4000/4000 [==============================] - 0s 115us/step - loss: 4474.1197 - val_loss: 5393.6994\n",
      "Epoch 558/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4416.9106 - val_loss: 5392.9215\n",
      "Epoch 559/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4452.8209 - val_loss: 5383.9037\n",
      "Epoch 560/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4497.3024 - val_loss: 5395.4172\n",
      "Epoch 561/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4453.7692 - val_loss: 5366.8758\n",
      "Epoch 562/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4465.9565 - val_loss: 5389.8899\n",
      "Epoch 563/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4382.7625 - val_loss: 5390.1791\n",
      "Epoch 564/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4457.6714 - val_loss: 5397.6915\n",
      "Epoch 565/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4492.3770 - val_loss: 5394.4927\n",
      "Epoch 566/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4418.7346 - val_loss: 5358.8698\n",
      "Epoch 567/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4460.7083 - val_loss: 5355.6069\n",
      "Epoch 568/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4425.7668 - val_loss: 5401.8628\n",
      "Epoch 569/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4453.9910 - val_loss: 5400.9432\n",
      "Epoch 570/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4431.4553 - val_loss: 5433.0948\n",
      "Epoch 571/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4461.4177 - val_loss: 5370.6064\n",
      "Epoch 572/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4455.3821 - val_loss: 5383.1775\n",
      "Epoch 573/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4456.2133 - val_loss: 5437.5719\n",
      "Epoch 574/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4453.2622 - val_loss: 5369.8506\n",
      "Epoch 575/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4501.5906 - val_loss: 5407.7315\n",
      "Epoch 576/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 89us/step - loss: 4463.4422 - val_loss: 5396.1573\n",
      "Epoch 577/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4448.7010 - val_loss: 5474.6087\n",
      "Epoch 578/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4425.9074 - val_loss: 5377.1432\n",
      "Epoch 579/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4429.9430 - val_loss: 5419.1120\n",
      "Epoch 580/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4438.4533 - val_loss: 5425.5659\n",
      "Epoch 581/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4450.7489 - val_loss: 5381.4534\n",
      "Epoch 582/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4448.2468 - val_loss: 5372.0631\n",
      "Epoch 583/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4396.8852 - val_loss: 5449.7850\n",
      "Epoch 584/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4432.6230 - val_loss: 5370.1627\n",
      "Epoch 585/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4390.0140 - val_loss: 5362.7963\n",
      "Epoch 586/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4390.2248 - val_loss: 5370.6649\n",
      "Epoch 587/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4421.1093 - val_loss: 5421.7290\n",
      "Epoch 588/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4438.7320 - val_loss: 5369.2098\n",
      "Epoch 589/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4487.8123 - val_loss: 5423.2034\n",
      "Epoch 590/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4438.2631 - val_loss: 5382.6703\n",
      "Epoch 591/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4417.4196 - val_loss: 5403.2891\n",
      "Epoch 592/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4433.5442 - val_loss: 5425.2530\n",
      "Epoch 593/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4490.0869 - val_loss: 5368.8086\n",
      "Epoch 594/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4456.9698 - val_loss: 5389.9985\n",
      "Epoch 595/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4453.8782 - val_loss: 5385.0351\n",
      "Epoch 596/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4476.5182 - val_loss: 5399.3819\n",
      "Epoch 597/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4494.9054 - val_loss: 5412.2567\n",
      "Epoch 598/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4468.6571 - val_loss: 5371.2916\n",
      "Epoch 599/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4423.1859 - val_loss: 5456.2807\n",
      "Epoch 600/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4448.8150 - val_loss: 5432.5813\n",
      "Epoch 601/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4447.8607 - val_loss: 5403.9559\n",
      "Epoch 602/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4485.9383 - val_loss: 5376.2958\n",
      "Epoch 603/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4379.4796 - val_loss: 5399.6270\n",
      "Epoch 604/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4414.1211 - val_loss: 5403.8250\n",
      "Epoch 605/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4441.1436 - val_loss: 5387.1668\n",
      "Epoch 606/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4388.5553 - val_loss: 5440.8119\n",
      "Epoch 607/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4503.1903 - val_loss: 5480.8064\n",
      "Epoch 608/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4430.4545 - val_loss: 5465.1474\n",
      "Epoch 609/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4420.0863 - val_loss: 5423.7333\n",
      "Epoch 610/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4437.7477 - val_loss: 5377.3689\n",
      "Epoch 611/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4483.7469 - val_loss: 5429.8234\n",
      "Epoch 612/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4429.5398 - val_loss: 5403.2381\n",
      "Epoch 613/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4383.9075 - val_loss: 5365.3574\n",
      "Epoch 614/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4434.0547 - val_loss: 5369.2821\n",
      "Epoch 615/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4417.8952 - val_loss: 5393.7934\n",
      "Epoch 616/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4495.0684 - val_loss: 5412.2503\n",
      "Epoch 617/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4487.3246 - val_loss: 5398.8632\n",
      "Epoch 618/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4450.5942 - val_loss: 5426.6816\n",
      "Epoch 619/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4460.1871 - val_loss: 5386.6553\n",
      "Epoch 620/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4412.7649 - val_loss: 5417.3865\n",
      "Epoch 621/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4422.1311 - val_loss: 5402.8521\n",
      "Epoch 622/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4465.5200 - val_loss: 5379.0956\n",
      "Epoch 623/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4429.2702 - val_loss: 5439.7533\n",
      "Epoch 624/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4468.2516 - val_loss: 5452.1016\n",
      "Epoch 625/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4434.3187 - val_loss: 5459.9145\n",
      "Epoch 626/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4431.5284 - val_loss: 5423.0973\n",
      "Epoch 627/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4463.4403 - val_loss: 5392.3274\n",
      "Epoch 628/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4428.4968 - val_loss: 5391.1203\n",
      "Epoch 629/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4335.1399 - val_loss: 5403.9391\n",
      "Epoch 630/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4411.0592 - val_loss: 5371.0102\n",
      "Epoch 631/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4435.9438 - val_loss: 5386.9117\n",
      "Epoch 632/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4416.5552 - val_loss: 5415.8860\n",
      "Epoch 633/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4406.5812 - val_loss: 5404.7220\n",
      "Epoch 634/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4411.1055 - val_loss: 5425.7799\n",
      "Epoch 635/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4465.9602 - val_loss: 5429.9372\n",
      "Epoch 636/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4408.2060 - val_loss: 5473.2204\n",
      "Epoch 637/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4440.8942 - val_loss: 5384.1461\n",
      "Epoch 638/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4453.0349 - val_loss: 5372.3603\n",
      "Epoch 639/1000\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 4441.6082 - val_loss: 5401.2779\n",
      "Epoch 640/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 4427.4781 - val_loss: 5378.7041\n",
      "Epoch 641/1000\n",
      "4000/4000 [==============================] - 1s 138us/step - loss: 4413.9756 - val_loss: 5382.8221\n",
      "Epoch 642/1000\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 4442.7028 - val_loss: 5468.0058\n",
      "Epoch 643/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4462.0604 - val_loss: 5408.4523\n",
      "Epoch 644/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4474.8666 - val_loss: 5364.7978\n",
      "Epoch 645/1000\n",
      "4000/4000 [==============================] - 0s 115us/step - loss: 4427.2037 - val_loss: 5378.2220\n",
      "Epoch 646/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4431.8273 - val_loss: 5406.0747\n",
      "Epoch 647/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4414.0371 - val_loss: 5475.7402\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 117us/step - loss: 4435.9402 - val_loss: 5425.9079\n",
      "Epoch 649/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4421.4929 - val_loss: 5401.5181\n",
      "Epoch 650/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4446.3529 - val_loss: 5407.4146\n",
      "Epoch 651/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4415.6810 - val_loss: 5403.9889\n",
      "Epoch 652/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4423.2263 - val_loss: 5378.7601\n",
      "Epoch 653/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4421.9801 - val_loss: 5443.3149\n",
      "Epoch 654/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4476.6999 - val_loss: 5359.9456\n",
      "Epoch 655/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4493.6966 - val_loss: 5356.7496\n",
      "Epoch 656/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4431.2597 - val_loss: 5445.3602\n",
      "Epoch 657/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4494.8803 - val_loss: 5408.1050\n",
      "Epoch 658/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4359.0143 - val_loss: 5367.8512\n",
      "Epoch 659/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4435.4813 - val_loss: 5428.7197\n",
      "Epoch 660/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4438.2019 - val_loss: 5386.4654\n",
      "Epoch 661/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4408.7840 - val_loss: 5420.6377\n",
      "Epoch 662/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4447.2345 - val_loss: 5393.4883\n",
      "Epoch 663/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4447.2102 - val_loss: 5508.0131\n",
      "Epoch 664/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4422.4180 - val_loss: 5407.7397\n",
      "Epoch 665/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4445.0749 - val_loss: 5413.6880\n",
      "Epoch 666/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4482.1968 - val_loss: 5360.6665\n",
      "Epoch 667/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4427.2970 - val_loss: 5433.6771\n",
      "Epoch 668/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4505.7715 - val_loss: 5416.4903\n",
      "Epoch 669/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4487.4528 - val_loss: 5367.5332\n",
      "Epoch 670/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4448.9685 - val_loss: 5391.7115\n",
      "Epoch 671/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4441.6346 - val_loss: 5379.7271\n",
      "Epoch 672/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4486.0975 - val_loss: 5416.3272\n",
      "Epoch 673/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4412.2849 - val_loss: 5383.4625\n",
      "Epoch 674/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4348.8302 - val_loss: 5366.4325\n",
      "Epoch 675/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4459.9158 - val_loss: 5402.6205\n",
      "Epoch 676/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4459.2901 - val_loss: 5402.8778\n",
      "Epoch 677/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4459.9694 - val_loss: 5458.3595\n",
      "Epoch 678/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4358.1358 - val_loss: 5478.4915\n",
      "Epoch 679/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4385.6239 - val_loss: 5372.7583\n",
      "Epoch 680/1000\n",
      "4000/4000 [==============================] - 0s 115us/step - loss: 4491.4029 - val_loss: 5351.7205\n",
      "Epoch 681/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4487.9958 - val_loss: 5371.4909\n",
      "Epoch 682/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4464.2494 - val_loss: 5388.2778\n",
      "Epoch 683/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4424.5370 - val_loss: 5352.5311\n",
      "Epoch 684/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4440.1351 - val_loss: 5353.1626\n",
      "Epoch 685/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4393.5378 - val_loss: 5421.2657\n",
      "Epoch 686/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4466.1688 - val_loss: 5400.5020\n",
      "Epoch 687/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4422.3703 - val_loss: 5367.6970\n",
      "Epoch 688/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4440.1680 - val_loss: 5372.3867\n",
      "Epoch 689/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4468.6426 - val_loss: 5419.8607\n",
      "Epoch 690/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4445.4572 - val_loss: 5366.1002\n",
      "Epoch 691/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4436.3926 - val_loss: 5396.2868\n",
      "Epoch 692/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4395.9514 - val_loss: 5386.5346\n",
      "Epoch 693/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4398.3127 - val_loss: 5381.9474\n",
      "Epoch 694/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4459.0812 - val_loss: 5408.3809\n",
      "Epoch 695/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4496.8359 - val_loss: 5371.2889\n",
      "Epoch 696/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4419.1480 - val_loss: 5363.6398\n",
      "Epoch 697/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4450.4244 - val_loss: 5380.0769\n",
      "Epoch 698/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4429.7870 - val_loss: 5380.8040\n",
      "Epoch 699/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4489.1511 - val_loss: 5387.2639\n",
      "Epoch 700/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4460.1210 - val_loss: 5430.5476\n",
      "Epoch 701/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4410.0128 - val_loss: 5370.9181\n",
      "Epoch 702/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4407.2841 - val_loss: 5384.5512\n",
      "Epoch 703/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4391.5960 - val_loss: 5378.0933\n",
      "Epoch 704/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4475.1637 - val_loss: 5421.2538\n",
      "Epoch 705/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4376.7397 - val_loss: 5379.4703\n",
      "Epoch 706/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4403.7346 - val_loss: 5373.2820\n",
      "Epoch 707/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4448.0823 - val_loss: 5387.2569\n",
      "Epoch 708/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4451.0394 - val_loss: 5381.4647\n",
      "Epoch 709/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4494.4064 - val_loss: 5465.3052\n",
      "Epoch 710/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4396.2525 - val_loss: 5390.6334\n",
      "Epoch 711/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4474.2180 - val_loss: 5420.7768\n",
      "Epoch 712/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4397.0007 - val_loss: 5390.4653\n",
      "Epoch 713/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4458.5215 - val_loss: 5367.5423\n",
      "Epoch 714/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4428.8235 - val_loss: 5362.7746\n",
      "Epoch 715/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4452.4822 - val_loss: 5371.6500\n",
      "Epoch 716/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4420.4837 - val_loss: 5382.8303\n",
      "Epoch 717/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4455.9812 - val_loss: 5374.3140\n",
      "Epoch 718/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 4432.2230 - val_loss: 5362.6714\n",
      "Epoch 719/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4404.9645 - val_loss: 5379.7277\n",
      "Epoch 720/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 110us/step - loss: 4391.4126 - val_loss: 5415.2778\n",
      "Epoch 721/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4421.1484 - val_loss: 5395.4688\n",
      "Epoch 722/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4466.4957 - val_loss: 5410.6093\n",
      "Epoch 723/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4447.0259 - val_loss: 5386.8160\n",
      "Epoch 724/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4459.1019 - val_loss: 5384.9667\n",
      "Epoch 725/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4408.2677 - val_loss: 5395.1689\n",
      "Epoch 726/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4472.3421 - val_loss: 5381.3178\n",
      "Epoch 727/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4404.1388 - val_loss: 5384.4628\n",
      "Epoch 728/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4415.7584 - val_loss: 5417.4525\n",
      "Epoch 729/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4478.2543 - val_loss: 5439.1410\n",
      "Epoch 730/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4363.1572 - val_loss: 5403.7651\n",
      "Epoch 731/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4443.2275 - val_loss: 5362.1678\n",
      "Epoch 732/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4432.5676 - val_loss: 5364.7092\n",
      "Epoch 733/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4422.8657 - val_loss: 5348.6189\n",
      "Epoch 734/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4437.5578 - val_loss: 5386.8041\n",
      "Epoch 735/1000\n",
      "4000/4000 [==============================] - 0s 116us/step - loss: 4467.0062 - val_loss: 5387.8648\n",
      "Epoch 736/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4451.8114 - val_loss: 5393.1436\n",
      "Epoch 737/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4500.2203 - val_loss: 5363.6168\n",
      "Epoch 738/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4430.9703 - val_loss: 5387.8647\n",
      "Epoch 739/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4440.7008 - val_loss: 5372.8735\n",
      "Epoch 740/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4458.0353 - val_loss: 5349.7508\n",
      "Epoch 741/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4436.6080 - val_loss: 5391.0509\n",
      "Epoch 742/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4424.4793 - val_loss: 5357.3502\n",
      "Epoch 743/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4395.8854 - val_loss: 5348.4296\n",
      "Epoch 744/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4405.6631 - val_loss: 5367.1521\n",
      "Epoch 745/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4429.1938 - val_loss: 5376.5643\n",
      "Epoch 746/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4395.5815 - val_loss: 5359.6041\n",
      "Epoch 747/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4427.3265 - val_loss: 5356.7678\n",
      "Epoch 748/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4426.0135 - val_loss: 5422.4753\n",
      "Epoch 749/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4449.5630 - val_loss: 5369.3397\n",
      "Epoch 750/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4459.9205 - val_loss: 5370.8940\n",
      "Epoch 751/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4372.9226 - val_loss: 5356.1874\n",
      "Epoch 752/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4403.8448 - val_loss: 5379.1270\n",
      "Epoch 753/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4394.6461 - val_loss: 5393.2401\n",
      "Epoch 754/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4426.5806 - val_loss: 5395.2969\n",
      "Epoch 755/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4407.5185 - val_loss: 5354.5886\n",
      "Epoch 756/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4412.8475 - val_loss: 5390.4114\n",
      "Epoch 757/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4411.7715 - val_loss: 5382.2473\n",
      "Epoch 758/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4426.2019 - val_loss: 5370.4429\n",
      "Epoch 759/1000\n",
      "4000/4000 [==============================] - 0s 119us/step - loss: 4345.3149 - val_loss: 5436.5286\n",
      "Epoch 760/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4409.6307 - val_loss: 5396.1442\n",
      "Epoch 761/1000\n",
      "4000/4000 [==============================] - 0s 118us/step - loss: 4440.0746 - val_loss: 5354.0462\n",
      "Epoch 762/1000\n",
      "4000/4000 [==============================] - 0s 123us/step - loss: 4449.7528 - val_loss: 5381.4472\n",
      "Epoch 763/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4366.3566 - val_loss: 5366.3733\n",
      "Epoch 764/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4449.8315 - val_loss: 5374.8505\n",
      "Epoch 765/1000\n",
      "4000/4000 [==============================] - 0s 117us/step - loss: 4395.9122 - val_loss: 5361.9228\n",
      "Epoch 766/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4426.6417 - val_loss: 5426.8190\n",
      "Epoch 767/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4477.4972 - val_loss: 5355.9809\n",
      "Epoch 768/1000\n",
      "4000/4000 [==============================] - 0s 119us/step - loss: 4415.8843 - val_loss: 5364.5297\n",
      "Epoch 769/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4412.2962 - val_loss: 5400.6261\n",
      "Epoch 770/1000\n",
      "4000/4000 [==============================] - 0s 115us/step - loss: 4416.5797 - val_loss: 5404.9228\n",
      "Epoch 771/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4449.4509 - val_loss: 5399.7804\n",
      "Epoch 772/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4477.9515 - val_loss: 5409.4651\n",
      "Epoch 773/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4464.4669 - val_loss: 5359.6066\n",
      "Epoch 774/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4434.4352 - val_loss: 5384.6305\n",
      "Epoch 775/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4425.9574 - val_loss: 5382.7962\n",
      "Epoch 776/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4375.9936 - val_loss: 5396.8769\n",
      "Epoch 777/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4421.3865 - val_loss: 5386.7870\n",
      "Epoch 778/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4443.8940 - val_loss: 5397.9800\n",
      "Epoch 779/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4438.2741 - val_loss: 5379.6241\n",
      "Epoch 780/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4454.5806 - val_loss: 5422.3816\n",
      "Epoch 781/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4473.7519 - val_loss: 5385.0746\n",
      "Epoch 782/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4365.3391 - val_loss: 5406.9195\n",
      "Epoch 783/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4408.8598 - val_loss: 5388.6416\n",
      "Epoch 784/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4429.8481 - val_loss: 5376.3241\n",
      "Epoch 785/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4450.4202 - val_loss: 5360.1714\n",
      "Epoch 786/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4386.5702 - val_loss: 5408.5506\n",
      "Epoch 787/1000\n",
      "4000/4000 [==============================] - 0s 108us/step - loss: 4396.5500 - val_loss: 5394.5847\n",
      "Epoch 788/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4423.7486 - val_loss: 5376.9895\n",
      "Epoch 789/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4466.4183 - val_loss: 5370.7448\n",
      "Epoch 790/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4387.6508 - val_loss: 5409.0670\n",
      "Epoch 791/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4456.1731 - val_loss: 5371.2182\n",
      "Epoch 792/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 91us/step - loss: 4416.2644 - val_loss: 5373.4082\n",
      "Epoch 793/1000\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 4271.6531 - val_loss: 5381.4559\n",
      "Epoch 794/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4507.9280 - val_loss: 5409.4658\n",
      "Epoch 795/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4399.8217 - val_loss: 5365.6190\n",
      "Epoch 796/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4367.4297 - val_loss: 5392.4682\n",
      "Epoch 797/1000\n",
      "4000/4000 [==============================] - 0s 118us/step - loss: 4462.1922 - val_loss: 5344.4507\n",
      "Epoch 798/1000\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 4377.8482 - val_loss: 5388.2820\n",
      "Epoch 799/1000\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 4471.9285 - val_loss: 5378.4249\n",
      "Epoch 800/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4402.0114 - val_loss: 5388.9145\n",
      "Epoch 801/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4441.4025 - val_loss: 5387.5495\n",
      "Epoch 802/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4452.8249 - val_loss: 5405.6628\n",
      "Epoch 803/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4444.3271 - val_loss: 5397.2407\n",
      "Epoch 804/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4414.5523 - val_loss: 5405.0804\n",
      "Epoch 805/1000\n",
      "4000/4000 [==============================] - 0s 89us/step - loss: 4429.2255 - val_loss: 5400.6006\n",
      "Epoch 806/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4388.6706 - val_loss: 5434.4639\n",
      "Epoch 807/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4420.7907 - val_loss: 5389.6701\n",
      "Epoch 808/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4345.6330 - val_loss: 5384.4941\n",
      "Epoch 809/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4445.8934 - val_loss: 5431.1805\n",
      "Epoch 810/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4366.5418 - val_loss: 5385.2763\n",
      "Epoch 811/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4383.8395 - val_loss: 5404.6737\n",
      "Epoch 812/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4410.1316 - val_loss: 5372.6007\n",
      "Epoch 813/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4383.7836 - val_loss: 5402.3659\n",
      "Epoch 814/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4359.5363 - val_loss: 5396.5554\n",
      "Epoch 815/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4393.5537 - val_loss: 5450.3654\n",
      "Epoch 816/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4424.8631 - val_loss: 5423.2546\n",
      "Epoch 817/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4390.4626 - val_loss: 5410.0810\n",
      "Epoch 818/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4416.5481 - val_loss: 5397.4111\n",
      "Epoch 819/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4431.3386 - val_loss: 5422.9946\n",
      "Epoch 820/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4405.9535 - val_loss: 5388.3433\n",
      "Epoch 821/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4381.4318 - val_loss: 5393.7605\n",
      "Epoch 822/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4371.9420 - val_loss: 5398.0396\n",
      "Epoch 823/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4336.6813 - val_loss: 5380.6945\n",
      "Epoch 824/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4422.0236 - val_loss: 5379.7833\n",
      "Epoch 825/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4400.9377 - val_loss: 5425.6147\n",
      "Epoch 826/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4411.6999 - val_loss: 5385.0764\n",
      "Epoch 827/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4385.2371 - val_loss: 5404.7864\n",
      "Epoch 828/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4430.9087 - val_loss: 5451.1784\n",
      "Epoch 829/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4430.7462 - val_loss: 5435.3249\n",
      "Epoch 830/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4420.3854 - val_loss: 5424.4287\n",
      "Epoch 831/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4417.5362 - val_loss: 5407.1667\n",
      "Epoch 832/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4423.5110 - val_loss: 5429.6519\n",
      "Epoch 833/1000\n",
      "4000/4000 [==============================] - 0s 119us/step - loss: 4369.9724 - val_loss: 5396.0098\n",
      "Epoch 834/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4390.0169 - val_loss: 5415.8086\n",
      "Epoch 835/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4446.6325 - val_loss: 5431.3095\n",
      "Epoch 836/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4422.4517 - val_loss: 5429.2281\n",
      "Epoch 837/1000\n",
      "4000/4000 [==============================] - 0s 114us/step - loss: 4379.7735 - val_loss: 5395.6501\n",
      "Epoch 838/1000\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 4417.6210 - val_loss: 5394.7096\n",
      "Epoch 839/1000\n",
      "4000/4000 [==============================] - 1s 167us/step - loss: 4374.0617 - val_loss: 5410.5803\n",
      "Epoch 840/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4414.9471 - val_loss: 5458.2709\n",
      "Epoch 841/1000\n",
      "4000/4000 [==============================] - 1s 128us/step - loss: 4364.6796 - val_loss: 5420.8862\n",
      "Epoch 842/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4455.3124 - val_loss: 5378.3708\n",
      "Epoch 843/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4417.1655 - val_loss: 5416.5856\n",
      "Epoch 844/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4414.6243 - val_loss: 5429.1802\n",
      "Epoch 845/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4368.6194 - val_loss: 5393.1642\n",
      "Epoch 846/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4353.9099 - val_loss: 5407.2833\n",
      "Epoch 847/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4412.2418 - val_loss: 5406.5902\n",
      "Epoch 848/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4415.7814 - val_loss: 5409.1541\n",
      "Epoch 849/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4366.5145 - val_loss: 5417.5827\n",
      "Epoch 850/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4368.8247 - val_loss: 5388.0750\n",
      "Epoch 851/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4418.4131 - val_loss: 5401.1150\n",
      "Epoch 852/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4422.4363 - val_loss: 5439.3033\n",
      "Epoch 853/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4401.2615 - val_loss: 5409.9873\n",
      "Epoch 854/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4461.6011 - val_loss: 5412.8978\n",
      "Epoch 855/1000\n",
      "4000/4000 [==============================] - 0s 116us/step - loss: 4389.7290 - val_loss: 5445.0493\n",
      "Epoch 856/1000\n",
      "4000/4000 [==============================] - 0s 114us/step - loss: 4345.3082 - val_loss: 5421.4061\n",
      "Epoch 857/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4385.2735 - val_loss: 5440.8481\n",
      "Epoch 858/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4433.0216 - val_loss: 5438.8229\n",
      "Epoch 859/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4398.0419 - val_loss: 5406.3713\n",
      "Epoch 860/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4392.9281 - val_loss: 5419.4829\n",
      "Epoch 861/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4372.3001 - val_loss: 5417.8205\n",
      "Epoch 862/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4375.1531 - val_loss: 5428.3846\n",
      "Epoch 863/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4456.3236 - val_loss: 5562.7962\n",
      "Epoch 864/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 85us/step - loss: 4407.8853 - val_loss: 5419.6621\n",
      "Epoch 865/1000\n",
      "4000/4000 [==============================] - 0s 82us/step - loss: 4431.3834 - val_loss: 5423.5169\n",
      "Epoch 866/1000\n",
      "4000/4000 [==============================] - 0s 83us/step - loss: 4462.3807 - val_loss: 5472.4252\n",
      "Epoch 867/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4479.2413 - val_loss: 5379.7358\n",
      "Epoch 868/1000\n",
      "4000/4000 [==============================] - 0s 106us/step - loss: 4390.4747 - val_loss: 5429.2641\n",
      "Epoch 869/1000\n",
      "4000/4000 [==============================] - 1s 136us/step - loss: 4396.4971 - val_loss: 5408.7191\n",
      "Epoch 870/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4443.0540 - val_loss: 5399.3017\n",
      "Epoch 871/1000\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 4440.6188 - val_loss: 5465.5981\n",
      "Epoch 872/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4443.8577 - val_loss: 5448.2657\n",
      "Epoch 873/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4429.8838 - val_loss: 5422.3940\n",
      "Epoch 874/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4367.8401 - val_loss: 5421.4945\n",
      "Epoch 875/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4404.1628 - val_loss: 5456.8244\n",
      "Epoch 876/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4374.5005 - val_loss: 5458.7923\n",
      "Epoch 877/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4387.8115 - val_loss: 5393.9540\n",
      "Epoch 878/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4433.7867 - val_loss: 5397.0172\n",
      "Epoch 879/1000\n",
      "4000/4000 [==============================] - 1s 137us/step - loss: 4348.3341 - val_loss: 5425.0034\n",
      "Epoch 880/1000\n",
      "4000/4000 [==============================] - 1s 135us/step - loss: 4391.1107 - val_loss: 5437.1275\n",
      "Epoch 881/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4409.1616 - val_loss: 5423.4597\n",
      "Epoch 882/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4424.4870 - val_loss: 5420.8942\n",
      "Epoch 883/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4394.6977 - val_loss: 5469.3705\n",
      "Epoch 884/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4412.4942 - val_loss: 5495.5894\n",
      "Epoch 885/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4407.2522 - val_loss: 5407.7618\n",
      "Epoch 886/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4435.5490 - val_loss: 5459.9397\n",
      "Epoch 887/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4417.5602 - val_loss: 5457.2743\n",
      "Epoch 888/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4476.9919 - val_loss: 5412.9660\n",
      "Epoch 889/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4370.5015 - val_loss: 5428.7642\n",
      "Epoch 890/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4405.6279 - val_loss: 5421.8309\n",
      "Epoch 891/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4434.7867 - val_loss: 5437.7529\n",
      "Epoch 892/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4348.6824 - val_loss: 5440.4247\n",
      "Epoch 893/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 4347.9058 - val_loss: 5409.3739\n",
      "Epoch 894/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4420.2206 - val_loss: 5450.5555\n",
      "Epoch 895/1000\n",
      "4000/4000 [==============================] - 0s 120us/step - loss: 4395.9456 - val_loss: 5458.7997\n",
      "Epoch 896/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4405.5417 - val_loss: 5433.6169\n",
      "Epoch 897/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4308.5125 - val_loss: 5403.4189\n",
      "Epoch 898/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4418.8122 - val_loss: 5430.5659\n",
      "Epoch 899/1000\n",
      "4000/4000 [==============================] - 0s 119us/step - loss: 4388.9254 - val_loss: 5417.6706\n",
      "Epoch 900/1000\n",
      "4000/4000 [==============================] - 0s 114us/step - loss: 4357.7647 - val_loss: 5430.0186\n",
      "Epoch 901/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4458.2043 - val_loss: 5420.0271\n",
      "Epoch 902/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4390.2363 - val_loss: 5407.1693\n",
      "Epoch 903/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4424.0758 - val_loss: 5418.5065\n",
      "Epoch 904/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4417.1489 - val_loss: 5432.2240\n",
      "Epoch 905/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4401.9435 - val_loss: 5429.3016\n",
      "Epoch 906/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4450.5746 - val_loss: 5435.9617\n",
      "Epoch 907/1000\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 4385.0777 - val_loss: 5437.6781\n",
      "Epoch 908/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4460.5696 - val_loss: 5420.2405\n",
      "Epoch 909/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4424.6018 - val_loss: 5440.3131\n",
      "Epoch 910/1000\n",
      "4000/4000 [==============================] - 0s 107us/step - loss: 4437.0900 - val_loss: 5464.7779\n",
      "Epoch 911/1000\n",
      "4000/4000 [==============================] - 1s 146us/step - loss: 4394.5734 - val_loss: 5438.6224\n",
      "Epoch 912/1000\n",
      "4000/4000 [==============================] - 1s 167us/step - loss: 4365.3346 - val_loss: 5421.4697\n",
      "Epoch 913/1000\n",
      "4000/4000 [==============================] - 1s 151us/step - loss: 4374.9522 - val_loss: 5423.2004\n",
      "Epoch 914/1000\n",
      "4000/4000 [==============================] - 0s 120us/step - loss: 4393.9275 - val_loss: 5433.5584\n",
      "Epoch 915/1000\n",
      "4000/4000 [==============================] - 1s 131us/step - loss: 4408.9215 - val_loss: 5440.2787\n",
      "Epoch 916/1000\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 4400.5073 - val_loss: 5394.8415\n",
      "Epoch 917/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4417.5093 - val_loss: 5413.9822\n",
      "Epoch 918/1000\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 4426.9044 - val_loss: 5448.2972\n",
      "Epoch 919/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4406.3170 - val_loss: 5434.0408\n",
      "Epoch 920/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4347.7003 - val_loss: 5433.0075\n",
      "Epoch 921/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4331.9134 - val_loss: 5444.7880\n",
      "Epoch 922/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4388.2665 - val_loss: 5424.1667\n",
      "Epoch 923/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4387.8177 - val_loss: 5406.9544\n",
      "Epoch 924/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4375.5491 - val_loss: 5486.4977\n",
      "Epoch 925/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4387.0900 - val_loss: 5429.9881\n",
      "Epoch 926/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4360.1713 - val_loss: 5443.1210\n",
      "Epoch 927/1000\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 4443.7733 - val_loss: 5478.4394\n",
      "Epoch 928/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4431.0367 - val_loss: 5438.4053\n",
      "Epoch 929/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4370.5901 - val_loss: 5450.6383\n",
      "Epoch 930/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4390.5246 - val_loss: 5426.7723\n",
      "Epoch 931/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4373.4294 - val_loss: 5424.0399\n",
      "Epoch 932/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4358.3729 - val_loss: 5467.1049\n",
      "Epoch 933/1000\n",
      "4000/4000 [==============================] - 0s 102us/step - loss: 4380.1701 - val_loss: 5408.0025\n",
      "Epoch 934/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4422.3141 - val_loss: 5491.3907\n",
      "Epoch 935/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4409.9215 - val_loss: 5460.2650\n",
      "Epoch 936/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 91us/step - loss: 4429.1229 - val_loss: 5433.8244\n",
      "Epoch 937/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4346.0364 - val_loss: 5461.2265\n",
      "Epoch 938/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4409.7000 - val_loss: 5432.3050\n",
      "Epoch 939/1000\n",
      "4000/4000 [==============================] - 0s 84us/step - loss: 4358.6786 - val_loss: 5435.8490\n",
      "Epoch 940/1000\n",
      "4000/4000 [==============================] - 0s 86us/step - loss: 4402.0036 - val_loss: 5451.0161\n",
      "Epoch 941/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4455.3791 - val_loss: 5443.1600\n",
      "Epoch 942/1000\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 4334.0008 - val_loss: 5446.6311\n",
      "Epoch 943/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4415.1642 - val_loss: 5429.7771\n",
      "Epoch 944/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4378.8605 - val_loss: 5425.9534\n",
      "Epoch 945/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4420.3344 - val_loss: 5419.6263\n",
      "Epoch 946/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4375.9585 - val_loss: 5437.5494\n",
      "Epoch 947/1000\n",
      "4000/4000 [==============================] - 0s 90us/step - loss: 4441.7412 - val_loss: 5436.6486\n",
      "Epoch 948/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4393.4850 - val_loss: 5442.4685\n",
      "Epoch 949/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4348.5735 - val_loss: 5459.7441\n",
      "Epoch 950/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4456.9499 - val_loss: 5443.4387\n",
      "Epoch 951/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4329.1174 - val_loss: 5446.8477\n",
      "Epoch 952/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4394.2691 - val_loss: 5435.3650\n",
      "Epoch 953/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4412.9424 - val_loss: 5465.8193\n",
      "Epoch 954/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4318.8777 - val_loss: 5459.1948\n",
      "Epoch 955/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4430.0625 - val_loss: 5444.3793\n",
      "Epoch 956/1000\n",
      "4000/4000 [==============================] - 0s 109us/step - loss: 4457.4160 - val_loss: 5436.1093\n",
      "Epoch 957/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 4411.4605 - val_loss: 5482.4448\n",
      "Epoch 958/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4425.5885 - val_loss: 5443.7136\n",
      "Epoch 959/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4362.5339 - val_loss: 5429.5853\n",
      "Epoch 960/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4362.1384 - val_loss: 5449.3793\n",
      "Epoch 961/1000\n",
      "4000/4000 [==============================] - 0s 92us/step - loss: 4317.3457 - val_loss: 5431.0358\n",
      "Epoch 962/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4285.5980 - val_loss: 5447.1042\n",
      "Epoch 963/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4401.4384 - val_loss: 5432.4276\n",
      "Epoch 964/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4456.6394 - val_loss: 5434.4839\n",
      "Epoch 965/1000\n",
      "4000/4000 [==============================] - 0s 91us/step - loss: 4392.9477 - val_loss: 5486.9068\n",
      "Epoch 966/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4383.6725 - val_loss: 5438.4031\n",
      "Epoch 967/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4355.7531 - val_loss: 5496.4653\n",
      "Epoch 968/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4374.1295 - val_loss: 5444.3609\n",
      "Epoch 969/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4402.4717 - val_loss: 5462.7545\n",
      "Epoch 970/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4384.0070 - val_loss: 5443.8113\n",
      "Epoch 971/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4337.6967 - val_loss: 5455.5059\n",
      "Epoch 972/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4366.6096 - val_loss: 5440.3193\n",
      "Epoch 973/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4378.6759 - val_loss: 5455.9574\n",
      "Epoch 974/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4445.5564 - val_loss: 5473.6225\n",
      "Epoch 975/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4399.1837 - val_loss: 5471.7924\n",
      "Epoch 976/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4338.6391 - val_loss: 5459.6967\n",
      "Epoch 977/1000\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 4319.9842 - val_loss: 5450.3804\n",
      "Epoch 978/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4393.0020 - val_loss: 5470.2805\n",
      "Epoch 979/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4400.2333 - val_loss: 5468.4969\n",
      "Epoch 980/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4362.6351 - val_loss: 5439.4464\n",
      "Epoch 981/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4380.0395 - val_loss: 5461.2644\n",
      "Epoch 982/1000\n",
      "4000/4000 [==============================] - 0s 96us/step - loss: 4393.8396 - val_loss: 5447.1243\n",
      "Epoch 983/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4441.2079 - val_loss: 5446.0478\n",
      "Epoch 984/1000\n",
      "4000/4000 [==============================] - 0s 101us/step - loss: 4270.3765 - val_loss: 5437.3467\n",
      "Epoch 985/1000\n",
      "4000/4000 [==============================] - 0s 104us/step - loss: 4414.6037 - val_loss: 5454.8091\n",
      "Epoch 986/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4367.5027 - val_loss: 5467.1868\n",
      "Epoch 987/1000\n",
      "4000/4000 [==============================] - 0s 93us/step - loss: 4374.0572 - val_loss: 5449.2130\n",
      "Epoch 988/1000\n",
      "4000/4000 [==============================] - 0s 99us/step - loss: 4369.7246 - val_loss: 5458.7054\n",
      "Epoch 989/1000\n",
      "4000/4000 [==============================] - 0s 94us/step - loss: 4369.8399 - val_loss: 5456.0812\n",
      "Epoch 990/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4433.4635 - val_loss: 5461.7111\n",
      "Epoch 991/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4439.7763 - val_loss: 5457.9431\n",
      "Epoch 992/1000\n",
      "4000/4000 [==============================] - 0s 100us/step - loss: 4405.0394 - val_loss: 5467.2753\n",
      "Epoch 993/1000\n",
      "4000/4000 [==============================] - 0s 97us/step - loss: 4443.9587 - val_loss: 5439.2194\n",
      "Epoch 994/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4423.5789 - val_loss: 5454.9200\n",
      "Epoch 995/1000\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 4405.6195 - val_loss: 5435.8056\n",
      "Epoch 996/1000\n",
      "4000/4000 [==============================] - 0s 108us/step - loss: 4424.4855 - val_loss: 5558.7772\n",
      "Epoch 997/1000\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 4371.3523 - val_loss: 5435.2655\n",
      "Epoch 998/1000\n",
      "4000/4000 [==============================] - 0s 105us/step - loss: 4383.0277 - val_loss: 5461.4484\n",
      "Epoch 999/1000\n",
      "4000/4000 [==============================] - 0s 98us/step - loss: 4335.4943 - val_loss: 5454.8347\n",
      "Epoch 1000/1000\n",
      "4000/4000 [==============================] - 0s 110us/step - loss: 4382.7294 - val_loss: 5439.9503\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 59))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(x_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1693.309   ],\n",
       "       [ 2640.018   ],\n",
       "       [ 1695.7252  ],\n",
       "       [ 2512.5837  ],\n",
       "       [ 2290.5137  ],\n",
       "       [ 2004.4769  ],\n",
       "       [ 3026.5894  ],\n",
       "       [ 1696.5527  ],\n",
       "       [ 2275.851   ],\n",
       "       [ 2453.2393  ],\n",
       "       [ 2363.108   ],\n",
       "       [ 5386.6094  ],\n",
       "       [ 2161.4475  ],\n",
       "       [ 1918.1283  ],\n",
       "       [ 2532.7222  ],\n",
       "       [ 1879.8496  ],\n",
       "       [ 1824.4535  ],\n",
       "       [ 2477.87    ],\n",
       "       [ 1898.97    ],\n",
       "       [ 2183.6372  ],\n",
       "       [ 1485.6763  ],\n",
       "       [ 1235.7169  ],\n",
       "       [ 2193.4023  ],\n",
       "       [ 2692.8503  ],\n",
       "       [ 1762.496   ],\n",
       "       [ 1750.7637  ],\n",
       "       [ 1584.4362  ],\n",
       "       [ 1861.0878  ],\n",
       "       [ 2542.7632  ],\n",
       "       [ 3993.4429  ],\n",
       "       [ 2245.629   ],\n",
       "       [ 3182.5244  ],\n",
       "       [ 2533.8018  ],\n",
       "       [ 3157.811   ],\n",
       "       [ 2953.8145  ],\n",
       "       [ 2185.122   ],\n",
       "       [ 2840.4622  ],\n",
       "       [ 3225.7432  ],\n",
       "       [ 3685.0671  ],\n",
       "       [ 2570.9485  ],\n",
       "       [ 1884.0715  ],\n",
       "       [ 2007.2186  ],\n",
       "       [ 1991.7101  ],\n",
       "       [ 1812.5829  ],\n",
       "       [12381.826   ],\n",
       "       [ 1777.1182  ],\n",
       "       [ 1962.6174  ],\n",
       "       [ 3141.9397  ],\n",
       "       [ 1764.0354  ],\n",
       "       [ 2012.1599  ],\n",
       "       [ 3489.8267  ],\n",
       "       [ 3309.4255  ],\n",
       "       [ 2330.73    ],\n",
       "       [15472.113   ],\n",
       "       [ 2603.7751  ],\n",
       "       [ 3899.9062  ],\n",
       "       [ 2065.092   ],\n",
       "       [ 2298.3118  ],\n",
       "       [ 2753.8674  ],\n",
       "       [ 1767.4886  ],\n",
       "       [ 3808.4731  ],\n",
       "       [ 2458.2275  ],\n",
       "       [ 1520.8154  ],\n",
       "       [ 4270.1167  ],\n",
       "       [ 3759.1426  ],\n",
       "       [ 2357.0269  ],\n",
       "       [ 2702.3503  ],\n",
       "       [ 1883.9691  ],\n",
       "       [ 2169.6929  ],\n",
       "       [ 1535.2001  ],\n",
       "       [ 4544.666   ],\n",
       "       [ 4346.942   ],\n",
       "       [ 2524.0334  ],\n",
       "       [ 2459.2158  ],\n",
       "       [ 2885.6023  ],\n",
       "       [ 1899.9032  ],\n",
       "       [ 4086.2356  ],\n",
       "       [ 2565.1729  ],\n",
       "       [ 2450.356   ],\n",
       "       [ 1699.1304  ],\n",
       "       [ 1181.2104  ],\n",
       "       [ 2463.337   ],\n",
       "       [ 2063.8936  ],\n",
       "       [ 2528.7795  ],\n",
       "       [ 1876.4868  ],\n",
       "       [ 3362.6238  ],\n",
       "       [ 1890.0061  ],\n",
       "       [ 2975.6897  ],\n",
       "       [ 2421.6526  ],\n",
       "       [ 1982.1101  ],\n",
       "       [ 2050.274   ],\n",
       "       [ 2882.7039  ],\n",
       "       [ 2974.0771  ],\n",
       "       [ 2514.3289  ],\n",
       "       [ 2172.4636  ],\n",
       "       [ 1892.776   ],\n",
       "       [ 2579.5007  ],\n",
       "       [ 2732.6973  ],\n",
       "       [ 3293.2622  ],\n",
       "       [ 1686.859   ],\n",
       "       [ 3393.2705  ],\n",
       "       [ 1145.2803  ],\n",
       "       [ 1848.5306  ],\n",
       "       [ 1477.7733  ],\n",
       "       [ 1671.297   ],\n",
       "       [ 3843.138   ],\n",
       "       [ 2153.2524  ],\n",
       "       [ 5225.2534  ],\n",
       "       [ 2140.1897  ],\n",
       "       [ 3257.312   ],\n",
       "       [ 1287.3422  ],\n",
       "       [ 2273.2166  ],\n",
       "       [ 3242.1318  ],\n",
       "       [ 2447.9326  ],\n",
       "       [ 1764.1748  ],\n",
       "       [ 2560.044   ],\n",
       "       [ 1673.1855  ],\n",
       "       [ 2852.7388  ],\n",
       "       [ 2101.599   ],\n",
       "       [ 2374.1287  ],\n",
       "       [ 1537.6865  ],\n",
       "       [ 2292.649   ],\n",
       "       [ 1372.0095  ],\n",
       "       [ 1463.9404  ],\n",
       "       [ 2199.3398  ],\n",
       "       [ 1522.3854  ],\n",
       "       [ 2408.539   ],\n",
       "       [ 2334.1763  ],\n",
       "       [ 2046.5194  ],\n",
       "       [ 2229.272   ],\n",
       "       [ 2358.9204  ],\n",
       "       [ 2310.689   ],\n",
       "       [ 3908.0017  ],\n",
       "       [ 3150.4124  ],\n",
       "       [  730.5894  ],\n",
       "       [ 2158.1587  ],\n",
       "       [ 2009.3387  ],\n",
       "       [ 2198.1946  ],\n",
       "       [ 2906.301   ],\n",
       "       [ 1998.549   ],\n",
       "       [ 2623.792   ],\n",
       "       [ 2487.484   ],\n",
       "       [ 1707.5543  ],\n",
       "       [ 4383.5044  ],\n",
       "       [ 2767.4175  ],\n",
       "       [ 2095.3467  ],\n",
       "       [ 1783.8485  ],\n",
       "       [ 2330.3535  ],\n",
       "       [ 1625.8617  ],\n",
       "       [ 3289.0234  ],\n",
       "       [ 1854.6598  ],\n",
       "       [ 2224.603   ],\n",
       "       [ 1605.8727  ],\n",
       "       [ 2286.3623  ],\n",
       "       [ 2755.911   ],\n",
       "       [ 1807.7662  ],\n",
       "       [ 2587.6453  ],\n",
       "       [ 1915.9719  ],\n",
       "       [ 2138.0288  ],\n",
       "       [ 4666.3467  ],\n",
       "       [ 3567.9048  ],\n",
       "       [ 2587.8162  ],\n",
       "       [ 4392.5723  ],\n",
       "       [ 3519.8213  ],\n",
       "       [ 2192.2769  ],\n",
       "       [ 3745.4749  ],\n",
       "       [ 2386.7522  ],\n",
       "       [ 3462.4688  ],\n",
       "       [ 2623.9404  ],\n",
       "       [ 3860.8242  ],\n",
       "       [ 3492.8694  ],\n",
       "       [ 4547.288   ],\n",
       "       [ 1834.0309  ],\n",
       "       [ 3002.357   ],\n",
       "       [ 1953.1821  ],\n",
       "       [ 2505.5435  ],\n",
       "       [ 2342.0986  ],\n",
       "       [ 3275.045   ],\n",
       "       [ 2141.8447  ],\n",
       "       [ 2154.8877  ],\n",
       "       [ 3088.8975  ],\n",
       "       [ 1487.822   ],\n",
       "       [ 2935.7798  ],\n",
       "       [ 2208.417   ],\n",
       "       [ 2412.6372  ],\n",
       "       [ 2275.7212  ],\n",
       "       [ 4868.048   ],\n",
       "       [ 2571.687   ],\n",
       "       [ 4759.8057  ],\n",
       "       [ 2654.935   ],\n",
       "       [ 3579.679   ],\n",
       "       [ 1386.4722  ],\n",
       "       [ 3024.065   ],\n",
       "       [ 2883.4634  ],\n",
       "       [ 2425.3035  ],\n",
       "       [ 2104.7383  ],\n",
       "       [ 1975.9557  ],\n",
       "       [ 2700.4321  ],\n",
       "       [ 2481.3232  ],\n",
       "       [ 2011.8773  ],\n",
       "       [ 2786.0032  ],\n",
       "       [ 1843.9277  ],\n",
       "       [ 2654.7468  ],\n",
       "       [ 3377.213   ],\n",
       "       [ 3044.0056  ],\n",
       "       [ 2125.1104  ],\n",
       "       [ 2587.4502  ],\n",
       "       [ 1706.4785  ],\n",
       "       [ 3423.598   ],\n",
       "       [ 4284.73    ],\n",
       "       [ 3751.475   ],\n",
       "       [ 1665.3383  ],\n",
       "       [ 2272.631   ],\n",
       "       [ 2038.6191  ],\n",
       "       [ 1824.9076  ],\n",
       "       [ 3851.3787  ],\n",
       "       [ 5438.5415  ],\n",
       "       [ 1937.105   ],\n",
       "       [ 2728.9243  ],\n",
       "       [ 2210.5962  ],\n",
       "       [ 1733.6094  ],\n",
       "       [ 1482.564   ],\n",
       "       [ 1915.4683  ],\n",
       "       [ 1344.3635  ],\n",
       "       [ 1043.6073  ],\n",
       "       [ 2640.4736  ],\n",
       "       [ 3708.354   ],\n",
       "       [ 3137.477   ],\n",
       "       [ 2038.28    ],\n",
       "       [ 2542.9314  ],\n",
       "       [ 3020.9597  ],\n",
       "       [ 2156.243   ],\n",
       "       [ 2000.4785  ],\n",
       "       [ 1715.7311  ],\n",
       "       [ 2367.1387  ],\n",
       "       [ 2777.202   ],\n",
       "       [ 1783.4055  ],\n",
       "       [ 1888.05    ],\n",
       "       [ 3331.6272  ],\n",
       "       [ 3230.1716  ],\n",
       "       [ 1448.0398  ],\n",
       "       [ 1674.3824  ],\n",
       "       [ 2744.689   ],\n",
       "       [ 1774.3163  ],\n",
       "       [ 3063.121   ],\n",
       "       [ 2736.4805  ],\n",
       "       [ 1657.3059  ],\n",
       "       [ 1813.3645  ],\n",
       "       [ 2520.2832  ],\n",
       "       [ 1731.0991  ],\n",
       "       [ 2202.065   ],\n",
       "       [ 2660.353   ],\n",
       "       [ 4035.596   ],\n",
       "       [ 2334.3745  ],\n",
       "       [ 1465.4517  ],\n",
       "       [ 2015.8986  ],\n",
       "       [ 2286.7026  ],\n",
       "       [ 2100.2202  ],\n",
       "       [ 2989.9966  ],\n",
       "       [ 3154.2427  ],\n",
       "       [ 1646.3478  ],\n",
       "       [ 3176.0742  ],\n",
       "       [ 2667.4263  ],\n",
       "       [ 1803.1604  ],\n",
       "       [ 2354.9136  ],\n",
       "       [ 2204.6448  ],\n",
       "       [ 1428.1299  ],\n",
       "       [  497.6528  ],\n",
       "       [ 2603.0146  ],\n",
       "       [ 2445.8403  ],\n",
       "       [ 3215.844   ],\n",
       "       [ 3201.084   ],\n",
       "       [ 2460.464   ],\n",
       "       [ 2984.253   ],\n",
       "       [ 2413.538   ],\n",
       "       [ 2829.556   ],\n",
       "       [ 2616.7651  ],\n",
       "       [ 2917.7883  ],\n",
       "       [ 3145.0496  ],\n",
       "       [ 3781.219   ],\n",
       "       [ 3339.671   ],\n",
       "       [ 2029.106   ],\n",
       "       [ 2535.8076  ],\n",
       "       [ 2805.931   ],\n",
       "       [ 2780.739   ],\n",
       "       [ 2142.3032  ],\n",
       "       [ 2866.2275  ],\n",
       "       [ 3315.5828  ],\n",
       "       [ 2158.4956  ],\n",
       "       [ 1845.99    ],\n",
       "       [ 2215.8716  ],\n",
       "       [ 1570.4066  ],\n",
       "       [ 3380.2722  ],\n",
       "       [ 2105.3613  ],\n",
       "       [ 1445.2942  ],\n",
       "       [ 1529.5258  ],\n",
       "       [ 4081.9128  ],\n",
       "       [ 2018.2716  ],\n",
       "       [ 2176.8945  ],\n",
       "       [ 2033.5963  ],\n",
       "       [ 2455.7473  ],\n",
       "       [ 3206.1921  ],\n",
       "       [ 1919.9739  ],\n",
       "       [ 3215.267   ],\n",
       "       [ 4374.94    ],\n",
       "       [ 2178.3882  ],\n",
       "       [ 1600.6122  ],\n",
       "       [ 3520.095   ],\n",
       "       [ 2941.4104  ],\n",
       "       [ 2949.501   ],\n",
       "       [ 2955.5476  ],\n",
       "       [ -216.3113  ],\n",
       "       [ 3568.815   ],\n",
       "       [ 2451.1533  ],\n",
       "       [ 2023.7837  ],\n",
       "       [ 1989.685   ],\n",
       "       [ -985.76984 ],\n",
       "       [ 2291.7039  ],\n",
       "       [ 2605.3223  ],\n",
       "       [ 1808.7458  ],\n",
       "       [ 2509.5015  ],\n",
       "       [ 1243.9686  ],\n",
       "       [ 4637.04    ],\n",
       "       [ 3074.8557  ],\n",
       "       [ 2124.794   ],\n",
       "       [ 4732.073   ],\n",
       "       [ 2711.5012  ],\n",
       "       [ 1487.957   ],\n",
       "       [ 1980.216   ],\n",
       "       [ 4180.6836  ],\n",
       "       [ 3389.4795  ],\n",
       "       [ 3180.7944  ],\n",
       "       [ 2736.6377  ],\n",
       "       [ 4101.3413  ],\n",
       "       [ 2961.2268  ],\n",
       "       [ 3022.032   ],\n",
       "       [ 2045.8757  ],\n",
       "       [ 2820.3264  ],\n",
       "       [ 2909.246   ],\n",
       "       [ 2175.4883  ],\n",
       "       [ 3787.8108  ],\n",
       "       [ 2099.7456  ],\n",
       "       [ 2428.2207  ],\n",
       "       [ 2028.4502  ],\n",
       "       [ 2547.9727  ],\n",
       "       [ 1435.6075  ],\n",
       "       [ 2214.919   ],\n",
       "       [ 1796.0858  ],\n",
       "       [ 2378.8074  ],\n",
       "       [ 2006.4637  ],\n",
       "       [ 2835.8313  ],\n",
       "       [ 1857.1816  ],\n",
       "       [ 2248.4033  ],\n",
       "       [ 4428.826   ],\n",
       "       [ 4195.7207  ],\n",
       "       [ 2413.4758  ],\n",
       "       [ 4664.4243  ],\n",
       "       [ 1814.6095  ],\n",
       "       [ 1766.6613  ],\n",
       "       [  889.4934  ],\n",
       "       [ 2064.4578  ],\n",
       "       [ 2561.9575  ],\n",
       "       [ 3391.2322  ],\n",
       "       [ 2127.0107  ],\n",
       "       [ 4443.1997  ],\n",
       "       [ 1909.3805  ],\n",
       "       [ 2191.6536  ],\n",
       "       [ 2151.1123  ],\n",
       "       [ 2945.1804  ],\n",
       "       [ 2238.9875  ],\n",
       "       [ 1597.9618  ],\n",
       "       [ 2009.4681  ],\n",
       "       [ 2413.2583  ],\n",
       "       [ 3319.5776  ],\n",
       "       [ 2662.6523  ],\n",
       "       [ 1540.6432  ],\n",
       "       [ 4063.0078  ],\n",
       "       [ 1425.8076  ],\n",
       "       [ 2295.2046  ],\n",
       "       [ 2946.8423  ],\n",
       "       [ 3143.7266  ],\n",
       "       [ 3196.1245  ],\n",
       "       [ 2024.727   ],\n",
       "       [ 2821.4988  ],\n",
       "       [ 1470.8016  ],\n",
       "       [ 2213.0466  ],\n",
       "       [ 4434.8867  ],\n",
       "       [ 3303.449   ],\n",
       "       [ 2371.5923  ],\n",
       "       [ 2015.6418  ],\n",
       "       [ 2405.6196  ],\n",
       "       [ 2805.6074  ],\n",
       "       [ 2967.303   ],\n",
       "       [ 1904.333   ],\n",
       "       [ 4040.8076  ],\n",
       "       [ 2849.0417  ],\n",
       "       [ 2738.674   ],\n",
       "       [ 2735.4893  ],\n",
       "       [ 2451.1475  ],\n",
       "       [ 3954.936   ],\n",
       "       [ 1759.7504  ],\n",
       "       [ 4868.7134  ],\n",
       "       [ 2144.9907  ],\n",
       "       [ 1513.4092  ],\n",
       "       [ 1963.9467  ],\n",
       "       [ 2230.4353  ],\n",
       "       [ 2656.167   ],\n",
       "       [ 1453.4395  ],\n",
       "       [  512.5189  ],\n",
       "       [ 2030.6346  ],\n",
       "       [ 2830.0269  ],\n",
       "       [ 1955.2849  ],\n",
       "       [ 1437.1133  ],\n",
       "       [ 1622.9568  ],\n",
       "       [ 1872.9695  ],\n",
       "       [ 2498.3098  ],\n",
       "       [ 2236.4275  ],\n",
       "       [  635.62476 ],\n",
       "       [ 2274.5845  ],\n",
       "       [ 1940.8171  ],\n",
       "       [ 2350.9705  ],\n",
       "       [ 2100.9954  ],\n",
       "       [ 3020.2717  ],\n",
       "       [ 1568.385   ],\n",
       "       [ 1886.0345  ],\n",
       "       [ 1936.5502  ],\n",
       "       [ 2687.3386  ],\n",
       "       [ 2955.1504  ],\n",
       "       [  709.54407 ],\n",
       "       [ 2995.5886  ],\n",
       "       [ 2667.486   ],\n",
       "       [ 2006.2407  ],\n",
       "       [ 2945.2876  ],\n",
       "       [ 1485.9202  ],\n",
       "       [ 1802.5348  ],\n",
       "       [ 1841.5258  ],\n",
       "       [ 2773.938   ],\n",
       "       [ 4018.2131  ],\n",
       "       [ 3008.7124  ],\n",
       "       [ 3436.889   ],\n",
       "       [ 2085.5115  ],\n",
       "       [ 2212.7095  ],\n",
       "       [ 2445.8022  ],\n",
       "       [ 3749.3716  ],\n",
       "       [ 2725.7576  ],\n",
       "       [ 1962.4106  ],\n",
       "       [ 2014.203   ],\n",
       "       [ 2119.371   ],\n",
       "       [ 2267.4285  ],\n",
       "       [ 2713.24    ],\n",
       "       [ 2641.9355  ],\n",
       "       [ 2052.7358  ],\n",
       "       [ 2076.8923  ],\n",
       "       [ 2914.0305  ],\n",
       "       [ 1156.6288  ],\n",
       "       [ 1838.4868  ],\n",
       "       [ 1917.4147  ],\n",
       "       [ 2372.7485  ],\n",
       "       [ 1627.2279  ],\n",
       "       [ 1893.6196  ],\n",
       "       [ 3639.7812  ],\n",
       "       [ 2260.7278  ],\n",
       "       [ 2312.677   ],\n",
       "       [ 2636.6895  ],\n",
       "       [ 2766.736   ],\n",
       "       [ 3490.7017  ],\n",
       "       [ 3487.5522  ],\n",
       "       [ 2705.9941  ],\n",
       "       [ 2403.6924  ],\n",
       "       [  959.2793  ],\n",
       "       [ 2367.5642  ],\n",
       "       [ 2544.4233  ],\n",
       "       [ 2653.9187  ],\n",
       "       [ 2233.5176  ],\n",
       "       [ 3866.8352  ],\n",
       "       [ 2475.5598  ],\n",
       "       [ 1368.8313  ],\n",
       "       [ 2064.4766  ],\n",
       "       [ 2805.5305  ],\n",
       "       [ 2064.1196  ],\n",
       "       [ 2335.558   ],\n",
       "       [ 2505.7573  ],\n",
       "       [-3844.9988  ],\n",
       "       [ 2002.7407  ],\n",
       "       [ 4382.784   ],\n",
       "       [ 1828.7025  ],\n",
       "       [ 2243.0547  ],\n",
       "       [ 5125.4814  ],\n",
       "       [ 2205.1318  ],\n",
       "       [ 2100.566   ],\n",
       "       [ 3225.6504  ],\n",
       "       [ 1879.9908  ],\n",
       "       [ 2843.394   ],\n",
       "       [ 2317.039   ],\n",
       "       [ 2242.7615  ],\n",
       "       [ 4258.006   ],\n",
       "       [ 1842.4534  ],\n",
       "       [ 3164.3809  ],\n",
       "       [ 1722.1917  ],\n",
       "       [ 2043.3468  ],\n",
       "       [ 3054.924   ],\n",
       "       [ 2097.5476  ],\n",
       "       [ 2139.584   ],\n",
       "       [ 2604.4768  ],\n",
       "       [ 2463.331   ],\n",
       "       [ 2951.0425  ],\n",
       "       [ 1351.6539  ],\n",
       "       [ 3216.2637  ],\n",
       "       [ 3595.498   ],\n",
       "       [ 2348.9082  ],\n",
       "       [ 2388.2356  ],\n",
       "       [ 2467.5808  ],\n",
       "       [ 2483.46    ],\n",
       "       [ 2264.4668  ],\n",
       "       [ 2110.166   ],\n",
       "       [ 1973.6686  ],\n",
       "       [ 2629.034   ],\n",
       "       [ 2072.097   ],\n",
       "       [ 1790.325   ],\n",
       "       [ 2828.4111  ],\n",
       "       [ 2028.7623  ],\n",
       "       [ 3650.4297  ],\n",
       "       [ 2211.8372  ],\n",
       "       [ 1852.5605  ],\n",
       "       [ 2663.3535  ],\n",
       "       [ 3843.6782  ],\n",
       "       [ 3128.5713  ],\n",
       "       [ 3478.0242  ],\n",
       "       [ 2189.9495  ],\n",
       "       [ 2393.4485  ],\n",
       "       [ 4310.4424  ],\n",
       "       [ 2453.9382  ],\n",
       "       [ 2678.122   ],\n",
       "       [ 1873.0286  ],\n",
       "       [ 1933.235   ],\n",
       "       [ 3589.8516  ],\n",
       "       [ 2096.5698  ],\n",
       "       [ 4224.4785  ],\n",
       "       [ 2821.4668  ],\n",
       "       [ 6544.6113  ],\n",
       "       [ 2692.1184  ],\n",
       "       [ 9961.505   ],\n",
       "       [ 2701.6108  ],\n",
       "       [ 2418.454   ],\n",
       "       [ 3266.0032  ],\n",
       "       [ 2225.783   ],\n",
       "       [ 2814.904   ],\n",
       "       [ 2588.4897  ],\n",
       "       [ 1679.0466  ],\n",
       "       [ 3250.9373  ],\n",
       "       [ 1994.6808  ],\n",
       "       [  799.5405  ],\n",
       "       [ 2304.704   ],\n",
       "       [ 1843.8995  ],\n",
       "       [14133.689   ],\n",
       "       [ 1443.7773  ],\n",
       "       [ 2454.224   ],\n",
       "       [ 1718.4752  ],\n",
       "       [ 2349.7456  ],\n",
       "       [ 1477.5262  ],\n",
       "       [ 1824.7007  ],\n",
       "       [ 1731.4027  ],\n",
       "       [ 2086.3223  ],\n",
       "       [ 2236.018   ],\n",
       "       [ 3489.7437  ],\n",
       "       [ 2206.0393  ],\n",
       "       [ 2323.538   ],\n",
       "       [ 1804.6188  ],\n",
       "       [ 2190.7861  ],\n",
       "       [ 2588.9045  ],\n",
       "       [ 3664.271   ],\n",
       "       [ 2582.5415  ],\n",
       "       [ 1410.6057  ],\n",
       "       [ 2243.1816  ],\n",
       "       [ 1679.6556  ],\n",
       "       [ 1790.5276  ],\n",
       "       [ 4088.8704  ],\n",
       "       [ 4274.6934  ],\n",
       "       [ 3149.5957  ],\n",
       "       [ 1567.7488  ],\n",
       "       [ 2097.272   ],\n",
       "       [ 3918.8206  ],\n",
       "       [ 2347.2705  ],\n",
       "       [  331.47433 ],\n",
       "       [ 1923.5619  ],\n",
       "       [ 1643.0494  ],\n",
       "       [ 1977.5938  ],\n",
       "       [ 2918.7524  ],\n",
       "       [ 3261.1018  ],\n",
       "       [ 5317.5254  ],\n",
       "       [ 3667.6611  ],\n",
       "       [ 1700.4852  ],\n",
       "       [ 3110.4731  ],\n",
       "       [ 3841.163   ],\n",
       "       [ 1931.6565  ],\n",
       "       [ 2700.2515  ],\n",
       "       [ 4430.833   ],\n",
       "       [ 2061.4446  ],\n",
       "       [ 4230.551   ],\n",
       "       [ 2776.1436  ],\n",
       "       [ 1402.763   ],\n",
       "       [ 2665.6035  ],\n",
       "       [ 2073.4424  ],\n",
       "       [ 4599.008   ],\n",
       "       [ 2706.8691  ],\n",
       "       [ 3379.1943  ],\n",
       "       [ 2988.123   ],\n",
       "       [ 2044.1324  ],\n",
       "       [ 2289.891   ],\n",
       "       [ 3223.0627  ],\n",
       "       [ 2574.1892  ],\n",
       "       [ 4304.7     ],\n",
       "       [ 2863.4775  ],\n",
       "       [ 4060.046   ],\n",
       "       [ 2134.559   ],\n",
       "       [ 1708.9833  ],\n",
       "       [ 3279.7266  ],\n",
       "       [ 3354.4673  ],\n",
       "       [ 2217.4126  ],\n",
       "       [ 3978.3274  ],\n",
       "       [ 3216.6812  ],\n",
       "       [ 2770.3916  ],\n",
       "       [ 5060.3677  ],\n",
       "       [ 2907.491   ],\n",
       "       [ 2258.7234  ],\n",
       "       [ 2326.9492  ],\n",
       "       [ 5672.713   ],\n",
       "       [ 2248.9084  ],\n",
       "       [ 3414.3682  ],\n",
       "       [ 3246.8774  ],\n",
       "       [ 2844.3696  ],\n",
       "       [ 1740.749   ],\n",
       "       [ 2209.9385  ],\n",
       "       [ 2361.3093  ],\n",
       "       [ 1910.5426  ],\n",
       "       [ 7424.9893  ],\n",
       "       [  -64.87648 ],\n",
       "       [ 1470.119   ],\n",
       "       [ 3148.7842  ],\n",
       "       [ 2469.091   ],\n",
       "       [ 3779.4595  ],\n",
       "       [ 1847.5753  ],\n",
       "       [ 1872.5072  ],\n",
       "       [ 1727.592   ],\n",
       "       [ 2718.6194  ],\n",
       "       [ 2168.5986  ],\n",
       "       [ 4331.6826  ],\n",
       "       [ 1753.9222  ],\n",
       "       [ 4316.5024  ],\n",
       "       [ 2608.289   ],\n",
       "       [ 1602.7944  ],\n",
       "       [ 3743.7292  ],\n",
       "       [ 2721.0408  ],\n",
       "       [ 2756.1006  ],\n",
       "       [  -52.138687],\n",
       "       [ -807.6262  ],\n",
       "       [ 2294.5547  ],\n",
       "       [ 9349.033   ],\n",
       "       [ 1776.1033  ],\n",
       "       [ 1715.1821  ],\n",
       "       [ 2681.691   ],\n",
       "       [ 1917.093   ],\n",
       "       [ 4848.285   ],\n",
       "       [ 3370.4724  ],\n",
       "       [ 2478.4292  ],\n",
       "       [ 3168.7827  ],\n",
       "       [ 2899.337   ],\n",
       "       [ 2174.2012  ],\n",
       "       [ 5159.824   ],\n",
       "       [ 1234.7518  ],\n",
       "       [ 3381.847   ],\n",
       "       [ 1901.8148  ],\n",
       "       [ 2675.5813  ],\n",
       "       [ 2884.3489  ],\n",
       "       [ 2389.073   ],\n",
       "       [ 2046.2185  ],\n",
       "       [ 1407.6117  ],\n",
       "       [ 2348.2322  ],\n",
       "       [ 2020.7584  ],\n",
       "       [ 1900.2626  ],\n",
       "       [ 2340.254   ],\n",
       "       [ 1848.3611  ],\n",
       "       [ 2689.3188  ],\n",
       "       [ 1152.0193  ],\n",
       "       [ 2599.7346  ],\n",
       "       [ 2467.3452  ],\n",
       "       [ 2408.0981  ],\n",
       "       [ 2605.339   ],\n",
       "       [ 1928.9235  ],\n",
       "       [ 2379.9673  ],\n",
       "       [ 8133.101   ],\n",
       "       [ 1532.7764  ],\n",
       "       [ 3060.2668  ],\n",
       "       [ 1425.1346  ],\n",
       "       [ 2102.013   ],\n",
       "       [ 2851.181   ],\n",
       "       [ 1664.6567  ],\n",
       "       [ 2050.2192  ],\n",
       "       [ 2837.9097  ],\n",
       "       [ 1889.3077  ],\n",
       "       [ 2960.4314  ],\n",
       "       [ 2054.0728  ],\n",
       "       [ 3117.2856  ],\n",
       "       [ 1725.8314  ],\n",
       "       [ 2239.9553  ],\n",
       "       [ 2933.5708  ],\n",
       "       [ 1998.7977  ],\n",
       "       [ 2423.62    ],\n",
       "       [ 1866.809   ],\n",
       "       [ 2842.3755  ],\n",
       "       [ 1385.9644  ],\n",
       "       [ 1693.1415  ],\n",
       "       [ 2352.1628  ],\n",
       "       [ 3751.6768  ],\n",
       "       [ 3382.414   ],\n",
       "       [ 1971.8362  ],\n",
       "       [ 1553.3665  ],\n",
       "       [ 1546.7507  ],\n",
       "       [ 2697.8828  ],\n",
       "       [ 3155.5312  ],\n",
       "       [ 1696.3026  ],\n",
       "       [ 2329.8945  ],\n",
       "       [ 3696.1912  ],\n",
       "       [ 1726.7172  ],\n",
       "       [ 1919.2268  ],\n",
       "       [  992.3218  ],\n",
       "       [ 2371.3071  ],\n",
       "       [ 1709.3573  ],\n",
       "       [ 2685.8525  ],\n",
       "       [ 1631.2765  ],\n",
       "       [ 1768.0265  ],\n",
       "       [ 1906.71    ],\n",
       "       [ 2481.4648  ],\n",
       "       [ 3300.1929  ],\n",
       "       [ 1330.8282  ],\n",
       "       [ 3211.973   ],\n",
       "       [ 4031.2449  ],\n",
       "       [ 3761.8794  ],\n",
       "       [ 2316.7178  ],\n",
       "       [ 2757.6206  ],\n",
       "       [ 2021.6709  ],\n",
       "       [ 1775.1008  ],\n",
       "       [ 2235.2197  ],\n",
       "       [ 2143.231   ],\n",
       "       [ 3267.8037  ],\n",
       "       [ 2119.1396  ],\n",
       "       [ 1379.146   ],\n",
       "       [ 2300.9321  ],\n",
       "       [ 2615.2302  ],\n",
       "       [ 1940.9287  ],\n",
       "       [ 2004.624   ],\n",
       "       [ 4022.0186  ],\n",
       "       [ 2315.3042  ],\n",
       "       [ 2722.8882  ],\n",
       "       [ 4036.0798  ],\n",
       "       [ 2016.1934  ],\n",
       "       [ 2250.6057  ],\n",
       "       [ 2962.224   ],\n",
       "       [ 1834.0774  ],\n",
       "       [ 1454.9419  ],\n",
       "       [ 2207.558   ],\n",
       "       [ 2590.332   ],\n",
       "       [ 2015.0277  ],\n",
       "       [ 3118.925   ],\n",
       "       [ 2921.6362  ],\n",
       "       [ 3966.3093  ],\n",
       "       [ 1540.8712  ],\n",
       "       [ 2081.8499  ],\n",
       "       [ 1650.6442  ],\n",
       "       [ 1772.731   ],\n",
       "       [ 2106.295   ],\n",
       "       [ 2191.1904  ],\n",
       "       [ 2426.9568  ],\n",
       "       [ 1665.491   ],\n",
       "       [ 1978.1885  ],\n",
       "       [ 2806.5383  ],\n",
       "       [ 2515.8784  ],\n",
       "       [ 2668.2012  ],\n",
       "       [ 1597.3092  ],\n",
       "       [ 2163.294   ],\n",
       "       [ 2490.268   ],\n",
       "       [ 2219.487   ],\n",
       "       [ 1831.055   ],\n",
       "       [ 2069.3125  ],\n",
       "       [ 2180.648   ],\n",
       "       [ 2533.936   ],\n",
       "       [ 2758.569   ],\n",
       "       [ 2062.1792  ],\n",
       "       [ 6068.1245  ],\n",
       "       [ 2874.1858  ],\n",
       "       [ 2401.0425  ],\n",
       "       [ 2157.6956  ],\n",
       "       [ 2236.5044  ],\n",
       "       [ 1638.8547  ],\n",
       "       [ 2669.2708  ],\n",
       "       [ 3035.3025  ],\n",
       "       [ 3343.6838  ],\n",
       "       [ 2091.895   ],\n",
       "       [ 3275.5068  ],\n",
       "       [ 3123.3628  ],\n",
       "       [ 2078.04    ],\n",
       "       [ 2357.9016  ],\n",
       "       [ 2456.585   ],\n",
       "       [ 3063.25    ],\n",
       "       [ 1843.7827  ],\n",
       "       [ 2464.7532  ],\n",
       "       [ 2674.3047  ],\n",
       "       [ 1324.1283  ],\n",
       "       [ 8267.412   ],\n",
       "       [ 4547.3213  ],\n",
       "       [  698.395   ],\n",
       "       [ 2926.5208  ],\n",
       "       [ 2088.8613  ],\n",
       "       [ 2401.865   ],\n",
       "       [ 1575.209   ],\n",
       "       [ 1428.2124  ],\n",
       "       [ 1821.7386  ],\n",
       "       [ 2325.2566  ],\n",
       "       [ 3807.4631  ],\n",
       "       [ 2064.9287  ],\n",
       "       [ 1794.3201  ],\n",
       "       [ 2495.542   ],\n",
       "       [ 8856.982   ],\n",
       "       [ 1904.0656  ],\n",
       "       [ 1711.8934  ],\n",
       "       [ 3336.5518  ],\n",
       "       [ 1906.4196  ],\n",
       "       [ 2096.3662  ],\n",
       "       [ 2848.4905  ],\n",
       "       [ 1885.0951  ],\n",
       "       [ 3435.2979  ],\n",
       "       [ 2137.9878  ],\n",
       "       [ 1863.6451  ],\n",
       "       [ 2566.3958  ],\n",
       "       [ 2016.4943  ],\n",
       "       [ 2591.397   ],\n",
       "       [ 2196.5735  ],\n",
       "       [ 1391.6282  ],\n",
       "       [ 2157.142   ],\n",
       "       [ 2527.3652  ],\n",
       "       [ 1717.1033  ],\n",
       "       [ 1441.6958  ],\n",
       "       [ 2888.992   ],\n",
       "       [ 3051.1682  ],\n",
       "       [ 2177.4673  ],\n",
       "       [ 1339.0527  ],\n",
       "       [ 2267.1077  ],\n",
       "       [ 1627.9858  ],\n",
       "       [ 2673.8374  ],\n",
       "       [ 3954.6272  ],\n",
       "       [ 2695.6733  ],\n",
       "       [ 2640.676   ],\n",
       "       [ 3863.1113  ],\n",
       "       [ 1743.666   ],\n",
       "       [ 1701.1627  ],\n",
       "       [ 1529.3729  ],\n",
       "       [ 2696.0134  ],\n",
       "       [ 3408.4739  ],\n",
       "       [ 1367.1256  ],\n",
       "       [ 1721.8853  ],\n",
       "       [ 1656.8788  ],\n",
       "       [ 6490.6875  ],\n",
       "       [ 1817.3079  ],\n",
       "       [  213.07323 ],\n",
       "       [ 2457.6384  ],\n",
       "       [ 2542.3943  ],\n",
       "       [ 3676.9487  ],\n",
       "       [ 1942.1007  ],\n",
       "       [ 2582.4546  ],\n",
       "       [ 2267.3467  ],\n",
       "       [ 4693.938   ],\n",
       "       [ 2283.6196  ],\n",
       "       [ 1422.8063  ],\n",
       "       [ 3428.5674  ],\n",
       "       [ 2203.6382  ],\n",
       "       [ 2075.5747  ],\n",
       "       [13417.564   ],\n",
       "       [ 3539.134   ],\n",
       "       [ 2098.0508  ],\n",
       "       [ 3065.4304  ],\n",
       "       [ -121.60646 ],\n",
       "       [ 2201.5615  ],\n",
       "       [ 2377.5688  ],\n",
       "       [ 1756.903   ],\n",
       "       [ 1802.0908  ],\n",
       "       [ 1564.2958  ],\n",
       "       [ 2693.271   ],\n",
       "       [ 3804.6558  ],\n",
       "       [ 3705.24    ],\n",
       "       [ 1176.3527  ],\n",
       "       [ 2507.6865  ],\n",
       "       [ 2338.6902  ],\n",
       "       [ 2694.9954  ],\n",
       "       [ 2713.4897  ],\n",
       "       [ 1901.8271  ],\n",
       "       [ 1823.7335  ],\n",
       "       [ 1902.7351  ],\n",
       "       [ 4286.4253  ],\n",
       "       [ 3584.1843  ],\n",
       "       [ 1818.6871  ],\n",
       "       [ 4775.408   ],\n",
       "       [ 2614.683   ],\n",
       "       [ 3918.799   ],\n",
       "       [ 2646.6719  ],\n",
       "       [ 1395.51    ],\n",
       "       [ 1736.0563  ],\n",
       "       [ 3810.6685  ],\n",
       "       [ 2996.7668  ],\n",
       "       [ 2947.6199  ],\n",
       "       [ 2484.5276  ],\n",
       "       [ 1794.8501  ],\n",
       "       [ 2240.0273  ],\n",
       "       [ 2317.6987  ],\n",
       "       [ 2796.2302  ],\n",
       "       [ 2589.8757  ],\n",
       "       [ 2916.6895  ],\n",
       "       [ 4389.728   ],\n",
       "       [ 2894.1895  ],\n",
       "       [ 1720.3955  ],\n",
       "       [ 2576.3232  ],\n",
       "       [ 2459.3765  ],\n",
       "       [ 1792.8264  ],\n",
       "       [ 2082.769   ],\n",
       "       [ 3150.2998  ],\n",
       "       [ 3723.266   ],\n",
       "       [ 2162.0615  ],\n",
       "       [ 1768.3141  ],\n",
       "       [ 2931.592   ],\n",
       "       [ 3144.2183  ],\n",
       "       [ 6084.2173  ],\n",
       "       [ 2412.7478  ],\n",
       "       [ 2272.667   ],\n",
       "       [ 1642.9108  ],\n",
       "       [  997.9861  ],\n",
       "       [ 2335.9048  ],\n",
       "       [ 2603.2112  ],\n",
       "       [ 2065.357   ],\n",
       "       [ 3389.2444  ],\n",
       "       [ 2169.3828  ],\n",
       "       [ 3420.7822  ],\n",
       "       [ 2629.958   ],\n",
       "       [ 2816.689   ],\n",
       "       [ 2975.9465  ],\n",
       "       [ 2499.748   ],\n",
       "       [ 1998.54    ],\n",
       "       [ 2766.8694  ],\n",
       "       [ 3942.236   ],\n",
       "       [ 2502.9622  ],\n",
       "       [ 4482.6562  ],\n",
       "       [ 1785.3723  ],\n",
       "       [ 2901.3027  ],\n",
       "       [ 2087.81    ],\n",
       "       [ 1351.5348  ],\n",
       "       [ 4151.7246  ],\n",
       "       [ 2567.3965  ],\n",
       "       [ 2256.5327  ],\n",
       "       [ 2795.843   ],\n",
       "       [ 3262.7417  ],\n",
       "       [  918.8884  ],\n",
       "       [ 3579.8403  ],\n",
       "       [ 2322.2637  ],\n",
       "       [ 1606.4647  ],\n",
       "       [ 2096.6387  ],\n",
       "       [ 1815.6696  ],\n",
       "       [ 2789.0078  ],\n",
       "       [ 2413.0095  ],\n",
       "       [ 1931.549   ],\n",
       "       [ 1504.1318  ],\n",
       "       [ 2513.916   ],\n",
       "       [ 2901.0667  ],\n",
       "       [ 3808.5735  ],\n",
       "       [ 3249.069   ],\n",
       "       [ 2061.4976  ],\n",
       "       [ 2968.4734  ],\n",
       "       [ 2899.8328  ],\n",
       "       [ 1513.7251  ],\n",
       "       [ 2545.9104  ],\n",
       "       [ 2629.5264  ],\n",
       "       [ 2083.8032  ],\n",
       "       [ 2005.5245  ],\n",
       "       [ 3535.9673  ],\n",
       "       [ 3258.7927  ],\n",
       "       [ 1951.9395  ],\n",
       "       [ 3016.6     ],\n",
       "       [ 2026.6031  ],\n",
       "       [ 1621.8524  ],\n",
       "       [ 2677.7585  ],\n",
       "       [  290.24313 ],\n",
       "       [ 1195.4286  ],\n",
       "       [ 1807.0339  ],\n",
       "       [ 1166.347   ],\n",
       "       [ 1312.1252  ],\n",
       "       [ 2150.3674  ],\n",
       "       [ 2233.729   ],\n",
       "       [ 3627.06    ],\n",
       "       [ 3997.7085  ],\n",
       "       [ 1659.0814  ],\n",
       "       [ 1315.9036  ],\n",
       "       [ 2237.8357  ],\n",
       "       [ 3177.719   ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
